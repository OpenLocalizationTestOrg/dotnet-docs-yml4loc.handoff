{"nodes":[{"content":"Provides the means to access and manage an in-process speech recognition engine.","nodes":[{"pos":[0,80],"content":"Provides the means to access and manage an in-process speech recognition engine.","nodes":[{"content":"Provides the means to access and manage an in-process speech recognition engine.","pos":[0,80]}]}],"pos":[5314,5395],"yaml":true},{"content":"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.  \n  \n This class is for running speech recognition engines in-process, and provides control over various aspects of speech recognition, as follows:  \n  \n-   To create an in-process speech recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> constructors.  \n  \n-   To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> methods, and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property.  \n  \n-   To configure the input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> method.  \n  \n-   To perform speech recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method.  \n  \n-   To modify how recognition handles silence or unexpected input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.  \n  \n-   To change the number of alternates the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.  \n  \n-   To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. The recognizer uses more than one thread to perform tasks.  \n  \n-   To emulate input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \n  \n The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object is for the sole use of the process that instantiated the object. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer> shares a single recognizer with any application that wants to use it.  \n  \n> [!NOTE]\n>  Always call <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.","nodes":[{"pos":[0,247],"content":"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","nodes":[{"content":"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","pos":[0,247],"nodes":[{"content":"You can create an instance of this class for any of the installed speech recognizers.","pos":[0,85]},{"content":"To get information about which recognizers are installed, use the static <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A&gt;</ph> method.","pos":[86,247],"source":" To get information about which recognizers are installed, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method."}]}]},{"pos":[254,395],"content":"This class is for running speech recognition engines in-process, and provides control over various aspects of speech recognition, as follows:","nodes":[{"content":"This class is for running speech recognition engines in-process, and provides control over various aspects of speech recognition, as follows:","pos":[0,141]}]},{"pos":[405,544],"content":"To create an in-process speech recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> constructors.","nodes":[{"content":"To create an in-process speech recognizer, use one of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A&gt;</ph> constructors.","pos":[0,139],"source":"To create an in-process speech recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> constructors."}]},{"pos":[554,1004],"content":"To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> methods, and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property.","nodes":[{"content":"To manage speech recognition grammars, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A&gt;</ph> methods, and the <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A&gt;</ph> property.","pos":[0,450],"source":"To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> methods, and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property."}]},{"pos":[1014,1484],"content":"To configure the input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> method.","nodes":[{"content":"To configure the input to the recognizer, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A&gt;</ph>, or <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A&gt;</ph> method.","pos":[0,470],"source":"To configure the input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> method."}]},{"pos":[1494,1688],"content":"To perform speech recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method.","nodes":[{"content":"To perform speech recognition, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> method.","pos":[0,194],"source":"To perform speech recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method."}]},{"pos":[1698,2108],"content":"To modify how recognition handles silence or unexpected input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.","nodes":[{"content":"To modify how recognition handles silence or unexpected input, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A&gt;</ph> properties.","pos":[0,410],"source":"To modify how recognition handles silence or unexpected input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties."}]},{"pos":[2118,2375],"content":"To change the number of alternates the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.","nodes":[{"content":"To change the number of alternates the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.","pos":[0,257],"nodes":[{"content":"To change the number of alternates the recognizer returns, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A&gt;</ph> property.","pos":[0,150],"source":"To change the number of alternates the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> property."},{"content":"The recognizer returns recognition results in a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object.","pos":[151,257],"source":" The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object."}]}]},{"pos":[2385,2585],"content":"To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. The recognizer uses more than one thread to perform tasks.","nodes":[{"content":"To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. The recognizer uses more than one thread to perform tasks.","pos":[0,200],"nodes":[{"content":"To synchronize changes to the recognizer, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[0,141],"source":"To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method."},{"content":"The recognizer uses more than one thread to perform tasks.","pos":[142,200]}]}]},{"pos":[2595,2810],"content":"To emulate input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.","nodes":[{"content":"To emulate input to the recognizer, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> methods.","pos":[0,215],"source":"To emulate input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods."}]},{"pos":[2817,3086],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object is for the sole use of the process that instantiated the object. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer> shares a single recognizer with any application that wants to use it.","nodes":[{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object is for the sole use of the process that instantiated the object. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer> shares a single recognizer with any application that wants to use it.","pos":[0,269],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> object is for the sole use of the process that instantiated the object.","pos":[0,132],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object is for the sole use of the process that instantiated the object."},{"content":"By contrast, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> shares a single recognizer with any application that wants to use it.","pos":[133,269],"source":" By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer> shares a single recognizer with any application that wants to use it."}]}]},{"pos":[3094,3381],"content":"[!NOTE]\n Always call <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.","leadings":["","> "],"nodes":[{"content":" Always call <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.","pos":[8,285],"nodes":[{"content":"Always call <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A&gt;</ph> before you release your last reference to the speech recognizer.","pos":[1,145],"source":" Always call <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> before you release your last reference to the speech recognizer."},{"content":"Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's <ph id=\"ph1\">`Finalize`</ph> method.","pos":[146,277],"source":" Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method."}]}]}],"pos":[5406,8813],"yaml":true,"extradata":"MT"},{"content":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class using the default speech recognizer for the system.","nodes":[{"pos":[0,162],"content":"Initializes a new instance of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> class using the default speech recognizer for the system.","source":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class using the default speech recognizer for the system."}],"pos":[11997,12160],"yaml":true},{"content":"Before the speech recognizer can begin speech recognition, you must load at least one recognition grammar and configure the input for the recognizer.  \n  \n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \n  \n To configure the audio input, use one of the following methods:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[{"pos":[0,149],"content":"Before the speech recognizer can begin speech recognition, you must load at least one recognition grammar and configure the input for the recognizer.","nodes":[{"content":"Before the speech recognizer can begin speech recognition, you must load at least one recognition grammar and configure the input for the recognizer.","pos":[0,149]}]},{"pos":[156,343],"content":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.","nodes":[{"content":"To load a grammar, call the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A&gt;</ph> method.","pos":[0,187],"source":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method."}]},{"pos":[350,413],"content":"To configure the audio input, use one of the following methods:","nodes":[{"content":"To configure the audio input, use one of the following methods:","pos":[0,63]}]},{"pos":[423,504],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>","nodes":[]},{"pos":[514,602],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>","nodes":[]},{"pos":[612,686],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>","nodes":[]},{"pos":[696,774],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>","nodes":[]},{"pos":[784,864],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[]}],"pos":[12171,13052],"yaml":true,"extradata":"MT"},{"content":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class using the default speech recognizer for a specified locale.","nodes":[{"pos":[0,170],"content":"Initializes a new instance of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> class using the default speech recognizer for a specified locale.","source":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class using the default speech recognizer for a specified locale."}],"pos":[14371,14542],"yaml":true},{"content":"Microsoft Windows and the System.Speech API accept all valid language-country codes. To perform speech recognition using the language specified in the `CultureInfo` argument, a speech recognition engine that supports that language-country code must be installed. The speech recognition engines that shipped with Microsoft Windows 7 work with the following language-country codes.  \n  \n-   en-GB. English (United Kingdom)  \n  \n-   en-US. English (United States)  \n  \n-   de-DE. German (Germany)  \n  \n-   es-ES. Spanish (Spain)  \n  \n-   fr-FR. French (France)  \n  \n-   ja-JP. Japanese (Japan)  \n  \n-   zh-CN. Chinese (China)  \n  \n-   zh-TW. Chinese (Taiwan)  \n  \n Two-letter language codes such as \"en\", \"fr\", or \"es\" are also permitted.  \n  \n Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.  \n  \n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \n  \n To configure the audio input, use one of the following methods:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[{"pos":[0,379],"content":"Microsoft Windows and the System.Speech API accept all valid language-country codes. To perform speech recognition using the language specified in the `CultureInfo` argument, a speech recognition engine that supports that language-country code must be installed. The speech recognition engines that shipped with Microsoft Windows 7 work with the following language-country codes.","nodes":[{"content":"Microsoft Windows and the System.Speech API accept all valid language-country codes.","pos":[0,84]},{"content":"To perform speech recognition using the language specified in the <ph id=\"ph1\">`CultureInfo`</ph> argument, a speech recognition engine that supports that language-country code must be installed.","pos":[85,262],"source":" To perform speech recognition using the language specified in the `CultureInfo` argument, a speech recognition engine that supports that language-country code must be installed."},{"content":"The speech recognition engines that shipped with Microsoft Windows 7 work with the following language-country codes.","pos":[263,379]}]},{"pos":[389,420],"content":"en-GB. English (United Kingdom)","nodes":[{"content":"en-GB. English (United Kingdom)","pos":[0,31],"nodes":[{"content":"en-GB.","pos":[0,6]},{"content":"English (United Kingdom)","pos":[7,31]}]}]},{"pos":[430,460],"content":"en-US. English (United States)","nodes":[{"content":"en-US. English (United States)","pos":[0,30],"nodes":[{"content":"en-US.","pos":[0,6]},{"content":"English (United States)","pos":[7,30]}]}]},{"pos":[470,493],"content":"de-DE. German (Germany)","nodes":[{"content":"de-DE. German (Germany)","pos":[0,23],"nodes":[{"content":"de-DE.","pos":[0,6]},{"content":"German (Germany)","pos":[7,23]}]}]},{"pos":[503,525],"content":"es-ES. Spanish (Spain)","nodes":[{"content":"es-ES. Spanish (Spain)","pos":[0,22],"nodes":[{"content":"es-ES.","pos":[0,6]},{"content":"Spanish (Spain)","pos":[7,22]}]}]},{"pos":[535,557],"content":"fr-FR. French (France)","nodes":[{"content":"fr-FR. French (France)","pos":[0,22],"nodes":[{"content":"fr-FR.","pos":[0,6]},{"content":"French (France)","pos":[7,22]}]}]},{"pos":[567,590],"content":"ja-JP. Japanese (Japan)","nodes":[{"content":"ja-JP. Japanese (Japan)","pos":[0,23],"nodes":[{"content":"ja-JP.","pos":[0,6]},{"content":"Japanese (Japan)","pos":[7,23]}]}]},{"pos":[600,622],"content":"zh-CN. Chinese (China)","nodes":[{"content":"zh-CN. Chinese (China)","pos":[0,22],"nodes":[{"content":"zh-CN.","pos":[0,6]},{"content":"Chinese (China)","pos":[7,22]}]}]},{"pos":[632,655],"content":"zh-TW. Chinese (Taiwan)","nodes":[{"content":"zh-TW. Chinese (Taiwan)","pos":[0,23],"nodes":[{"content":"zh-TW.","pos":[0,6]},{"content":"Chinese (Taiwan)","pos":[7,23]}]}]},{"pos":[662,735],"content":"Two-letter language codes such as \"en\", \"fr\", or \"es\" are also permitted.","nodes":[{"content":"Two-letter language codes such as \"en\", \"fr\", or \"es\" are also permitted.","pos":[0,73]}]},{"pos":[742,891],"content":"Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.","nodes":[{"content":"Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.","pos":[0,149]}]},{"pos":[898,1085],"content":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.","nodes":[{"content":"To load a grammar, call the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A&gt;</ph> method.","pos":[0,187],"source":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method."}]},{"pos":[1092,1155],"content":"To configure the audio input, use one of the following methods:","nodes":[{"content":"To configure the audio input, use one of the following methods:","pos":[0,63]}]},{"pos":[1165,1246],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>","nodes":[]},{"pos":[1256,1344],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>","nodes":[]},{"pos":[1354,1428],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>","nodes":[]},{"pos":[1438,1516],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>","nodes":[]},{"pos":[1526,1606],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[]}],"pos":[14553,16202],"yaml":true,"extradata":"MT"},{"content":"The locale that the speech recognizer must support.","nodes":[{"pos":[0,51],"content":"The locale that the speech recognizer must support.","nodes":[{"content":"The locale that the speech recognizer must support.","pos":[0,51]}]}],"pos":[17965,18017],"yaml":true},{"content":"None of the installed speech recognizers support the specified locale, or <code>culture</code> is the invariant culture.","nodes":[{"pos":[0,120],"content":"None of the installed speech recognizers support the specified locale, or <bpt id=\"p1\">&lt;code&gt;</bpt><ph id=\"ph1\">culture</ph><ept id=\"p1\">&lt;/code&gt;</ept> is the invariant culture.","source":"None of the installed speech recognizers support the specified locale, or <code>culture</code> is the invariant culture."}],"pos":[18194,18315],"yaml":true},{"content":"<code>Culture</code> is `null`.","nodes":[{"pos":[0,31],"content":"<ph id=\"ph1\">&lt;code&gt;Culture&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>Culture</code> is `null`."}],"pos":[18417,18449],"yaml":true},{"content":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> using the information in a <xref href=\"System.Speech.Recognition.RecognizerInfo\"></xref> object to specify the recognizer to use.","nodes":[{"pos":[0,234],"content":"Initializes a new instance of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> using the information in a <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.RecognizerInfo\"&gt;&lt;/xref&gt;</ph> object to specify the recognizer to use.","source":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> using the information in a <xref href=\"System.Speech.Recognition.RecognizerInfo\"></xref> object to specify the recognizer to use."}],"pos":[19638,19873],"yaml":true},{"content":"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.  \n  \n Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.  \n  \n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \n  \n To configure the audio input, use one of the following methods:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[{"pos":[0,240],"content":"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","nodes":[{"content":"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","pos":[0,240],"nodes":[{"content":"You can create an instance of this class for any of the installed speech recognizers.","pos":[0,85]},{"content":"To get information about which recognizers are installed, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A&gt;</ph> method.","pos":[86,240],"source":" To get information about which recognizers are installed, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method."}]}]},{"pos":[247,396],"content":"Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.","nodes":[{"content":"Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.","pos":[0,149]}]},{"pos":[403,590],"content":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.","nodes":[{"content":"To load a grammar, call the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A&gt;</ph> method.","pos":[0,187],"source":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method."}]},{"pos":[597,660],"content":"To configure the audio input, use one of the following methods:","nodes":[{"content":"To configure the audio input, use one of the following methods:","pos":[0,63]}]},{"pos":[670,751],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>","nodes":[]},{"pos":[761,849],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>","nodes":[]},{"pos":[859,933],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>","nodes":[]},{"pos":[943,1021],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>","nodes":[]},{"pos":[1031,1111],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[]}],"pos":[19884,21014],"yaml":true,"extradata":"MT"},{"content":"The information for the specific speech recognizer.","nodes":[{"pos":[0,51],"content":"The information for the specific speech recognizer.","nodes":[{"content":"The information for the specific speech recognizer.","pos":[0,51]}]}],"pos":[23110,23162],"yaml":true},{"content":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class with a string parameter that specifies the name of the recognizer to use.","nodes":[{"pos":[0,184],"content":"Initializes a new instance of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> class with a string parameter that specifies the name of the recognizer to use.","source":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class with a string parameter that specifies the name of the recognizer to use."}],"pos":[24332,24517],"yaml":true},{"content":"The token name of the recognizer is the value of the <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> property of the <xref:System.Speech.Recognition.RecognizerInfo> object returned by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property of the recognizer. To get a collection of all the installed recognizers, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.  \n  \n Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.  \n  \n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \n  \n To configure the audio input, use one of the following methods:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[{"pos":[0,454],"content":"The token name of the recognizer is the value of the <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> property of the <xref:System.Speech.Recognition.RecognizerInfo> object returned by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property of the recognizer. To get a collection of all the installed recognizers, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","nodes":[{"content":"The token name of the recognizer is the value of the <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> property of the <xref:System.Speech.Recognition.RecognizerInfo> object returned by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property of the recognizer. To get a collection of all the installed recognizers, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","pos":[0,454],"nodes":[{"content":"The token name of the recognizer is the value of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizerInfo.Id%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerInfo&gt;</ph> object returned by the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A&gt;</ph> property of the recognizer.","pos":[0,296],"source":"The token name of the recognizer is the value of the <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> property of the <xref:System.Speech.Recognition.RecognizerInfo> object returned by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property of the recognizer."},{"content":"To get a collection of all the installed recognizers, use the static <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A&gt;</ph> method.","pos":[297,454],"source":" To get a collection of all the installed recognizers, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method."}]}]},{"pos":[461,610],"content":"Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.","nodes":[{"content":"Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.","pos":[0,149]}]},{"pos":[617,804],"content":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.","nodes":[{"content":"To load a grammar, call the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A&gt;</ph> method.","pos":[0,187],"source":"To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method."}]},{"pos":[811,874],"content":"To configure the audio input, use one of the following methods:","nodes":[{"content":"To configure the audio input, use one of the following methods:","pos":[0,63]}]},{"pos":[884,965],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>","nodes":[]},{"pos":[975,1063],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>","nodes":[]},{"pos":[1073,1147],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>","nodes":[]},{"pos":[1157,1235],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>","nodes":[]},{"pos":[1245,1325],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[]}],"pos":[24528,25872],"yaml":true,"extradata":"MT"},{"content":"The token name of the speech recognizer to use.","nodes":[{"pos":[0,47],"content":"The token name of the speech recognizer to use.","nodes":[{"content":"The token name of the speech recognizer to use.","pos":[0,47]}]}],"pos":[27571,27619],"yaml":true},{"content":"No speech recognizer with that token name is installed, or <code>recognizerId</code> is the empty string (\"\").","nodes":[{"pos":[0,110],"content":"No speech recognizer with that token name is installed, or <bpt id=\"p1\">&lt;code&gt;</bpt><ph id=\"ph1\">recognizerId</ph><ept id=\"p1\">&lt;/code&gt;</ept> is the empty string (\"\").","source":"No speech recognizer with that token name is installed, or <code>recognizerId</code> is the empty string (\"\")."}],"pos":[27796,27907],"yaml":true},{"content":"<code>recognizerId</code> is `null`.","nodes":[{"pos":[0,36],"content":"<ph id=\"ph1\">&lt;code&gt;recognizerId&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>recognizerId</code> is `null`."}],"pos":[28009,28046],"yaml":true},{"content":"Gets the format of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.","nodes":[{"pos":[0,122],"content":"Gets the format of the audio being received by the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph>.","source":"Gets the format of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>."}],"pos":[29038,29161],"yaml":true},{"content":"To configure the audio input, use one of the following methods:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[{"pos":[0,63],"content":"To configure the audio input, use one of the following methods:","nodes":[{"content":"To configure the audio input, use one of the following methods:","pos":[0,63]}]},{"pos":[73,154],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>","nodes":[]},{"pos":[164,252],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>","nodes":[]},{"pos":[262,336],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>","nodes":[]},{"pos":[346,424],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>","nodes":[]},{"pos":[434,514],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>","nodes":[]}],"pos":[29172,29699],"yaml":true,"extradata":"MT"},{"content":"The format of audio at the input to the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance, or `null` if the input is not configured or set to the null input.","nodes":[{"pos":[0,187],"content":"The format of audio at the input to the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> instance, or <ph id=\"ph2\">`null`</ph> if the input is not configured or set to the null input.","source":"The format of audio at the input to the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance, or `null` if the input is not configured or set to the null input."}],"pos":[30939,31127],"yaml":true},{"content":"Gets the level of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.","nodes":[{"pos":[0,121],"content":"Gets the level of the audio being received by the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph>.","source":"Gets the level of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>."}],"pos":[32205,32327],"yaml":true},{"content":"The value 0 represents silence, and 100 represents the maximum input volume.","nodes":[{"pos":[0,76],"content":"The value 0 represents silence, and 100 represents the maximum input volume.","nodes":[{"content":"The value 0 represents silence, and 100 represents the maximum input volume.","pos":[0,76]}]}],"pos":[32338,32415],"yaml":true,"extradata":"MT"},{"content":"The audio level of the input to the speech recognizer, from 0 through 100.","nodes":[{"pos":[0,74],"content":"The audio level of the input to the speech recognizer, from 0 through 100.","nodes":[{"content":"The audio level of the input to the speech recognizer, from 0 through 100.","pos":[0,74]}]}],"pos":[32525,32600],"yaml":true},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> reports the level of its audio input.","nodes":[{"pos":[0,124],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> reports the level of its audio input.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> reports the level of its audio input."}],"pos":[33716,33841],"yaml":true},{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.  \n  \n To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> property.  \n  \n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,211],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.","nodes":[{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.","pos":[0,211],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> raises this event multiple times per second.","pos":[0,105],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event multiple times per second."},{"content":"The frequency with which the event is raised depends on the computer on which the application is running.","pos":[106,211]}]}]},{"pos":[218,601],"content":"To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> property.","nodes":[{"content":"To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> property.","pos":[0,383],"nodes":[{"content":"To get the audio level at the time of the event, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ph>.","pos":[0,218],"source":"To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>."},{"content":"To get the current audio level of the input to the recognizer, use the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A&gt;</ph> property.","pos":[219,383],"source":" To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> property."}]}]},{"pos":[608,1071],"content":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create an <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,155],"source":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[156,249]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[250,336]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[337,463],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[33852,34930],"yaml":true,"extradata":"MT"},{"content":"Gets the current location in the audio stream being generated by the device that is providing input to the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.","nodes":[{"pos":[0,178],"content":"Gets the current location in the audio stream being generated by the device that is providing input to the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph>.","source":"Gets the current location in the audio stream being generated by the device that is providing input to the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>."}],"pos":[37166,37345],"yaml":true},{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the recognizer's position within its audio input. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.","nodes":[{"pos":[0,696],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the recognizer's position within its audio input. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.","nodes":[{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the recognizer's position within its audio input. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.","pos":[0,696],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A&gt;</ph> property references the input device's position in its generated audio stream.","pos":[0,156],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream."},{"content":"By contrast, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A&gt;</ph> property references the recognizer's position within its audio input.","pos":[157,327],"source":" By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the recognizer's position within its audio input."},{"content":"These positions can be different.","pos":[328,361]},{"content":"For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A&gt;</ph> property is less than the value of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A&gt;</ph> property.","pos":[362,696],"source":" For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property."}]}]}],"pos":[37356,38053],"yaml":true,"extradata":"MT"},{"content":"The current location in the audio stream being generated by the input device.","nodes":[{"pos":[0,77],"content":"The current location in the audio stream being generated by the input device.","nodes":[{"content":"The current location in the audio stream being generated by the input device.","pos":[0,77]}]}],"pos":[41426,41504],"yaml":true},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> detects a problem in the audio signal.","nodes":[{"pos":[0,125],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> detects a problem in the audio signal.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> detects a problem in the audio signal."}],"pos":[42677,42803],"yaml":true},{"content":"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  \n  \n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,226],"content":"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.","nodes":[{"content":"To get which problem occurred, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ph>.","pos":[0,226],"source":"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>."}]},{"pos":[233,705],"content":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create an <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,164],"source":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[165,258]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[259,345]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[346,472],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[42814,43524],"yaml":true,"extradata":"MT"},{"content":"Gets the state of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.","nodes":[{"pos":[0,121],"content":"Gets the state of the audio being received by the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph>.","source":"Gets the state of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>."}],"pos":[46179,46301],"yaml":true},{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property represents the audio state with a member of the <xref:System.Speech.Recognition.AudioState> enumeration.","nodes":[{"pos":[0,188],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property represents the audio state with a member of the <xref:System.Speech.Recognition.AudioState> enumeration.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A&gt;</ph> property represents the audio state with a member of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.AudioState&gt;</ph> enumeration.","pos":[0,188],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property represents the audio state with a member of the <xref:System.Speech.Recognition.AudioState> enumeration."}]}],"pos":[46312,46501],"yaml":true,"extradata":"MT"},{"content":"The state of the audio input to the speech recognizer.","nodes":[{"pos":[0,54],"content":"The state of the audio input to the speech recognizer.","nodes":[{"content":"The state of the audio input to the speech recognizer.","pos":[0,54]}]}],"pos":[46668,46723],"yaml":true},{"content":"Raised when the state changes in the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.","nodes":[{"pos":[0,136],"content":"Raised when the state changes in the audio being received by the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph>.","source":"Raised when the state changes in the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>."}],"pos":[47839,47976],"yaml":true},{"content":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.  \n  \n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,488],"content":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.","nodes":[{"content":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.","pos":[0,488],"nodes":[{"content":"To get the audio state at the time of the event, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ph>.","pos":[0,218],"source":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>."},{"content":"To get the current audio state of the input to the recognizer, use the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A&gt;</ph> property.","pos":[219,383],"source":" To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property."},{"content":"For more information about audio state, see the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioState&gt;</ph> enumeration.","pos":[384,488],"source":" For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration."}]}]},{"pos":[495,958],"content":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create an <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,155],"source":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[156,249]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[250,336]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[337,463],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[47987,48950],"yaml":true,"extradata":"MT"},{"content":"Gets or sets the time interval during which a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> accepts input containing only background noise, before finalizing recognition.","nodes":[{"pos":[0,195],"content":"Gets or sets the time interval during which a <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> accepts input containing only background noise, before finalizing recognition.","source":"Gets or sets the time interval during which a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> accepts input containing only background noise, before finalizing recognition."}],"pos":[53238,53434],"yaml":true},{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. The recognizer classifies as background noise any non-silence input that does not match the initial rule of any of the recognizer's loaded and enabled speech recognition grammars. If the recognizer receives only background noise and silence within the babble timeout interval, then the recognizer finalizes that recognition operation.  \n  \n-   For asynchronous recognition operations, the recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, where the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=fullName> property is `true`, and the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName> property is `null`.  \n  \n-   For synchronous recognition operations and emulation, the recognizer returns `null`, instead of a valid <xref:System.Speech.Recognition.RecognitionResult>.  \n  \n If the babble timeout period is set to 0, the recognizer does not perform a babble timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.","nodes":[{"pos":[0,417],"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. The recognizer classifies as background noise any non-silence input that does not match the initial rule of any of the recognizer's loaded and enabled speech recognition grammars. If the recognizer receives only background noise and silence within the babble timeout interval, then the recognizer finalizes that recognition operation.","nodes":[{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. The recognizer classifies as background noise any non-silence input that does not match the initial rule of any of the recognizer's loaded and enabled speech recognition grammars. If the recognizer receives only background noise and silence within the babble timeout interval, then the recognizer finalizes that recognition operation.","pos":[0,417],"nodes":[{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech.","pos":[0,82]},{"content":"The recognizer classifies as background noise any non-silence input that does not match the initial rule of any of the recognizer's loaded and enabled speech recognition grammars.","pos":[83,262]},{"content":"If the recognizer receives only background noise and silence within the babble timeout interval, then the recognizer finalizes that recognition operation.","pos":[263,417]}]}]},{"pos":[427,833],"content":"For asynchronous recognition operations, the recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> event, where the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=fullName&gt;</ph> property is <ph id=\"ph3\">`true`</ph>, and the <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName&gt;</ph> property is <ph id=\"ph5\">`null`</ph>.","source":"For asynchronous recognition operations, the recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, where the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=fullName> property is `true`, and the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName> property is `null`."},{"pos":[843,998],"content":"For synchronous recognition operations and emulation, the recognizer returns <ph id=\"ph1\">`null`</ph>, instead of a valid <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph>.","source":"For synchronous recognition operations and emulation, the recognizer returns `null`, instead of a valid <xref:System.Speech.Recognition.RecognitionResult>."},{"pos":[1005,1180],"content":"If the babble timeout period is set to 0, the recognizer does not perform a babble timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.","nodes":[{"content":"If the babble timeout period is set to 0, the recognizer does not perform a babble timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.","pos":[0,175],"nodes":[{"content":"If the babble timeout period is set to 0, the recognizer does not perform a babble timeout check.","pos":[0,97]},{"content":"The timeout interval can be any non-negative value.","pos":[98,149]},{"content":"The default is 0 seconds.","pos":[150,175]}]}]}],"pos":[53445,54634],"yaml":true,"extradata":"MT"},{"content":"The duration of the time interval.","nodes":[{"pos":[0,34],"content":"The duration of the time interval.","nodes":[{"content":"The duration of the time interval.","pos":[0,34]}]}],"pos":[59549,59584],"yaml":true},{"content":"This property is set to less than 0 seconds.","nodes":[{"pos":[0,44],"content":"This property is set to less than 0 seconds.","nodes":[{"content":"This property is set to less than 0 seconds.","pos":[0,44]}]}],"pos":[59789,59834],"yaml":true},{"content":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object and releases resources used during the session.","nodes":[{"pos":[0,138],"content":"Disposes the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> object and releases resources used during the session.","source":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object and releases resources used during the session."}],"pos":[60892,61031],"yaml":true},{"content":"`true` to release both managed and unmanaged resources; `false` to release only unmanaged resources.","nodes":[{"pos":[0,100],"content":"<ph id=\"ph1\">`true`</ph> to release both managed and unmanaged resources; <ph id=\"ph2\">`false`</ph> to release only unmanaged resources.","source":"`true` to release both managed and unmanaged resources; `false` to release only unmanaged resources."}],"pos":[61185,61288],"yaml":true},{"content":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object.","nodes":[{"pos":[0,91],"content":"Disposes the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> object.","source":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object."}],"pos":[62348,62440],"yaml":true},{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition.","nodes":[{"pos":[0,117],"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition.","nodes":[{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition.","pos":[0,117]}]}],"pos":[63683,63801],"yaml":true},{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.  \n  \n The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"pos":[0,400],"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.","nodes":[{"content":"The speech recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events as if the recognition operation is not emulated.","pos":[0,400],"source":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated."}]},{"pos":[407,881],"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[0,474],"nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase.","pos":[0,131]},{"content":"For more information about this type of comparison, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration values <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> and <ph id=\"ph3\">&lt;xref:System.Globalization.CompareOptions.IgnoreWidth&gt;</ph>.","pos":[132,374],"source":" For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>."},{"content":"The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[375,474]}]}]}],"pos":[63812,64698],"yaml":true,"extradata":"MT"},{"content":"The input for the recognition operation.","nodes":[{"pos":[0,40],"content":"The input for the recognition operation.","nodes":[{"content":"The input for the recognition operation.","pos":[0,40]}]}],"pos":[70711,70752],"yaml":true},{"content":"The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled.","nodes":[{"pos":[0,120],"content":"The result for the recognition operation, or <ph id=\"ph1\">`null`</ph> if the operation is not successful or the recognizer is not enabled.","source":"The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled."}],"pos":[70839,70960],"yaml":true},{"content":"The recognizer has no speech recognition grammars loaded.","nodes":[{"pos":[0,57],"content":"The recognizer has no speech recognition grammars loaded.","nodes":[{"content":"The recognizer has no speech recognition grammars loaded.","pos":[0,57]}]}],"pos":[71164,71222],"yaml":true},{"content":"<code>inputText</code> is `null`.","nodes":[{"pos":[0,33],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>inputText</code> is `null`."}],"pos":[71324,71358],"yaml":true},{"content":"<code>inputText</code> is the empty string (\"\").","nodes":[{"pos":[0,48],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is the empty string (\"\").","source":"<code>inputText</code> is the empty string (\"\")."}],"pos":[71452,71501],"yaml":true},{"content":"Emulates input of specific words to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","nodes":[{"pos":[0,245],"content":"Emulates input of specific words to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","nodes":[{"content":"Emulates input of specific words to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","pos":[0,245]}]}],"pos":[72887,73133],"yaml":true},{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.  \n  \n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizer always ignores the character width and never ignores the Kana type. The recognizer also ignores new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,400],"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.","nodes":[{"content":"The speech recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events as if the recognition operation is not emulated.","pos":[0,400],"source":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated."}]},{"pos":[407,1006],"content":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizer always ignores the character width and never ignores the Kana type. The recognizer also ignores new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,87],"source":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[88,293],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizer always ignores the character width and never ignores the Kana type.","pos":[294,376]},{"content":"The recognizer also ignores new lines and extra white space and treats punctuation as literal input.","pos":[377,477]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[478,599],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[73144,74155],"yaml":true,"extradata":"MT"},{"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"pos":[0,77],"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"content":"An array of word units that contains the input for the recognition operation.","pos":[0,77]}]}],"pos":[74472,74550],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[74642,74774],"yaml":true},{"content":"The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled.","nodes":[{"pos":[0,120],"content":"The result for the recognition operation, or <ph id=\"ph1\">`null`</ph> if the operation is not successful or the recognizer is not enabled.","source":"The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled."}],"pos":[74861,74982],"yaml":true},{"content":"The recognizer has no speech recognition grammars loaded.","nodes":[{"pos":[0,57],"content":"The recognizer has no speech recognition grammars loaded.","nodes":[{"content":"The recognizer has no speech recognition grammars loaded.","pos":[0,57]}]}],"pos":[75186,75244],"yaml":true},{"content":"<code>wordUnits</code> is `null`.","nodes":[{"pos":[0,33],"content":"<ph id=\"ph1\">&lt;code&gt;wordUnits&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>wordUnits</code> is `null`."}],"pos":[75346,75380],"yaml":true},{"content":"<code>wordUnits</code> contains one or more `null` elements.","nodes":[{"pos":[0,60],"content":"<ph id=\"ph1\">&lt;code&gt;wordUnits&lt;/code&gt;</ph> contains one or more <ph id=\"ph2\">`null`</ph> elements.","source":"<code>wordUnits</code> contains one or more `null` elements."}],"pos":[75474,75535],"yaml":true},{"content":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.","nodes":[{"pos":[0,262],"content":"<ph id=\"ph1\">&lt;code&gt;compareOptions&lt;/code&gt;</ph> contains the <ph id=\"ph2\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"&gt;&lt;/xref&gt;</ph>, <ph id=\"ph3\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"&gt;&lt;/xref&gt;</ph>, or <ph id=\"ph4\">&lt;xref href=\"System.Globalization.CompareOptions.StringSort\"&gt;&lt;/xref&gt;</ph> flag.","source":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag."}],"pos":[75637,75900],"yaml":true},{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"pos":[0,240],"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","pos":[0,240]}]}],"pos":[77145,77386],"yaml":true},{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.  \n  \n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizer always ignores the character width and never ignores the Kana type. The recognizer also ignores new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,400],"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.","nodes":[{"content":"The speech recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events as if the recognition operation is not emulated.","pos":[0,400],"source":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated."}]},{"pos":[407,1006],"content":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizer always ignores the character width and never ignores the Kana type. The recognizer also ignores new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,87],"source":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[88,293],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizer always ignores the character width and never ignores the Kana type.","pos":[294,376]},{"content":"The recognizer also ignores new lines and extra white space and treats punctuation as literal input.","pos":[377,477]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[478,599],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[77397,78408],"yaml":true,"extradata":"MT"},{"content":"The input phrase for the recognition operation.","nodes":[{"pos":[0,47],"content":"The input phrase for the recognition operation.","nodes":[{"content":"The input phrase for the recognition operation.","pos":[0,47]}]}],"pos":[78652,78700],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[78792,78924],"yaml":true},{"content":"The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled.","nodes":[{"pos":[0,120],"content":"The result for the recognition operation, or <ph id=\"ph1\">`null`</ph> if the operation is not successful or the recognizer is not enabled.","source":"The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled."}],"pos":[79011,79132],"yaml":true},{"content":"The recognizer has no speech recognition grammars loaded.","nodes":[{"pos":[0,57],"content":"The recognizer has no speech recognition grammars loaded.","nodes":[{"content":"The recognizer has no speech recognition grammars loaded.","pos":[0,57]}]}],"pos":[79336,79394],"yaml":true},{"content":"<code>inputText</code> is `null`.","nodes":[{"pos":[0,33],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>inputText</code> is `null`."}],"pos":[79496,79530],"yaml":true},{"content":"<code>inputText</code> is the empty string (\"\").","nodes":[{"pos":[0,48],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is the empty string (\"\").","source":"<code>inputText</code> is the empty string (\"\")."}],"pos":[79624,79673],"yaml":true},{"content":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.","nodes":[{"pos":[0,262],"content":"<ph id=\"ph1\">&lt;code&gt;compareOptions&lt;/code&gt;</ph> contains the <ph id=\"ph2\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"&gt;&lt;/xref&gt;</ph>, <ph id=\"ph3\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"&gt;&lt;/xref&gt;</ph>, or <ph id=\"ph4\">&lt;xref href=\"System.Globalization.CompareOptions.StringSort\"&gt;&lt;/xref&gt;</ph> flag.","source":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag."}],"pos":[79775,80038],"yaml":true},{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition.","nodes":[{"pos":[0,118],"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition.","nodes":[{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition.","pos":[0,118]}]}],"pos":[81157,81276],"yaml":true},{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \n  \n The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"pos":[0,574],"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.","nodes":[{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.","pos":[0,574],"nodes":[{"content":"The speech recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events as if the recognition operation is not emulated.","pos":[0,400],"source":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated."},{"content":"When the recognizer completes the asynchronous recognition operation, it raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event.","pos":[401,574],"source":" When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event."}]}]},{"pos":[581,1055],"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[0,474],"nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase.","pos":[0,131]},{"content":"For more information about this type of comparison, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration values <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> and <ph id=\"ph3\">&lt;xref:System.Globalization.CompareOptions.IgnoreWidth&gt;</ph>.","pos":[132,374],"source":" For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>."},{"content":"The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[375,474]}]}]}],"pos":[81287,82347],"yaml":true,"extradata":"MT"},{"content":"The input for the recognition operation.","nodes":[{"pos":[0,40],"content":"The input for the recognition operation.","nodes":[{"content":"The input for the recognition operation.","pos":[0,40]}]}],"pos":[89565,89606],"yaml":true},{"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","nodes":[{"pos":[0,143],"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","nodes":[{"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","pos":[0,143]}]}],"pos":[89815,89959],"yaml":true},{"content":"<code>inputText</code> is `null`.","nodes":[{"pos":[0,33],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>inputText</code> is `null`."}],"pos":[90061,90095],"yaml":true},{"content":"<code>inputText</code> is the empty string (\"\").","nodes":[{"pos":[0,48],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is the empty string (\"\").","source":"<code>inputText</code> is the empty string (\"\")."}],"pos":[90189,90238],"yaml":true},{"content":"Emulates input of specific words to the speech recognizer, using an array of <xref href=\"System.Speech.Recognition.RecognizedWordUnit\"></xref> objects in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","nodes":[{"pos":[0,327],"content":"Emulates input of specific words to the speech recognizer, using an array of <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.RecognizedWordUnit\"&gt;&lt;/xref&gt;</ph> objects in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","source":"Emulates input of specific words to the speech recognizer, using an array of <xref href=\"System.Speech.Recognition.RecognizedWordUnit\"></xref> objects in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars."}],"pos":[91654,91982],"yaml":true},{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \n  \n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treat punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,574],"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.","nodes":[{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.","pos":[0,574],"nodes":[{"content":"The speech recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events as if the recognition operation is not emulated.","pos":[0,400],"source":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated."},{"content":"When the recognizer completes the asynchronous recognition operation, it raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event.","pos":[401,574],"source":" When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event."}]}]},{"pos":[581,1178],"content":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treat punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,87],"source":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[88,293],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizers always ignore the character width and never ignore the Kana type.","pos":[294,375]},{"content":"The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[376,475]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[476,597],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[91993,93176],"yaml":true,"extradata":"MT"},{"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"pos":[0,77],"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"content":"An array of word units that contains the input for the recognition operation.","pos":[0,77]}]}],"pos":[93459,93537],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[93629,93761],"yaml":true},{"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","nodes":[{"pos":[0,143],"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","nodes":[{"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","pos":[0,143]}]}],"pos":[93970,94114],"yaml":true},{"content":"<code>wordUnits</code> is `null`.","nodes":[{"pos":[0,33],"content":"<ph id=\"ph1\">&lt;code&gt;wordUnits&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>wordUnits</code> is `null`."}],"pos":[94216,94250],"yaml":true},{"content":"<code>wordUnits</code> contains one or more `null` elements.","nodes":[{"pos":[0,60],"content":"<ph id=\"ph1\">&lt;code&gt;wordUnits&lt;/code&gt;</ph> contains one or more <ph id=\"ph2\">`null`</ph> elements.","source":"<code>wordUnits</code> contains one or more `null` elements."}],"pos":[94344,94405],"yaml":true},{"content":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.","nodes":[{"pos":[0,262],"content":"<ph id=\"ph1\">&lt;code&gt;compareOptions&lt;/code&gt;</ph> contains the <ph id=\"ph2\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"&gt;&lt;/xref&gt;</ph>, <ph id=\"ph3\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"&gt;&lt;/xref&gt;</ph>, or <ph id=\"ph4\">&lt;xref href=\"System.Globalization.CompareOptions.StringSort\"&gt;&lt;/xref&gt;</ph> flag.","source":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag."}],"pos":[94507,94770],"yaml":true},{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"pos":[0,241],"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"content":"Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","pos":[0,241]}]}],"pos":[96045,96287],"yaml":true},{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \n  \n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treat punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,574],"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.","nodes":[{"content":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.","pos":[0,574],"nodes":[{"content":"The speech recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events as if the recognition operation is not emulated.","pos":[0,400],"source":"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated."},{"content":"When the recognizer completes the asynchronous recognition operation, it raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event.","pos":[401,574],"source":" When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event."}]}]},{"pos":[581,1178],"content":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treat punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,87],"source":"The recognizer uses `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[88,293],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizers always ignore the character width and never ignore the Kana type.","pos":[294,375]},{"content":"The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[376,475]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[476,597],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[96298,97481],"yaml":true,"extradata":"MT"},{"content":"The input phrase for the recognition operation.","nodes":[{"pos":[0,47],"content":"The input phrase for the recognition operation.","nodes":[{"content":"The input phrase for the recognition operation.","pos":[0,47]}]}],"pos":[97691,97739],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[97831,97963],"yaml":true},{"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","nodes":[{"pos":[0,143],"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","nodes":[{"content":"The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.","pos":[0,143]}]}],"pos":[98172,98316],"yaml":true},{"content":"<code>inputText</code> is `null`.","nodes":[{"pos":[0,33],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>inputText</code> is `null`."}],"pos":[98418,98452],"yaml":true},{"content":"<code>inputText</code> is the empty string (\"\").","nodes":[{"pos":[0,48],"content":"<ph id=\"ph1\">&lt;code&gt;inputText&lt;/code&gt;</ph> is the empty string (\"\").","source":"<code>inputText</code> is the empty string (\"\")."}],"pos":[98546,98595],"yaml":true},{"content":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.","nodes":[{"pos":[0,262],"content":"<ph id=\"ph1\">&lt;code&gt;compareOptions&lt;/code&gt;</ph> contains the <ph id=\"ph2\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"&gt;&lt;/xref&gt;</ph>, <ph id=\"ph3\">&lt;xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"&gt;&lt;/xref&gt;</ph>, or <ph id=\"ph4\">&lt;xref href=\"System.Globalization.CompareOptions.StringSort\"&gt;&lt;/xref&gt;</ph> flag.","source":"<code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag."}],"pos":[98697,98960],"yaml":true},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finalizes an asynchronous recognition operation of emulated input.","nodes":[{"pos":[0,153],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> finalizes an asynchronous recognition operation of emulated input.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finalizes an asynchronous recognition operation of emulated input."}],"pos":[100033,100187],"yaml":true},{"content":"Each <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when it finalizes the asynchronous operation.  \n  \n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operation can raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.  \n  \n If emulated recognition was successful, you can access the recognition result using the either of the following:  \n  \n-   The <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \n  \n-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event.  \n  \n If emulated recognition was not successful, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event is not raised and the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> will be null.  \n  \n <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> derives from <xref:System.ComponentModel.AsyncCompletedEventArgs>.  \n  \n <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>.  \n  \n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,346],"content":"Each <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when it finalizes the asynchronous operation.","nodes":[{"content":"Each <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when it finalizes the asynchronous operation.","pos":[0,346],"nodes":[{"content":"Each <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> method begins an asynchronous recognition operation.","pos":[0,139],"source":"Each <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> raises the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event when it finalizes the asynchronous operation.","pos":[140,346],"source":" The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when it finalizes the asynchronous operation."}]}]},{"pos":[353,948],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operation can raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.","nodes":[{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operation can raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.","pos":[0,595],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> operation can raise the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events.","pos":[0,429],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operation can raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event is the last such event that the recognizer raises for a given operation.","pos":[430,595],"source":" The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation."}]}]},{"pos":[955,1067],"content":"If emulated recognition was successful, you can access the recognition result using the either of the following:","nodes":[{"content":"If emulated recognition was successful, you can access the recognition result using the either of the following:","pos":[0,112]}]},{"pos":[1077,1362],"content":"The <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A&gt;</ph> property in the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ph> object in the handler for the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event.","pos":[0,285],"source":"The <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event."}]},{"pos":[1372,1621],"content":"<xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event.","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A&gt;</ph> property in the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ph> object in the handler for the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event.","pos":[0,249],"source":"<xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event."}]},{"pos":[1628,1869],"content":"If emulated recognition was not successful, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event is not raised and the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> will be null.","nodes":[{"content":"If emulated recognition was not successful, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event is not raised and the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A&gt;</ph> will be null.","pos":[0,241],"source":"If emulated recognition was not successful, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event is not raised and the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> will be null."}]},{"pos":[1876,2010],"content":"<xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> derives from <xref:System.ComponentModel.AsyncCompletedEventArgs>.","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ph> derives from <ph id=\"ph2\">&lt;xref:System.ComponentModel.AsyncCompletedEventArgs&gt;</ph>.","pos":[0,134],"source":"<xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> derives from <xref:System.ComponentModel.AsyncCompletedEventArgs>."}]},{"pos":[2017,2143],"content":"<xref:System.Speech.Recognition.SpeechRecognizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>.","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ph> derives from <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs&gt;</ph>.","pos":[0,126],"source":"<xref:System.Speech.Recognition.SpeechRecognizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>."}]},{"pos":[2150,2621],"content":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create an <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,163],"source":"When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[164,257]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[258,344]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[345,471],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[100198,102838],"yaml":true,"extradata":"MT"},{"content":"Gets or sets the interval of silence that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> will accept at the end of unambiguous input before finalizing a recognition operation.","nodes":[{"pos":[0,203],"content":"Gets or sets the interval of silence that the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> will accept at the end of unambiguous input before finalizing a recognition operation.","source":"Gets or sets the interval of silence that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> will accept at the end of unambiguous input before finalizing a recognition operation."}],"pos":[107408,107612],"yaml":true},{"content":"The speech recognizer uses this timeout interval when the recognition input is unambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.  \n  \n This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 150 milliseconds.  \n  \n To set the timeout interval for ambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property.","nodes":[{"pos":[0,287],"content":"The speech recognizer uses this timeout interval when the recognition input is unambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","nodes":[{"content":"The speech recognizer uses this timeout interval when the recognition input is unambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","pos":[0,287],"nodes":[{"content":"The speech recognizer uses this timeout interval when the recognition input is unambiguous.","pos":[0,91]},{"content":"For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","pos":[92,287]}]}]},{"pos":[294,533],"content":"This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 150 milliseconds.","nodes":[{"content":"This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 150 milliseconds.","pos":[0,239],"nodes":[{"content":"This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation.","pos":[0,137]},{"content":"The timeout interval can be from 0 seconds to 10 seconds, inclusive.","pos":[138,206]},{"content":"The default is 150 milliseconds.","pos":[207,239]}]}]},{"pos":[540,693],"content":"To set the timeout interval for ambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property.","nodes":[{"content":"To set the timeout interval for ambiguous input, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A&gt;</ph> property.","pos":[0,153],"source":"To set the timeout interval for ambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property."}]}],"pos":[107623,108331],"yaml":true,"extradata":"MT"},{"content":"The duration of the interval of silence.","nodes":[{"pos":[0,40],"content":"The duration of the interval of silence.","nodes":[{"content":"The duration of the interval of silence.","pos":[0,40]}]}],"pos":[108461,108502],"yaml":true},{"content":"This property is set to less than 0 seconds or greater than 10 seconds.","nodes":[{"pos":[0,71],"content":"This property is set to less than 0 seconds or greater than 10 seconds.","nodes":[{"content":"This property is set to less than 0 seconds or greater than 10 seconds.","pos":[0,71]}]}],"pos":[108711,108783],"yaml":true},{"content":"Gets or sets the interval of silence that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> will accept at the end of ambiguous input before finalizing a recognition operation.","nodes":[{"pos":[0,201],"content":"Gets or sets the interval of silence that the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> will accept at the end of ambiguous input before finalizing a recognition operation.","source":"Gets or sets the interval of silence that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> will accept at the end of ambiguous input before finalizing a recognition operation."}],"pos":[109882,110084],"yaml":true},{"content":"The speech recognizer uses this timeout interval when the recognition input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.  \n  \n This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 500 milliseconds.  \n  \n To set the timeout interval for unambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property.","nodes":[{"pos":[0,285],"content":"The speech recognizer uses this timeout interval when the recognition input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","nodes":[{"content":"The speech recognizer uses this timeout interval when the recognition input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","pos":[0,285],"nodes":[{"content":"The speech recognizer uses this timeout interval when the recognition input is ambiguous.","pos":[0,89]},{"content":"For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","pos":[90,285]}]}]},{"pos":[292,531],"content":"This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 500 milliseconds.","nodes":[{"content":"This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 500 milliseconds.","pos":[0,239],"nodes":[{"content":"This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation.","pos":[0,137]},{"content":"The timeout interval can be from 0 seconds to 10 seconds, inclusive.","pos":[138,206]},{"content":"The default is 500 milliseconds.","pos":[207,239]}]}]},{"pos":[538,684],"content":"To set the timeout interval for unambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property.","nodes":[{"content":"To set the timeout interval for unambiguous input, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A&gt;</ph> property.","pos":[0,146],"source":"To set the timeout interval for unambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property."}]}],"pos":[110095,110794],"yaml":true,"extradata":"MT"},{"content":"The duration of the interval of silence.","nodes":[{"pos":[0,40],"content":"The duration of the interval of silence.","nodes":[{"content":"The duration of the interval of silence.","pos":[0,40]}]}],"pos":[110933,110974],"yaml":true},{"content":"This property is set to less than 0 seconds or greater than 10 seconds.","nodes":[{"pos":[0,71],"content":"This property is set to less than 0 seconds or greater than 10 seconds.","nodes":[{"content":"This property is set to less than 0 seconds or greater than 10 seconds.","pos":[0,71]}]}],"pos":[111192,111264],"yaml":true},{"content":"Gets a collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that are loaded in this <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance.","nodes":[{"pos":[0,192],"content":"Gets a collection of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> objects that are loaded in this <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> instance.","source":"Gets a collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that are loaded in this <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance."}],"pos":[112255,112448],"yaml":true},{"content":"The collection of <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects.","nodes":[{"pos":[0,81],"content":"The collection of <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> objects.","source":"The collection of <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects."}],"pos":[113455,113537],"yaml":true},{"content":"Gets or sets the time interval during which a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> accepts input containing only silence before finalizing recognition.","nodes":[{"pos":[0,185],"content":"Gets or sets the time interval during which a <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> accepts input containing only silence before finalizing recognition.","source":"Gets or sets the time interval during which a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> accepts input containing only silence before finalizing recognition."}],"pos":[114678,114864],"yaml":true},{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. If the recognizer input is silence during the initial silence timeout period, then the recognizer finalizes that recognition operation.  \n  \n-   For asynchronous recognition operations and emulation, the recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, where the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=fullName> property is `true`, and the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName> property is `null`.  \n  \n-   For synchronous recognition operations and emulation, the recognizer returns `null`, instead of a valid <xref:System.Speech.Recognition.RecognitionResult>.  \n  \n If the initial silence timeout interval is set to 0, the recognizer does not perform an initial silence timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.","nodes":[{"pos":[0,218],"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. If the recognizer input is silence during the initial silence timeout period, then the recognizer finalizes that recognition operation.","nodes":[{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. If the recognizer input is silence during the initial silence timeout period, then the recognizer finalizes that recognition operation.","pos":[0,218],"nodes":[{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech.","pos":[0,82]},{"content":"If the recognizer input is silence during the initial silence timeout period, then the recognizer finalizes that recognition operation.","pos":[83,218]}]}]},{"pos":[228,656],"content":"For asynchronous recognition operations and emulation, the recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> event, where the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=fullName&gt;</ph> property is <ph id=\"ph3\">`true`</ph>, and the <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName&gt;</ph> property is <ph id=\"ph5\">`null`</ph>.","source":"For asynchronous recognition operations and emulation, the recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, where the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=fullName> property is `true`, and the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName> property is `null`."},{"pos":[666,821],"content":"For synchronous recognition operations and emulation, the recognizer returns <ph id=\"ph1\">`null`</ph>, instead of a valid <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph>.","source":"For synchronous recognition operations and emulation, the recognizer returns `null`, instead of a valid <xref:System.Speech.Recognition.RecognitionResult>."},{"pos":[828,1024],"content":"If the initial silence timeout interval is set to 0, the recognizer does not perform an initial silence timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.","nodes":[{"content":"If the initial silence timeout interval is set to 0, the recognizer does not perform an initial silence timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.","pos":[0,196],"nodes":[{"content":"If the initial silence timeout interval is set to 0, the recognizer does not perform an initial silence timeout check.","pos":[0,118]},{"content":"The timeout interval can be any non-negative value.","pos":[119,170]},{"content":"The default is 0 seconds.","pos":[171,196]}]}]}],"pos":[114875,115908],"yaml":true,"extradata":"MT"},{"content":"The duration of the interval of silence.","nodes":[{"pos":[0,40],"content":"The duration of the interval of silence.","nodes":[{"content":"The duration of the interval of silence.","pos":[0,40]}]}],"pos":[120850,120891],"yaml":true},{"content":"This property is set to less than 0 seconds.","nodes":[{"pos":[0,44],"content":"This property is set to less than 0 seconds.","nodes":[{"content":"This property is set to less than 0 seconds.","pos":[0,44]}]}],"pos":[121104,121149],"yaml":true},{"content":"Returns information for all of the installed speech recognizers on the current system.","nodes":[{"pos":[0,86],"content":"Returns information for all of the installed speech recognizers on the current system.","nodes":[{"content":"Returns information for all of the installed speech recognizers on the current system.","pos":[0,86]}]}],"pos":[122216,122303],"yaml":true},{"content":"To get information about the current recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property.","nodes":[{"pos":[0,141],"content":"To get information about the current recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property.","nodes":[{"content":"To get information about the current recognizer, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A&gt;</ph> property.","pos":[0,141],"source":"To get information about the current recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property."}]}],"pos":[122314,122456],"yaml":true,"extradata":"MT"},{"content":"A read-only collection of the <xref href=\"System.Speech.Recognition.RecognizerInfo\"></xref> objects that describe the installed recognizers.","nodes":[{"pos":[0,140],"content":"A read-only collection of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.RecognizerInfo\"&gt;&lt;/xref&gt;</ph> objects that describe the installed recognizers.","source":"A read-only collection of the <xref href=\"System.Speech.Recognition.RecognizerInfo\"></xref> objects that describe the installed recognizers."}],"pos":[124761,124902],"yaml":true},{"content":"Synchronously loads a <xref href=\"System.Speech.Recognition.Grammar\"></xref> object.","nodes":[{"pos":[0,84],"content":"Synchronously loads a <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> object.","source":"Synchronously loads a <xref href=\"System.Speech.Recognition.Grammar\"></xref> object."}],"pos":[126125,126210],"yaml":true},{"content":"The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  \n  \n If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \n  \n When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.  \n  \n To load a <xref:System.Speech.Recognition.Grammar> object asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.","nodes":[{"pos":[0,487],"content":"The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.","nodes":[{"content":"The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.","pos":[0,487],"nodes":[{"content":"The recognizer throws an exception if the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer.","pos":[0,183],"source":"The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer."},{"content":"You cannot load the same <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object into multiple instances of <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph>.","pos":[184,341],"source":" You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>."},{"content":"Instead, create a new <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object for each <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> instance.","pos":[342,487],"source":" Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance."}]}]},{"pos":[494,730],"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","nodes":[{"content":"If the recognizer is running, applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[0,236],"source":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar."}]},{"pos":[737,886],"content":"When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.","nodes":[{"content":"When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.","pos":[0,149],"nodes":[{"content":"When you load a grammar, it is enabled by default.","pos":[0,50]},{"content":"To disable a loaded grammar, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar.Enabled%2A&gt;</ph> property.","pos":[51,149],"source":" To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property."}]}]},{"pos":[893,1059],"content":"To load a <xref:System.Speech.Recognition.Grammar> object asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.","nodes":[{"content":"To load a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object asynchronously, use the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A&gt;</ph> method.","pos":[0,166],"source":"To load a <xref:System.Speech.Recognition.Grammar> object asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method."}]}],"pos":[126221,127289],"yaml":true,"extradata":"MT"},{"content":"The grammar object to load.","nodes":[{"pos":[0,27],"content":"The grammar object to load.","nodes":[{"content":"The grammar object to load.","pos":[0,27]}]}],"pos":[129104,129132],"yaml":true},{"content":"<code>Grammar</code> is `null`.","nodes":[{"pos":[0,31],"content":"<ph id=\"ph1\">&lt;code&gt;Grammar&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>Grammar</code> is `null`."}],"pos":[129323,129355],"yaml":true},{"content":"<code>Grammar</code> is not in a valid state.","nodes":[{"pos":[0,45],"content":"<ph id=\"ph1\">&lt;code&gt;Grammar&lt;/code&gt;</ph> is not in a valid state.","source":"<code>Grammar</code> is not in a valid state."}],"pos":[129465,129511],"yaml":true},{"content":"Asynchronously loads a speech recognition grammar.","nodes":[{"pos":[0,50],"content":"Asynchronously loads a speech recognition grammar.","nodes":[{"content":"Asynchronously loads a speech recognition grammar.","pos":[0,50]}]}],"pos":[130663,130714],"yaml":true},{"content":"When the recognizer completes loading a <xref:System.Speech.Recognition.Grammar> object, it raises a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> event. The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  \n  \n If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \n  \n When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.  \n  \n To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> method.","nodes":[{"pos":[0,673],"content":"When the recognizer completes loading a <xref:System.Speech.Recognition.Grammar> object, it raises a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> event. The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.","nodes":[{"content":"When the recognizer completes loading a <xref:System.Speech.Recognition.Grammar> object, it raises a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> event. The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.","pos":[0,673],"nodes":[{"content":"When the recognizer completes loading a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object, it raises a <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted&gt;</ph> event.","pos":[0,185],"source":"When the recognizer completes loading a <xref:System.Speech.Recognition.Grammar> object, it raises a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> event."},{"content":"The recognizer throws an exception if the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer.","pos":[186,369],"source":" The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer."},{"content":"You cannot load the same <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object into multiple instances of <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph>.","pos":[370,527],"source":" You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>."},{"content":"Instead, create a new <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object for each <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> instance.","pos":[528,673],"source":" Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance."}]}]},{"pos":[680,916],"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","nodes":[{"content":"If the recognizer is running, applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[0,236],"source":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar."}]},{"pos":[923,1072],"content":"When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.","nodes":[{"content":"When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.","pos":[0,149],"nodes":[{"content":"When you load a grammar, it is enabled by default.","pos":[0,50]},{"content":"To disable a loaded grammar, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar.Enabled%2A&gt;</ph> property.","pos":[51,149],"source":" To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property."}]}]},{"pos":[1079,1218],"content":"To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> method.","nodes":[{"content":"To load a speech recognition grammar synchronously, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A&gt;</ph> method.","pos":[0,139],"source":"To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> method."}]}],"pos":[130725,131952],"yaml":true,"extradata":"MT"},{"content":"The speech recognition grammar to load.","nodes":[{"pos":[0,39],"content":"The speech recognition grammar to load.","nodes":[{"content":"The speech recognition grammar to load.","pos":[0,39]}]}],"pos":[132148,132188],"yaml":true},{"content":"<code>Grammar</code> is `null`.","nodes":[{"pos":[0,31],"content":"<ph id=\"ph1\">&lt;code&gt;Grammar&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>Grammar</code> is `null`."}],"pos":[132384,132416],"yaml":true},{"content":"<code>Grammar</code> is not in a valid state.","nodes":[{"pos":[0,45],"content":"<ph id=\"ph1\">&lt;code&gt;Grammar&lt;/code&gt;</ph> is not in a valid state.","source":"<code>Grammar</code> is not in a valid state."}],"pos":[132526,132572],"yaml":true},{"content":"The asynchronous operation was canceled.","nodes":[{"pos":[0,40],"content":"The asynchronous operation was canceled.","nodes":[{"content":"The asynchronous operation was canceled.","pos":[0,40]}]}],"pos":[132684,132725],"yaml":true},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finishes the asynchronous loading of a <xref href=\"System.Speech.Recognition.Grammar\"></xref> object.","nodes":[{"pos":[0,188],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> finishes the asynchronous loading of a <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> object.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finishes the asynchronous loading of a <xref href=\"System.Speech.Recognition.Grammar\"></xref> object."}],"pos":[133768,133957],"yaml":true},{"content":"The recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method initiates an asynchronous operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it completes the operation. To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property.  \n  \n If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \n  \n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,704],"content":"The recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method initiates an asynchronous operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it completes the operation. To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property.","nodes":[{"content":"The recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method initiates an asynchronous operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it completes the operation. To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property.","pos":[0,704],"nodes":[{"content":"The recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A&gt;</ph> method initiates an asynchronous operation.","pos":[0,137],"source":"The recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method initiates an asynchronous operation."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> raises this event when it completes the operation.","pos":[138,249],"source":" The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it completes the operation."},{"content":"To get the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object that the recognizer loaded, use the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A&gt;</ph> property of the associated <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ph>.","pos":[250,509],"source":" To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>."},{"content":"To get the current <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects the recognizer has loaded, use the recognizer's <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A&gt;</ph> property.","pos":[510,704],"source":" To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property."}]}]},{"pos":[711,947],"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","nodes":[{"content":"If the recognizer is running, applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[0,236],"source":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar."}]},{"pos":[954,1419],"content":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,157],"source":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[158,251]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[252,338]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[339,465],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[133968,135394],"yaml":true,"extradata":"MT"},{"content":"Gets or sets the maximum number of alternate recognition results that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> returns for each recognition operation.","nodes":[{"pos":[0,184],"content":"Gets or sets the maximum number of alternate recognition results that the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> returns for each recognition operation.","source":"Gets or sets the maximum number of alternate recognition results that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> returns for each recognition operation."}],"pos":[140673,140858],"yaml":true},{"content":"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent possible interpretations of the input.  \n  \n The default value for <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> is 10.","nodes":[{"pos":[0,280],"content":"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent possible interpretations of the input.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult.Alternates%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> class contains the collection of <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizedPhrase&gt;</ph> objects that represent possible interpretations of the input.","pos":[0,280],"source":"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent possible interpretations of the input."}]},{"pos":[287,389],"content":"The default value for <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> is 10.","nodes":[{"content":"The default value for <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A&gt;</ph> is 10.","pos":[0,102],"source":"The default value for <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> is 10."}]}],"pos":[140869,141263],"yaml":true,"extradata":"MT"},{"content":"The number of alternate results to return.","nodes":[{"pos":[0,42],"content":"The number of alternate results to return.","nodes":[{"content":"The number of alternate results to return.","pos":[0,42]}]}],"pos":[141381,141424],"yaml":true},{"content":"<xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\"></xref> is set to a value less than 0.","nodes":[{"pos":[0,115],"content":"<ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\"&gt;&lt;/xref&gt;</ph> is set to a value less than 0.","source":"<xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\"></xref> is set to a value less than 0."}],"pos":[141629,141745],"yaml":true},{"content":"Returns the values of settings for the recognizer.","nodes":[{"pos":[0,50],"content":"Returns the values of settings for the recognizer.","nodes":[{"content":"Returns the values of settings for the recognizer.","pos":[0,50]}]}],"pos":[142870,142921],"yaml":true},{"content":"Recognizer settings can contain string, 64-bit integer, or memory address data. The following table describes the settings that are defined for a Microsoft Speech API (SAPI)-compliant recognizer. The following settings must have the same range for each recognizer that supports the setting. A SAPI-compliant recognizer is not required to support these settings and can support other settings.  \n  \n|Name|Description|  \n|----------|-----------------|  \n|`ResourceUsage`|Specifies the recognizer's CPU consumption. The range is from 0 to 100. The default value is 50.|  \n|`ResponseSpeed`|Indicates the length of silence at the end of unambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000 milliseconds (ms). This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property.  Default = 150ms.|  \n|`ComplexResponseSpeed`|Indicates the length of silence at the end of ambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000ms. This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property. Default = 500ms.|  \n|`AdaptationOn`|Indicates whether adaptation of the acoustic model is ON (value = `1`) or OFF (value = `0`). The default value is `1` (ON).|  \n|`PersistedBackgroundAdaptation`|Indicates whether background adaptation is ON (value = `1`) or OFF (value = `0`), and persists the setting in the registry. The default value is `1` (ON).|  \n  \n To update a setting for the recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods.","nodes":[{"pos":[0,392],"content":"Recognizer settings can contain string, 64-bit integer, or memory address data. The following table describes the settings that are defined for a Microsoft Speech API (SAPI)-compliant recognizer. The following settings must have the same range for each recognizer that supports the setting. A SAPI-compliant recognizer is not required to support these settings and can support other settings.","nodes":[{"content":"Recognizer settings can contain string, 64-bit integer, or memory address data. The following table describes the settings that are defined for a Microsoft Speech API (SAPI)-compliant recognizer. The following settings must have the same range for each recognizer that supports the setting. A SAPI-compliant recognizer is not required to support these settings and can support other settings.","pos":[0,392],"nodes":[{"content":"Recognizer settings can contain string, 64-bit integer, or memory address data.","pos":[0,79]},{"content":"The following table describes the settings that are defined for a Microsoft Speech API (SAPI)-compliant recognizer.","pos":[80,195]},{"content":"The following settings must have the same range for each recognizer that supports the setting.","pos":[196,290]},{"content":"A SAPI-compliant recognizer is not required to support these settings and can support other settings.","pos":[291,392]}]}]},{"pos":[399,403],"content":"Name","nodes":[{"content":"Name","pos":[0,4]}]},{"pos":[404,415],"content":"Description","nodes":[{"content":"Description","pos":[0,11]}]},{"pos":[469,565],"content":"Specifies the recognizer's CPU consumption. The range is from 0 to 100. The default value is 50.","nodes":[{"content":"Specifies the recognizer's CPU consumption. The range is from 0 to 100. The default value is 50.","pos":[0,96],"nodes":[{"content":"Specifies the recognizer's CPU consumption.","pos":[0,43]},{"content":"The range is from 0 to 100.","pos":[44,71]},{"content":"The default value is 50.","pos":[72,96]}]}]},{"pos":[586,913],"content":"Indicates the length of silence at the end of unambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000 milliseconds (ms). This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property.  Default = 150ms.","nodes":[{"content":"Indicates the length of silence at the end of unambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000 milliseconds (ms). This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property.  Default = 150ms.","pos":[0,327],"nodes":[{"content":"Indicates the length of silence at the end of unambiguous input before the speech recognizer completes a recognition operation.","pos":[0,127]},{"content":"The range is from 0 to 10,000 milliseconds (ms).","pos":[128,176]},{"content":"This setting corresponds to the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A&gt;</ph> property.","pos":[177,309],"source":" This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property."},{"content":"Default = 150ms.","pos":[311,327]}]}]},{"pos":[941,1258],"content":"Indicates the length of silence at the end of ambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000ms. This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property. Default = 500ms.","nodes":[{"content":"Indicates the length of silence at the end of ambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000ms. This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property. Default = 500ms.","pos":[0,317],"nodes":[{"content":"Indicates the length of silence at the end of ambiguous input before the speech recognizer completes a recognition operation.","pos":[0,125]},{"content":"The range is from 0 to 10,000ms.","pos":[126,158]},{"content":"This setting corresponds to the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A&gt;</ph> property.","pos":[159,300],"source":" This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property."},{"content":"Default = 500ms.","pos":[301,317]}]}]},{"pos":[1278,1401],"content":"Indicates whether adaptation of the acoustic model is ON (value = `1`) or OFF (value = `0`). The default value is `1` (ON).","nodes":[{"content":"Indicates whether adaptation of the acoustic model is ON (value = <ph id=\"ph1\">`1`</ph>) or OFF (value = <ph id=\"ph2\">`0`</ph>).","pos":[0,92],"source":"Indicates whether adaptation of the acoustic model is ON (value = `1`) or OFF (value = `0`)."},{"content":"The default value is <ph id=\"ph1\">`1`</ph> (ON).","pos":[93,123],"source":" The default value is `1` (ON)."}]},{"pos":[1438,1592],"content":"Indicates whether background adaptation is ON (value = `1`) or OFF (value = `0`), and persists the setting in the registry. The default value is `1` (ON).","nodes":[{"content":"Indicates whether background adaptation is ON (value = <ph id=\"ph1\">`1`</ph>) or OFF (value = <ph id=\"ph2\">`0`</ph>), and persists the setting in the registry.","pos":[0,123],"source":"Indicates whether background adaptation is ON (value = `1`) or OFF (value = `0`), and persists the setting in the registry."},{"content":"The default value is <ph id=\"ph1\">`1`</ph> (ON).","pos":[124,154],"source":" The default value is `1` (ON)."}]},{"pos":[1600,1747],"content":"To update a setting for the recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods.","nodes":[{"content":"To update a setting for the recognizer, use one of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A&gt;</ph> methods.","pos":[0,147],"source":"To update a setting for the recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods."}]}],"pos":[142932,144692],"yaml":true,"extradata":"MT"},{"content":"The name of the setting to return.","nodes":[{"pos":[0,34],"content":"The name of the setting to return.","nodes":[{"content":"The name of the setting to return.","pos":[0,34]}]}],"pos":[146761,146796],"yaml":true},{"content":"The value of the setting.","nodes":[{"pos":[0,25],"content":"The value of the setting.","nodes":[{"content":"The value of the setting.","pos":[0,25]}]}],"pos":[146853,146879],"yaml":true},{"content":"<code>settingName</code> is `null`.","nodes":[{"pos":[0,35],"content":"<ph id=\"ph1\">&lt;code&gt;settingName&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>settingName</code> is `null`."}],"pos":[147081,147117],"yaml":true},{"content":"<code>settingName</code> is the empty string (\"\").","nodes":[{"pos":[0,50],"content":"<ph id=\"ph1\">&lt;code&gt;settingName&lt;/code&gt;</ph> is the empty string (\"\").","source":"<code>settingName</code> is the empty string (\"\")."}],"pos":[147211,147262],"yaml":true},{"content":"The recognizer does not have a setting by that name.","nodes":[{"pos":[0,52],"content":"The recognizer does not have a setting by that name.","nodes":[{"content":"The recognizer does not have a setting by that name.","pos":[0,52]}]}],"pos":[147402,147455],"yaml":true},{"content":"Performs a synchronous speech recognition operation.","nodes":[{"pos":[0,52],"content":"Performs a synchronous speech recognition operation.","nodes":[{"content":"Performs a synchronous speech recognition operation.","pos":[0,52]}]}],"pos":[148439,148492],"yaml":true},{"content":"This method performs a single recognition operation. The recognizer performs this operation against its loaded and enabled speech recognition grammars.  \n  \n During a call to this method, the recognizer can raise the following events:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \n  \n The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method.  \n  \n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> method returns a <xref:System.Speech.Recognition.RecognitionResult> object, or `null` if the operation is not successful.  \n  \n A synchronous recognition operation can fail for the following reasons:  \n  \n-   Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties.  \n  \n-   The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.  \n  \n To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods.","nodes":[{"pos":[0,151],"content":"This method performs a single recognition operation. The recognizer performs this operation against its loaded and enabled speech recognition grammars.","nodes":[{"content":"This method performs a single recognition operation. The recognizer performs this operation against its loaded and enabled speech recognition grammars.","pos":[0,151],"nodes":[{"content":"This method performs a single recognition operation.","pos":[0,52]},{"content":"The recognizer performs this operation against its loaded and enabled speech recognition grammars.","pos":[53,151]}]}]},{"pos":[158,234],"content":"During a call to this method, the recognizer can raise the following events:","nodes":[{"content":"During a call to this method, the recognizer can raise the following events:","pos":[0,76]}]},{"pos":[244,390],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","pos":[0,146],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>.","pos":[0,72],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>."},{"content":"Raised when the recognizer detects input that it can identify as speech.","pos":[74,146]}]}]},{"pos":[400,555],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","pos":[0,155],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>.","pos":[0,76],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>."},{"content":"Raised when input creates an ambiguous match with one of the active grammars.","pos":[78,155]}]}]},{"pos":[565,787],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","pos":[0,222],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph>.","pos":[0,160],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>."},{"content":"Raised when the recognizer finalizes a recognition operation.","pos":[161,222]}]}]},{"pos":[794,933],"content":"The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method.","nodes":[{"content":"The recognizer does not raise the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> event when using this method.","pos":[0,139],"source":"The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method."}]},{"pos":[940,1132],"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize&gt;</ph> method returns a <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object, or <ph id=\"ph3\">`null`</ph> if the operation is not successful.","source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> method returns a <xref:System.Speech.Recognition.RecognitionResult> object, or `null` if the operation is not successful."},{"pos":[1139,1210],"content":"A synchronous recognition operation can fail for the following reasons:","nodes":[{"content":"A synchronous recognition operation can fail for the following reasons:","pos":[0,71]}]},{"pos":[1220,1457],"content":"Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties.","nodes":[{"content":"Speech is not detected before the timeout intervals expire for the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A&gt;</ph> properties.","pos":[0,237],"source":"Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties."}]},{"pos":[1467,1608],"content":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.","nodes":[{"content":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects.","pos":[0,141],"source":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects."}]},{"pos":[1615,1750],"content":"To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods.","nodes":[{"content":"To perform asynchronous recognition, use one of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> methods.","pos":[0,135],"source":"To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods."}]}],"pos":[148503,150276],"yaml":true,"extradata":"MT"},{"content":"The recognition result for the input, or `null` if the operation is not successful or the recognizer is not enabled.","nodes":[{"pos":[0,116],"content":"The recognition result for the input, or <ph id=\"ph1\">`null`</ph> if the operation is not successful or the recognizer is not enabled.","source":"The recognition result for the input, or `null` if the operation is not successful or the recognizer is not enabled."}],"pos":[152062,152179],"yaml":true},{"content":"Performs a synchronous speech recognition operation with a specified initial silence timeout period.","nodes":[{"pos":[0,100],"content":"Performs a synchronous speech recognition operation with a specified initial silence timeout period.","nodes":[{"content":"Performs a synchronous speech recognition operation with a specified initial silence timeout period.","pos":[0,100]}]}],"pos":[153328,153429],"yaml":true},{"content":"If the speech recognition engine detects speech within the time interval specified by `initialSilenceTimeout` argument, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> performs a single recognition operation and then terminates.  The `initialSilenceTimeout` parameter supersedes the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> property.  \n  \n During a call to this method, the recognizer can raise the following events:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \n  \n The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method.  \n  \n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> method returns a <xref:System.Speech.Recognition.RecognitionResult> object, or `null` if the operation is not successful.  \n  \n A synchronous recognition operation can fail for the following reasons:  \n  \n-   Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or for the `initialSilenceTimeout` parameter.  \n  \n-   The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.  \n  \n To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods.","nodes":[{"pos":[0,427],"content":"If the speech recognition engine detects speech within the time interval specified by `initialSilenceTimeout` argument, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> performs a single recognition operation and then terminates.  The `initialSilenceTimeout` parameter supersedes the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> property.","nodes":[{"content":"If the speech recognition engine detects speech within the time interval specified by <ph id=\"ph1\">`initialSilenceTimeout`</ph> argument, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29&gt;</ph> performs a single recognition operation and then terminates.","pos":[0,268],"source":"If the speech recognition engine detects speech within the time interval specified by `initialSilenceTimeout` argument, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> performs a single recognition operation and then terminates."},{"content":"The <ph id=\"ph1\">`initialSilenceTimeout`</ph> parameter supersedes the recognizer's <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A&gt;</ph> property.","pos":[270,427],"source":"  The `initialSilenceTimeout` parameter supersedes the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> property."}]},{"pos":[434,510],"content":"During a call to this method, the recognizer can raise the following events:","nodes":[{"content":"During a call to this method, the recognizer can raise the following events:","pos":[0,76]}]},{"pos":[520,666],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","pos":[0,146],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>.","pos":[0,72],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>."},{"content":"Raised when the recognizer detects input that it can identify as speech.","pos":[74,146]}]}]},{"pos":[676,831],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","pos":[0,155],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>.","pos":[0,76],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>."},{"content":"Raised when input creates an ambiguous match with one of the active grammars.","pos":[78,155]}]}]},{"pos":[841,1063],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","pos":[0,222],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph>.","pos":[0,160],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>."},{"content":"Raised when the recognizer finalizes a recognition operation.","pos":[161,222]}]}]},{"pos":[1070,1209],"content":"The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method.","nodes":[{"content":"The recognizer does not raise the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> event when using this method.","pos":[0,139],"source":"The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method."}]},{"pos":[1216,1408],"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize&gt;</ph> method returns a <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object, or <ph id=\"ph3\">`null`</ph> if the operation is not successful.","source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> method returns a <xref:System.Speech.Recognition.RecognitionResult> object, or `null` if the operation is not successful."},{"pos":[1415,1486],"content":"A synchronous recognition operation can fail for the following reasons:","nodes":[{"content":"A synchronous recognition operation can fail for the following reasons:","pos":[0,71]}]},{"pos":[1496,1682],"content":"Speech is not detected before the timeout intervals expire for the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A&gt;</ph> or for the <ph id=\"ph2\">`initialSilenceTimeout`</ph> parameter.","source":"Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or for the `initialSilenceTimeout` parameter."},{"pos":[1692,1833],"content":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.","nodes":[{"content":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects.","pos":[0,141],"source":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects."}]},{"pos":[1840,1975],"content":"To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods.","nodes":[{"content":"To perform asynchronous recognition, use one of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> methods.","pos":[0,135],"source":"To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods."}]}],"pos":[153440,155438],"yaml":true,"extradata":"MT"},{"content":"The interval of time a speech recognizer accepts input containing only silence before finalizing recognition.","nodes":[{"pos":[0,109],"content":"The interval of time a speech recognizer accepts input containing only silence before finalizing recognition.","nodes":[{"content":"The interval of time a speech recognizer accepts input containing only silence before finalizing recognition.","pos":[0,109]}]}],"pos":[157136,157246],"yaml":true},{"content":"The recognition result for the input, or `null` if the operation is not successful or the recognizer is not enabled.","nodes":[{"pos":[0,116],"content":"The recognition result for the input, or <ph id=\"ph1\">`null`</ph> if the operation is not successful or the recognizer is not enabled.","source":"The recognition result for the input, or `null` if the operation is not successful or the recognizer is not enabled."}],"pos":[157333,157450],"yaml":true},{"content":"Performs a single, asynchronous speech recognition operation.","nodes":[{"pos":[0,61],"content":"Performs a single, asynchronous speech recognition operation.","nodes":[{"content":"Performs a single, asynchronous speech recognition operation.","pos":[0,61]}]}],"pos":[158554,158616],"yaml":true},{"content":"This method performs a single, asynchronous recognition operation. The recognizer performs the operation against its loaded and enabled speech recognition grammars.  \n  \n During a call to this method, the recognizer can raise the following events:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.  \n  \n To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation. If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`.  \n  \n To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods.","nodes":[{"pos":[0,164],"content":"This method performs a single, asynchronous recognition operation. The recognizer performs the operation against its loaded and enabled speech recognition grammars.","nodes":[{"content":"This method performs a single, asynchronous recognition operation. The recognizer performs the operation against its loaded and enabled speech recognition grammars.","pos":[0,164],"nodes":[{"content":"This method performs a single, asynchronous recognition operation.","pos":[0,66]},{"content":"The recognizer performs the operation against its loaded and enabled speech recognition grammars.","pos":[67,164]}]}]},{"pos":[171,247],"content":"During a call to this method, the recognizer can raise the following events:","nodes":[{"content":"During a call to this method, the recognizer can raise the following events:","pos":[0,76]}]},{"pos":[257,403],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","pos":[0,146],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>.","pos":[0,72],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>."},{"content":"Raised when the recognizer detects input that it can identify as speech.","pos":[74,146]}]}]},{"pos":[413,568],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","pos":[0,155],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>.","pos":[0,76],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>."},{"content":"Raised when input creates an ambiguous match with one of the active grammars.","pos":[78,155]}]}]},{"pos":[578,800],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","pos":[0,222],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph>.","pos":[0,160],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>."},{"content":"Raised when the recognizer finalizes a recognition operation.","pos":[161,222]}]}]},{"pos":[810,995],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.","pos":[0,185],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph>.","pos":[0,76],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>."},{"content":"Raised when a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> operation finishes.","pos":[77,185],"source":" Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes."}]}]},{"pos":[1002,1646],"content":"To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation. If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`.","nodes":[{"content":"To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event.","pos":[0,189],"source":"To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event."},{"content":"The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation.","pos":[190,310]},{"content":"If recognition was not successful, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A&gt;</ph> property on <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ph> object, which you can access in the handler for the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> event, will be <ph id=\"ph4\">`null`</ph>.","pos":[311,644],"source":" If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`."}]},{"pos":[1653,1782],"content":"To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods.","nodes":[{"content":"To perform synchronous recognition, use one of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A&gt;</ph> methods.","pos":[0,129],"source":"To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods."}]}],"pos":[158627,160426],"yaml":true,"extradata":"MT"},{"content":"Performs one or more asynchronous speech recognition operations.","nodes":[{"pos":[0,64],"content":"Performs one or more asynchronous speech recognition operations.","nodes":[{"content":"Performs one or more asynchronous speech recognition operations.","pos":[0,64]}]}],"pos":[168352,168417],"yaml":true},{"content":"If `mode` is <xref:System.Speech.Recognition.RecognizeMode.Multiple>, the recognizer continues performing asynchronous recognition operations until the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method is called.  \n  \n During a call to this method, the recognizer can raise the following events:  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \n  \n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.  \n  \n To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation. If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`.  \n  \n An asynchronous recognition operation can fail for the following reasons:  \n  \n-   Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties.  \n  \n-   The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.  \n  \n To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods.","nodes":[{"pos":[0,332],"content":"If <ph id=\"ph1\">`mode`</ph> is <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizeMode.Multiple&gt;</ph>, the recognizer continues performing asynchronous recognition operations until the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A&gt;</ph> or <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A&gt;</ph> method is called.","source":"If `mode` is <xref:System.Speech.Recognition.RecognizeMode.Multiple>, the recognizer continues performing asynchronous recognition operations until the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method is called."},{"pos":[339,415],"content":"During a call to this method, the recognizer can raise the following events:","nodes":[{"content":"During a call to this method, the recognizer can raise the following events:","pos":[0,76]}]},{"pos":[425,571],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.","pos":[0,146],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph>.","pos":[0,72],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>."},{"content":"Raised when the recognizer detects input that it can identify as speech.","pos":[74,146]}]}]},{"pos":[581,736],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.","pos":[0,155],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>.","pos":[0,76],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>."},{"content":"Raised when input creates an ambiguous match with one of the active grammars.","pos":[78,155]}]}]},{"pos":[746,968],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.","pos":[0,222],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph>.","pos":[0,160],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>."},{"content":"Raised when the recognizer finalizes a recognition operation.","pos":[161,222]}]}]},{"pos":[978,1163],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.","nodes":[{"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.","pos":[0,185],"nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph>.","pos":[0,76],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>."},{"content":"Raised when a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> operation finishes.","pos":[77,185],"source":" Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes."}]}]},{"pos":[1170,1814],"content":"To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation. If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`.","nodes":[{"content":"To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event.","pos":[0,189],"source":"To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event."},{"content":"The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation.","pos":[190,310]},{"content":"If recognition was not successful, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A&gt;</ph> property on <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ph> object, which you can access in the handler for the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> event, will be <ph id=\"ph4\">`null`</ph>.","pos":[311,644],"source":" If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`."}]},{"pos":[1821,1894],"content":"An asynchronous recognition operation can fail for the following reasons:","nodes":[{"content":"An asynchronous recognition operation can fail for the following reasons:","pos":[0,73]}]},{"pos":[1904,2141],"content":"Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties.","nodes":[{"content":"Speech is not detected before the timeout intervals expire for the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A&gt;</ph> properties.","pos":[0,237],"source":"Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties."}]},{"pos":[2151,2292],"content":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.","nodes":[{"content":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects.","pos":[0,141],"source":"The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects."}]},{"pos":[2299,2428],"content":"To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods.","nodes":[{"content":"To perform synchronous recognition, use one of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A&gt;</ph> methods.","pos":[0,129],"source":"To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods."}]}],"pos":[168428,170879],"yaml":true,"extradata":"MT"},{"content":"Indicates whether to perform one or multiple recognition operations.","nodes":[{"pos":[0,68],"content":"Indicates whether to perform one or multiple recognition operations.","nodes":[{"content":"Indicates whether to perform one or multiple recognition operations.","pos":[0,68]}]}],"pos":[177898,177967],"yaml":true},{"content":"Terminates asynchronous recognition without waiting for the current recognition operation to complete.","nodes":[{"pos":[0,102],"content":"Terminates asynchronous recognition without waiting for the current recognition operation to complete.","nodes":[{"content":"Terminates asynchronous recognition without waiting for the current recognition operation to complete.","pos":[0,102]}]}],"pos":[179112,179215],"yaml":true},{"content":"This method immediately finalizes asynchronous recognition. If the current asynchronous recognition operation is receiving input, the input is truncated and the operation completes with the existing input. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is canceled, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`. This method cancels asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \n  \n To stop asynchronous recognition without truncating the input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method.","nodes":[{"pos":[0,841],"content":"This method immediately finalizes asynchronous recognition. If the current asynchronous recognition operation is receiving input, the input is truncated and the operation completes with the existing input. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is canceled, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`. This method cancels asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.","nodes":[{"content":"This method immediately finalizes asynchronous recognition.","pos":[0,59]},{"content":"If the current asynchronous recognition operation is receiving input, the input is truncated and the operation completes with the existing input.","pos":[60,205]},{"content":"The recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event when an asynchronous operation is canceled, and sets the <ph id=\"ph3\">&lt;xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A&gt;</ph> property of the <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ph> to <ph id=\"ph5\">`true`</ph>.","pos":[206,610],"source":" The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is canceled, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`."},{"content":"This method cancels asynchronous operations initiated by the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> methods.","pos":[611,841],"source":" This method cancels asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods."}]},{"pos":[848,1005],"content":"To stop asynchronous recognition without truncating the input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method.","nodes":[{"content":"To stop asynchronous recognition without truncating the input, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A&gt;</ph> method.","pos":[0,157],"source":"To stop asynchronous recognition without truncating the input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method."}]}],"pos":[179226,180236],"yaml":true,"extradata":"MT"},{"content":"Stops asynchronous recognition after the current recognition operation completes.","nodes":[{"pos":[0,81],"content":"Stops asynchronous recognition after the current recognition operation completes.","nodes":[{"content":"Stops asynchronous recognition after the current recognition operation completes.","pos":[0,81]}]}],"pos":[187988,188070],"yaml":true},{"content":"This method finalizes asynchronous recognition without truncating input. If the current asynchronous recognition operation is receiving input, the recognizer continues accepting input until the current recognition operation is completed. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is stopped, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`. This method stops asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \n  \n To immediately cancel asynchronous recognition with only the existing input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> method.","nodes":[{"pos":[0,870],"content":"This method finalizes asynchronous recognition without truncating input. If the current asynchronous recognition operation is receiving input, the recognizer continues accepting input until the current recognition operation is completed. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is stopped, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`. This method stops asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.","nodes":[{"content":"This method finalizes asynchronous recognition without truncating input.","pos":[0,72]},{"content":"If the current asynchronous recognition operation is receiving input, the recognizer continues accepting input until the current recognition operation is completed.","pos":[73,237]},{"content":"The recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted&gt;</ph> event when an asynchronous operation is stopped, and sets the <ph id=\"ph3\">&lt;xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A&gt;</ph> property of the <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ph> to <ph id=\"ph5\">`true`</ph>.","pos":[238,641],"source":" The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is stopped, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`."},{"content":"This method stops asynchronous operations initiated by the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> methods.","pos":[642,870],"source":" This method stops asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods."}]},{"pos":[877,1050],"content":"To immediately cancel asynchronous recognition with only the existing input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> method.","nodes":[{"content":"To immediately cancel asynchronous recognition with only the existing input, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A&gt;</ph> method.","pos":[0,173],"source":"To immediately cancel asynchronous recognition with only the existing input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> method."}]}],"pos":[188081,189136],"yaml":true,"extradata":"MT"},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finalizes an asynchronous recognition operation.","nodes":[{"pos":[0,135],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> finalizes an asynchronous recognition operation.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finalizes an asynchronous recognition operation."}],"pos":[196873,197009],"yaml":true},{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method initiates an asynchronous recognition operation. When the recognizer finalizes the asynchronous operation, it raises this event.  \n  \n Using the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, you can access the <xref:System.Speech.Recognition.RecognitionResult> in the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object. If recognition was not successful, <xref:System.Speech.Recognition.RecognitionResult> will be `null`. To determine whether a timeout or an interruption in audio input caused recognition to fail, you can access the properties for <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, or <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  \n  \n See the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> class for more information.  \n  \n To obtain details on the best rejected recognition candidates, attach a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event.  \n  \n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,280],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method initiates an asynchronous recognition operation. When the recognizer finalizes the asynchronous operation, it raises this event.","nodes":[{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method initiates an asynchronous recognition operation. When the recognizer finalizes the asynchronous operation, it raises this event.","pos":[0,280],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> object's <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> method initiates an asynchronous recognition operation.","pos":[0,200],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method initiates an asynchronous recognition operation."},{"content":"When the recognizer finalizes the asynchronous operation, it raises this event.","pos":[201,280]}]}]},{"pos":[287,1021],"content":"Using the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, you can access the <xref:System.Speech.Recognition.RecognitionResult> in the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object. If recognition was not successful, <xref:System.Speech.Recognition.RecognitionResult> will be `null`. To determine whether a timeout or an interruption in audio input caused recognition to fail, you can access the properties for <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, or <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.","nodes":[{"content":"Using the handler for the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> event, you can access the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> in the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ph> object.","pos":[0,254],"source":"Using the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, you can access the <xref:System.Speech.Recognition.RecognitionResult> in the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object."},{"content":"If recognition was not successful, <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> will be <ph id=\"ph2\">`null`</ph>.","pos":[255,356],"source":" If recognition was not successful, <xref:System.Speech.Recognition.RecognitionResult> will be `null`."},{"content":"To determine whether a timeout or an interruption in audio input caused recognition to fail, you can access the properties for <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A&gt;</ph>, or <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A&gt;</ph>.","pos":[357,734],"source":" To determine whether a timeout or an interruption in audio input caused recognition to fail, you can access the properties for <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, or <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>."}]},{"pos":[1028,1124],"content":"See the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> class for more information.","nodes":[{"content":"See the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ph> class for more information.","pos":[0,96],"source":"See the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> class for more information."}]},{"pos":[1131,1308],"content":"To obtain details on the best rejected recognition candidates, attach a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event.","nodes":[{"content":"To obtain details on the best rejected recognition candidates, attach a handler for the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> event.","pos":[0,177],"source":"To obtain details on the best rejected recognition candidates, attach a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event."}]},{"pos":[1315,1778],"content":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,155],"source":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[156,249]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[250,336]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[337,463],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[197020,198809],"yaml":true,"extradata":"MT"},{"content":"Gets the current location of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> in the audio input that it is processing.","nodes":[{"pos":[0,145],"content":"Gets the current location of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> in the audio input that it is processing.","source":"Gets the current location of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> in the audio input that it is processing."}],"pos":[204144,204290],"yaml":true},{"content":"The audio position is specific to each speech recognizer. The zero value of an input stream is established when it is enabled.  \n  \n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's position within its audio input. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.","nodes":[{"pos":[0,126],"content":"The audio position is specific to each speech recognizer. The zero value of an input stream is established when it is enabled.","nodes":[{"content":"The audio position is specific to each speech recognizer. The zero value of an input stream is established when it is enabled.","pos":[0,126],"nodes":[{"content":"The audio position is specific to each speech recognizer.","pos":[0,57]},{"content":"The zero value of an input stream is established when it is enabled.","pos":[58,126]}]}]},{"pos":[133,882],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's position within its audio input. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.","nodes":[{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's position within its audio input. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.","pos":[0,749],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A&gt;</ph> property references the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> object's position within its audio input.","pos":[0,210],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's position within its audio input."},{"content":"By contrast, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A&gt;</ph> property references the input device's position in its generated audio stream.","pos":[211,380],"source":" By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream."},{"content":"These positions can be different.","pos":[381,414]},{"content":"For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A&gt;</ph> property is less than the value of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A&gt;</ph> property.","pos":[415,749],"source":" For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property."}]}]}],"pos":[204301,205188],"yaml":true,"extradata":"MT"},{"content":"The position of the recognizer in the audio input that it is processing.","nodes":[{"pos":[0,72],"content":"The position of the recognizer in the audio input that it is processing.","nodes":[{"content":"The position of the recognizer in the audio input that it is processing.","pos":[0,72]}]}],"pos":[205319,205392],"yaml":true},{"content":"Gets information about the current instance of <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.","nodes":[{"pos":[0,118],"content":"Gets information about the current instance of <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph>.","source":"Gets information about the current instance of <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>."}],"pos":[206506,206625],"yaml":true},{"content":"To get information about all of the installed speech recognizers for the current system, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","nodes":[{"pos":[0,185],"content":"To get information about all of the installed speech recognizers for the current system, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.","nodes":[{"content":"To get information about all of the installed speech recognizers for the current system, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A&gt;</ph> method.","pos":[0,185],"source":"To get information about all of the installed speech recognizers for the current system, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method."}]}],"pos":[206636,206822],"yaml":true,"extradata":"MT"},{"content":"Information about the current speech recognizer.","nodes":[{"pos":[0,48],"content":"Information about the current speech recognizer.","nodes":[{"content":"Information about the current speech recognizer.","pos":[0,48]}]}],"pos":[207991,208040],"yaml":true},{"content":"Raised when a running <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> pauses to accept modifications.","nodes":[{"pos":[0,124],"content":"Raised when a running <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> pauses to accept modifications.","source":"Raised when a running <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> pauses to accept modifications."}],"pos":[209196,209321],"yaml":true},{"content":"Applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine> before modifying its settings or its <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it is ready to accept modifications.  \n  \n For example, while the <xref:System.Speech.Recognition.SpeechRecognitionEngine> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects, and modify values for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> properties. For more information, see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.  \n  \n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,401],"content":"Applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine> before modifying its settings or its <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it is ready to accept modifications.","nodes":[{"content":"Applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine> before modifying its settings or its <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it is ready to accept modifications.","pos":[0,401],"nodes":[{"content":"Applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> to pause a running instance of <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> before modifying its settings or its <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects.","pos":[0,280],"source":"Applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine> before modifying its settings or its <xref:System.Speech.Recognition.Grammar> objects."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> raises this event when it is ready to accept modifications.","pos":[281,401],"source":" The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it is ready to accept modifications."}]}]},{"pos":[408,990],"content":"For example, while the <xref:System.Speech.Recognition.SpeechRecognitionEngine> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects, and modify values for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> properties. For more information, see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.","nodes":[{"content":"For example, while the <xref:System.Speech.Recognition.SpeechRecognitionEngine> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects, and modify values for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> properties. For more information, see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.","pos":[0,582],"nodes":[{"content":"For example, while the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> is paused, you can load, unload, enable, and disable <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects, and modify values for the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A&gt;</ph>, and <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A&gt;</ph> properties.","pos":[0,460],"source":"For example, while the <xref:System.Speech.Recognition.SpeechRecognitionEngine> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects, and modify values for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> properties."},{"content":"For more information, see the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[461,582],"source":" For more information, see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method."}]}]},{"pos":[997,1465],"content":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,160],"source":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[161,254]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[255,341]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[342,468],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[209332,210804],"yaml":true,"extradata":"MT"},{"content":"Requests that the recognizer pauses to update its state and provides an offset and a user token for the associated event.","nodes":[{"pos":[0,121],"content":"Requests that the recognizer pauses to update its state and provides an offset and a user token for the associated event.","nodes":[{"content":"Requests that the recognizer pauses to update its state and provides an offset and a user token for the associated event.","pos":[0,121]}]}],"pos":[217058,217180],"yaml":true},{"content":"The recognizer does not initiate the recognizer update request until the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> equals the current <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  \n  \n When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.","nodes":[{"pos":[0,302],"content":"The recognizer does not initiate the recognizer update request until the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A&gt;</ph> equals the current <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A&gt;</ph> plus <ph id=\"ph3\">`audioPositionAheadToRaiseUpdate`</ph>.","source":"The recognizer does not initiate the recognizer update request until the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> equals the current <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`."},{"pos":[309,644],"content":"When the recognizer generates the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached&gt;</ph> event, the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A&gt;</ph> property of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ph> contains the value of the <ph id=\"ph4\">`userToken`</ph> parameter.","source":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter."}],"pos":[217191,217840],"yaml":true,"extradata":"MT"},{"content":"User-defined information that contains information for the operation.","nodes":[{"pos":[0,69],"content":"User-defined information that contains information for the operation.","nodes":[{"content":"User-defined information that contains information for the operation.","pos":[0,69]}]}],"pos":[218042,218112],"yaml":true},{"content":"The offset from the current <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\"></xref> to delay the request.","nodes":[{"pos":[0,134],"content":"The offset from the current <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\"&gt;&lt;/xref&gt;</ph> to delay the request.","source":"The offset from the current <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\"></xref> to delay the request."}],"pos":[218201,218336],"yaml":true},{"content":"Requests that the recognizer pauses to update its state and provides a user token for the associated event.","nodes":[{"pos":[0,107],"content":"Requests that the recognizer pauses to update its state and provides a user token for the associated event.","nodes":[{"content":"Requests that the recognizer pauses to update its state and provides a user token for the associated event.","pos":[0,107]}]}],"pos":[219571,219679],"yaml":true},{"content":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.  \n  \n To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.","nodes":[{"pos":[0,335],"content":"When the recognizer generates the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached&gt;</ph> event, the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A&gt;</ph> property of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ph> contains the value of the <ph id=\"ph4\">`userToken`</ph> parameter.","source":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter."},{"pos":[342,478],"content":"To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.","nodes":[{"content":"To specify an audio position offset, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[0,136],"source":"To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method."}]}],"pos":[219690,220173],"yaml":true,"extradata":"MT"},{"content":"User-defined information that contains information for the operation.","nodes":[{"pos":[0,69],"content":"User-defined information that contains information for the operation.","nodes":[{"content":"User-defined information that contains information for the operation.","pos":[0,69]}]}],"pos":[220333,220403],"yaml":true},{"content":"Requests that the recognizer pauses to update its state.","nodes":[{"pos":[0,56],"content":"Requests that the recognizer pauses to update its state.","nodes":[{"content":"Requests that the recognizer pauses to update its state.","pos":[0,56]}]}],"pos":[221575,221632],"yaml":true},{"content":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> is `null`.  \n  \n To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.","nodes":[{"pos":[0,297],"content":"When the recognizer generates the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached&gt;</ph> event, the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A&gt;</ph> property of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ph> is <ph id=\"ph4\">`null`</ph>.","source":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> is `null`."},{"pos":[304,652],"content":"To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.","nodes":[{"content":"To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.","pos":[0,348],"nodes":[{"content":"To provide a user token, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[0,211],"source":"To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method."},{"content":"To specify an audio position offset, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[212,348],"source":" To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method."}]}]}],"pos":[221643,222300],"yaml":true,"extradata":"MT"},{"content":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from an audio stream.","nodes":[{"pos":[0,131],"content":"Configures the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> object to receive input from an audio stream.","source":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from an audio stream."}],"pos":[228595,228727],"yaml":true},{"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","nodes":[{"pos":[0,259],"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","nodes":[{"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","pos":[0,259],"nodes":[{"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input.","pos":[0,147]},{"content":"Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","pos":[148,259]}]}]}],"pos":[228738,228998],"yaml":true,"extradata":"MT"},{"content":"The audio input stream.","nodes":[{"pos":[0,23],"content":"The audio input stream.","nodes":[{"content":"The audio input stream.","pos":[0,23]}]}],"pos":[232648,232672],"yaml":true},{"content":"The format of the audio input.","nodes":[{"pos":[0,30],"content":"The format of the audio input.","nodes":[{"content":"The format of the audio input.","pos":[0,30]}]}],"pos":[232773,232804],"yaml":true},{"content":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from the default audio device.","nodes":[{"pos":[0,140],"content":"Configures the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> object to receive input from the default audio device.","source":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from the default audio device."}],"pos":[234004,234145],"yaml":true},{"content":"Disables the input to the speech recognizer.","nodes":[{"pos":[0,44],"content":"Disables the input to the speech recognizer.","nodes":[{"content":"Disables the input to the speech recognizer.","pos":[0,44]}]}],"pos":[238386,238431],"yaml":true},{"content":"Configure the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object for no input when using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods, or when taking a recognition engine temporarily off line.","nodes":[{"pos":[0,335],"content":"Configure the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object for no input when using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods, or when taking a recognition engine temporarily off line.","nodes":[{"content":"Configure the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> object for no input when using the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A&gt;</ph> and <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> methods, or when taking a recognition engine temporarily off line.","pos":[0,335],"source":"Configure the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object for no input when using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods, or when taking a recognition engine temporarily off line."}]}],"pos":[238442,238778],"yaml":true,"extradata":"MT"},{"content":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from a Waveform audio format (.wav) file.","nodes":[{"pos":[0,151],"content":"Configures the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> object to receive input from a Waveform audio format (.wav) file.","source":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from a Waveform audio format (.wav) file."}],"pos":[240047,240199],"yaml":true},{"content":"If the recognizer reaches the end of the input file during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","nodes":[{"pos":[0,257],"content":"If the recognizer reaches the end of the input file during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","nodes":[{"content":"If the recognizer reaches the end of the input file during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","pos":[0,257],"nodes":[{"content":"If the recognizer reaches the end of the input file during a recognition operation, the recognition operation finalizes with the available input.","pos":[0,145]},{"content":"Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","pos":[146,257]}]}]}],"pos":[240210,240468],"yaml":true,"extradata":"MT"},{"content":"The path of the file to use as input.","nodes":[{"pos":[0,37],"content":"The path of the file to use as input.","nodes":[{"content":"The path of the file to use as input.","pos":[0,37]}]}],"pos":[243395,243433],"yaml":true},{"content":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from a stream that contains Waveform audio format (.wav) data.","nodes":[{"pos":[0,172],"content":"Configures the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> object to receive input from a stream that contains Waveform audio format (.wav) data.","source":"Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from a stream that contains Waveform audio format (.wav) data."}],"pos":[244654,244827],"yaml":true},{"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","nodes":[{"pos":[0,259],"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","nodes":[{"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","pos":[0,259],"nodes":[{"content":"If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input.","pos":[0,147]},{"content":"Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.","pos":[148,259]}]}]}],"pos":[244838,245098],"yaml":true,"extradata":"MT"},{"content":"The stream containing the audio data.","nodes":[{"pos":[0,37],"content":"The stream containing the audio data.","nodes":[{"content":"The stream containing the audio data.","pos":[0,37]}]}],"pos":[245272,245310],"yaml":true},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> detects input that it can identify as speech.","nodes":[{"pos":[0,132],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> detects input that it can identify as speech.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> detects input that it can identify as speech."}],"pos":[246418,246551],"yaml":true},{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. When the <xref:System.Speech.Recognition.SpeechRecognitionEngine> performs a speech recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event when its algorithm identifies the input as speech. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event before it raises any of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> events.  \n  \n For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \n  \n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,1002],"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. When the <xref:System.Speech.Recognition.SpeechRecognitionEngine> performs a speech recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event when its algorithm identifies the input as speech. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event before it raises any of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> events.","nodes":[{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech. When the <xref:System.Speech.Recognition.SpeechRecognitionEngine> performs a speech recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event when its algorithm identifies the input as speech. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event before it raises any of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> events.","pos":[0,1002],"nodes":[{"content":"Each speech recognizer has an algorithm to distinguish between silence and speech.","pos":[0,82]},{"content":"When the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> performs a speech recognition operation, it raises the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph> event when its algorithm identifies the input as speech.","pos":[83,332],"source":" When the <xref:System.Speech.Recognition.SpeechRecognitionEngine> performs a speech recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event when its algorithm identifies the input as speech."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ph> object indicates location in the input stream where the recognizer detected speech.","pos":[333,578],"source":" The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> raises the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph> event before it raises any of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph>, or <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> events.","pos":[579,1002],"source":" The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event before it raises any of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> events."}]}]},{"pos":[1009,1357],"content":"For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.","nodes":[{"content":"For more information see the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> methods.","pos":[0,348],"source":"For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods."}]},{"pos":[1364,1823],"content":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,151],"source":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[152,245]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[246,332]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[333,459],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[246562,248392],"yaml":true,"extradata":"MT"},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> has recognized a word or words that may be a component of multiple complete phrases in a grammar.","nodes":[{"pos":[0,184],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> has recognized a word or words that may be a component of multiple complete phrases in a grammar.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> has recognized a word or words that may be a component of multiple complete phrases in a grammar."}],"pos":[252491,252676],"yaml":true},{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> generates numerous <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> events as it attempts to identify an input phrase. You can access the text of partially recognized phrases in the <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> event. Typically, handling these events is useful only for debugging.  \n  \n <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>.  \n  \n For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \n  \n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,586],"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> generates numerous <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> events as it attempts to identify an input phrase. You can access the text of partially recognized phrases in the <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> event. Typically, handling these events is useful only for debugging.","nodes":[{"content":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> generates numerous <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> events as it attempts to identify an input phrase. You can access the text of partially recognized phrases in the <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> event. Typically, handling these events is useful only for debugging.","pos":[0,586],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> generates numerous <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph> events as it attempts to identify an input phrase.","pos":[0,206],"source":"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> generates numerous <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> events as it attempts to identify an input phrase."},{"content":"You can access the text of partially recognized phrases in the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ph> object in the handler for the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph> event.","pos":[207,523],"source":" You can access the text of partially recognized phrases in the <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> event."},{"content":"Typically, handling these events is useful only for debugging.","pos":[524,586]}]}]},{"pos":[593,721],"content":"<xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>.","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ph> derives from <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs&gt;</ph>.","pos":[0,128],"source":"<xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>."}]},{"pos":[728,1180],"content":"For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.","nodes":[{"content":"For more information see the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A&gt;</ph> property and the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A&gt;</ph>, and <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A&gt;</ph> methods.","pos":[0,452],"source":"For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods."}]},{"pos":[1187,1650],"content":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,155],"source":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[156,249]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[250,336]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[337,463],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[252687,254346],"yaml":true,"extradata":"MT"},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> receives input that does not match any of its loaded and enabled <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects.","nodes":[{"pos":[0,215],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> receives input that does not match any of its loaded and enabled <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> objects.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> receives input that does not match any of its loaded and enabled <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects."}],"pos":[258794,259010],"yaml":true},{"content":"The recognizer raises this event if it determines that input does not match with sufficient confidence any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object. You can use the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event to retrieve recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> that were rejected and their <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> scores.  \n  \n If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods. You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.  \n  \n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,726],"content":"The recognizer raises this event if it determines that input does not match with sufficient confidence any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object. You can use the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event to retrieve recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> that were rejected and their <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> scores.","nodes":[{"content":"The recognizer raises this event if it determines that input does not match with sufficient confidence any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object. You can use the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event to retrieve recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> that were rejected and their <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> scores.","pos":[0,726],"nodes":[{"content":"The recognizer raises this event if it determines that input does not match with sufficient confidence any of its loaded and enabled <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects.","pos":[0,182],"source":"The recognizer raises this event if it determines that input does not match with sufficient confidence any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ph> contains the rejected <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object.","pos":[183,415],"source":" The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object."},{"content":"You can use the handler for the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> event to retrieve recognition <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult.Alternates%2A&gt;</ph> that were rejected and their <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A&gt;</ph> scores.","pos":[416,726],"source":" You can use the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event to retrieve recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> that were rejected and their <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> scores."}]}]},{"pos":[733,1442],"content":"If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods. You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.","nodes":[{"content":"If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods. You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.","pos":[0,709],"nodes":[{"content":"If your application is using a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A&gt;</ph> methods.","pos":[0,288],"source":"If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods."},{"content":"You can modify how the speech recognition responds to non-speech input using the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A&gt;</ph> properties.","pos":[289,709],"source":" You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties."}]}]},{"pos":[1449,1919],"content":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,162],"source":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[163,256]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[257,343]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[344,470],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[259021,260947],"yaml":true,"extradata":"MT"},{"content":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> receives input that matches any of its loaded and enabled <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects.","nodes":[{"pos":[0,208],"content":"Raised when the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> receives input that matches any of its loaded and enabled <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> objects.","source":"Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> receives input that matches any of its loaded and enabled <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects."}],"pos":[266108,266317],"yaml":true},{"content":"You can initiate a recognition operation using the one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event if it determines that input matches one of its loaded <xref:System.Speech.Recognition.Grammar> objects with a sufficient level of confidence to constitute recognition. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object. Handlers of <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events can obtain the recognized phrase as well as a list of recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> with lower confidence scores.  \n  \n If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods.  You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.  \n  \n When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise its <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. Any tasks specific to a particular grammar should always be performed by a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event.  \n  \n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,979],"content":"You can initiate a recognition operation using the one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event if it determines that input matches one of its loaded <xref:System.Speech.Recognition.Grammar> objects with a sufficient level of confidence to constitute recognition. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object. Handlers of <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events can obtain the recognized phrase as well as a list of recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> with lower confidence scores.","nodes":[{"content":"You can initiate a recognition operation using the one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event if it determines that input matches one of its loaded <xref:System.Speech.Recognition.Grammar> objects with a sufficient level of confidence to constitute recognition. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object. Handlers of <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events can obtain the recognized phrase as well as a list of recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> with lower confidence scores.","pos":[0,979],"nodes":[{"content":"You can initiate a recognition operation using the one of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A&gt;</ph> methods.","pos":[0,218],"source":"You can initiate a recognition operation using the one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods."},{"content":"The recognizer raises the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event if it determines that input matches one of its loaded <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects with a sufficient level of confidence to constitute recognition.","pos":[219,492],"source":" The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event if it determines that input matches one of its loaded <xref:System.Speech.Recognition.Grammar> objects with a sufficient level of confidence to constitute recognition."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ph> contains the accepted <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object.","pos":[493,725],"source":" The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object."},{"content":"Handlers of <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> events can obtain the recognized phrase as well as a list of recognition <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult.Alternates%2A&gt;</ph> with lower confidence scores.","pos":[726,979],"source":" Handlers of <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events can obtain the recognized phrase as well as a list of recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> with lower confidence scores."}]}]},{"pos":[986,1696],"content":"If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods.  You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.","nodes":[{"content":"If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods.  You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.","pos":[0,710],"nodes":[{"content":"If your application is using a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A&gt;</ph> methods.","pos":[0,288],"source":"If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods."},{"content":"You can modify how the speech recognition responds to non-speech input using the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A&gt;</ph> properties.","pos":[290,710],"source":"  You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties."}]}]},{"pos":[1703,2290],"content":"When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise its <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. Any tasks specific to a particular grammar should always be performed by a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event.","nodes":[{"content":"When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise its <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. Any tasks specific to a particular grammar should always be performed by a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event.","pos":[0,587],"nodes":[{"content":"When the recognizer receives input that matches a grammar, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object can raise its <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> event.","pos":[0,189],"source":"When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise its <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object's <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> event is raised prior to the speech recognizer's <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event.","pos":[190,431],"source":" The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event."},{"content":"Any tasks specific to a particular grammar should always be performed by a handler for the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> event.","pos":[432,587],"source":" Any tasks specific to a particular grammar should always be performed by a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event."}]}]},{"pos":[2297,2758],"content":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> delegate, you identify the method that will handle the event.","pos":[0,153],"source":"When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> delegate, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[154,247]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[248,334]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[335,461],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[266328,269095],"yaml":true,"extradata":"MT"},{"content":"Unloads all <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects from the recognizer.","nodes":[{"pos":[0,95],"content":"Unloads all <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> objects from the recognizer.","source":"Unloads all <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects from the recognizer."}],"pos":[274613,274709],"yaml":true},{"content":"If the recognizer is currently loading a <xref:System.Speech.Recognition.Grammar> asynchronously, this method waits until the <xref:System.Speech.Recognition.Grammar> is loaded, before it unloads all of the <xref:System.Speech.Recognition.Grammar> objects from the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  \n  \n To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> method.","nodes":[{"pos":[0,331],"content":"If the recognizer is currently loading a <xref:System.Speech.Recognition.Grammar> asynchronously, this method waits until the <xref:System.Speech.Recognition.Grammar> is loaded, before it unloads all of the <xref:System.Speech.Recognition.Grammar> objects from the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.","nodes":[{"content":"If the recognizer is currently loading a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> asynchronously, this method waits until the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> is loaded, before it unloads all of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects from the <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> instance.","pos":[0,331],"source":"If the recognizer is currently loading a <xref:System.Speech.Recognition.Grammar> asynchronously, this method waits until the <xref:System.Speech.Recognition.Grammar> is loaded, before it unloads all of the <xref:System.Speech.Recognition.Grammar> objects from the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance."}]},{"pos":[338,457],"content":"To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> method.","nodes":[{"content":"To unload a specific grammar, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A&gt;</ph> method.","pos":[0,119],"source":"To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> method."}]}],"pos":[274720,275182],"yaml":true,"extradata":"MT"},{"content":"Unloads a specified <xref href=\"System.Speech.Recognition.Grammar\"></xref> object from the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance.","nodes":[{"pos":[0,171],"content":"Unloads a specified <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> object from the <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> instance.","source":"Unloads a specified <xref href=\"System.Speech.Recognition.Grammar\"></xref> object from the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance."}],"pos":[279187,279359],"yaml":true},{"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance before loading, unloading,  enabling, or disabling a <xref:System.Speech.Recognition.Grammar> object. To unload all <xref:System.Speech.Recognition.Grammar> objects, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> method.","nodes":[{"pos":[0,474],"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance before loading, unloading,  enabling, or disabling a <xref:System.Speech.Recognition.Grammar> object. To unload all <xref:System.Speech.Recognition.Grammar> objects, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> method.","nodes":[{"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance before loading, unloading,  enabling, or disabling a <xref:System.Speech.Recognition.Grammar> object. To unload all <xref:System.Speech.Recognition.Grammar> objects, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> method.","pos":[0,474],"nodes":[{"content":"If the recognizer is running, applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A&gt;</ph> to pause the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> instance before loading, unloading,  enabling, or disabling a <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object.","pos":[0,316],"source":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance before loading, unloading,  enabling, or disabling a <xref:System.Speech.Recognition.Grammar> object."},{"content":"To unload all <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects, use the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A&gt;</ph> method.","pos":[317,474],"source":" To unload all <xref:System.Speech.Recognition.Grammar> objects, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> method."}]}]}],"pos":[279370,279845],"yaml":true,"extradata":"MT"},{"content":"The grammar object to unload.","nodes":[{"pos":[0,29],"content":"The grammar object to unload.","nodes":[{"content":"The grammar object to unload.","pos":[0,29]}]}],"pos":[282735,282765],"yaml":true},{"content":"<code>Grammar</code> is `null`.","nodes":[{"pos":[0,31],"content":"<ph id=\"ph1\">&lt;code&gt;Grammar&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>Grammar</code> is `null`."}],"pos":[282958,282990],"yaml":true},{"content":"The grammar is not loaded in this recognizer, or this recognizer is currently loading the grammar asynchronously.","nodes":[{"pos":[0,113],"content":"The grammar is not loaded in this recognizer, or this recognizer is currently loading the grammar asynchronously.","nodes":[{"content":"The grammar is not loaded in this recognizer, or this recognizer is currently loading the grammar asynchronously.","pos":[0,113]}]}],"pos":[283100,283214],"yaml":true},{"content":"Updates the specified setting for the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> with the specified integer value.","nodes":[{"pos":[0,142],"content":"Updates the specified setting for the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"&gt;&lt;/xref&gt;</ph> with the specified integer value.","source":"Updates the specified setting for the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> with the specified integer value."}],"pos":[284405,284548],"yaml":true},{"content":"With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings. See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings.","nodes":[{"pos":[0,462],"content":"With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings. See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings.","nodes":[{"content":"With the exception of <ph id=\"ph1\">`PersistedBackgroundAdaptation`</ph>, property values set using the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A&gt;</ph> method remain in effect only for the current instance of <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph>, after which they revert to their default settings.","pos":[0,334],"source":"With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings."},{"content":"See <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A&gt;</ph> for descriptions of supported settings.","pos":[335,462],"source":" See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings."}]}],"pos":[284559,285022],"yaml":true,"extradata":"MT"},{"content":"The name of the setting to update.","nodes":[{"pos":[0,34],"content":"The name of the setting to update.","nodes":[{"content":"The name of the setting to update.","pos":[0,34]}]}],"pos":[288153,288188],"yaml":true},{"content":"The new value for the setting.","nodes":[{"pos":[0,30],"content":"The new value for the setting.","nodes":[{"content":"The new value for the setting.","pos":[0,30]}]}],"pos":[288255,288286],"yaml":true},{"content":"<code>settingName</code> is `null`.","nodes":[{"pos":[0,35],"content":"<ph id=\"ph1\">&lt;code&gt;settingName&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>settingName</code> is `null`."}],"pos":[288489,288525],"yaml":true},{"content":"<code>settingName</code> is the empty string (\"\").","nodes":[{"pos":[0,50],"content":"<ph id=\"ph1\">&lt;code&gt;settingName&lt;/code&gt;</ph> is the empty string (\"\").","source":"<code>settingName</code> is the empty string (\"\")."}],"pos":[288619,288670],"yaml":true},{"content":"The recognizer does not have a setting by that name.","nodes":[{"pos":[0,52],"content":"The recognizer does not have a setting by that name.","nodes":[{"content":"The recognizer does not have a setting by that name.","pos":[0,52]}]}],"pos":[288810,288863],"yaml":true},{"content":"Updates the specified speech recognition engine setting with the specified string value.","nodes":[{"pos":[0,88],"content":"Updates the specified speech recognition engine setting with the specified string value.","nodes":[{"content":"Updates the specified speech recognition engine setting with the specified string value.","pos":[0,88]}]}],"pos":[290060,290149],"yaml":true},{"content":"With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings. See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings.","nodes":[{"pos":[0,462],"content":"With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings. See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings.","nodes":[{"content":"With the exception of <ph id=\"ph1\">`PersistedBackgroundAdaptation`</ph>, property values set using the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A&gt;</ph> method remain in effect only for the current instance of <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph>, after which they revert to their default settings.","pos":[0,334],"source":"With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings."},{"content":"See <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A&gt;</ph> for descriptions of supported settings.","pos":[335,462],"source":" See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings."}]}],"pos":[290160,290623],"yaml":true,"extradata":"MT"},{"content":"The name of the setting to update.","nodes":[{"pos":[0,34],"content":"The name of the setting to update.","nodes":[{"content":"The name of the setting to update.","pos":[0,34]}]}],"pos":[290808,290843],"yaml":true},{"content":"The new value for the setting.","nodes":[{"pos":[0,30],"content":"The new value for the setting.","nodes":[{"content":"The new value for the setting.","pos":[0,30]}]}],"pos":[290911,290942],"yaml":true},{"content":"<code>settingName</code> is `null`.","nodes":[{"pos":[0,35],"content":"<ph id=\"ph1\">&lt;code&gt;settingName&lt;/code&gt;</ph> is <ph id=\"ph2\">`null`</ph>.","source":"<code>settingName</code> is `null`."}],"pos":[291145,291181],"yaml":true},{"content":"<code>settingName</code> is the empty string (\"\").","nodes":[{"pos":[0,50],"content":"<ph id=\"ph1\">&lt;code&gt;settingName&lt;/code&gt;</ph> is the empty string (\"\").","source":"<code>settingName</code> is the empty string (\"\")."}],"pos":[291275,291326],"yaml":true},{"content":"The recognizer does not have a setting by that name.","nodes":[{"pos":[0,52],"content":"The recognizer does not have a setting by that name.","nodes":[{"content":"The recognizer does not have a setting by that name.","pos":[0,52]}]}],"pos":[291466,291519],"yaml":true}],"content":"### YamlMime:ManagedReference\nitems:\n- uid: System.Speech.Recognition.SpeechRecognitionEngine\n  commentId: T:System.Speech.Recognition.SpeechRecognitionEngine\n  id: SpeechRecognitionEngine\n  children:\n  - System.Speech.Recognition.SpeechRecognitionEngine.#ctor\n  - System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)\n  - System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)\n  - System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)\n  - System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat\n  - System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel\n  - System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated\n  - System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\n  - System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred\n  - System.Speech.Recognition.SpeechRecognitionEngine.AudioState\n  - System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged\n  - System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout\n  - System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)\n  - System.Speech.Recognition.SpeechRecognitionEngine.Dispose\n  - System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)\n  - System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)\n  - System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted\n  - System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout\n  - System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  - System.Speech.Recognition.SpeechRecognitionEngine.Grammars\n  - System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout\n  - System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers\n  - System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)\n  - System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  - System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted\n  - System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\n  - System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)\n  - System.Speech.Recognition.SpeechRecognitionEngine.Recognize\n  - System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo\n  - System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached\n  - System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  - System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)\n  - System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate\n  - System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)\n  - System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice\n  - System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull\n  - System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)\n  - System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)\n  - System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected\n  - System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized\n  - System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected\n  - System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\n  - System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars\n  - System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)\n  - System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)\n  - System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)\n  langs:\n  - csharp\n  name: SpeechRecognitionEngine\n  nameWithType: SpeechRecognitionEngine\n  fullName: System.Speech.Recognition.SpeechRecognitionEngine\n  type: Class\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Provides the means to access and manage an in-process speech recognition engine.\n  remarks: \"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.  \\n  \\n This class is for running speech recognition engines in-process, and provides control over various aspects of speech recognition, as follows:  \\n  \\n-   To create an in-process speech recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> constructors.  \\n  \\n-   To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> methods, and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property.  \\n  \\n-   To configure the input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> method.  \\n  \\n-   To perform speech recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method.  \\n  \\n-   To modify how recognition handles silence or unexpected input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.  \\n  \\n-   To change the number of alternates the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.  \\n  \\n-   To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. The recognizer uses more than one thread to perform tasks.  \\n  \\n-   To emulate input to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \\n  \\n The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object is for the sole use of the process that instantiated the object. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer> shares a single recognizer with any application that wants to use it.  \\n  \\n> [!NOTE]\\n>  Always call <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. Because this example uses the `Multiple` mode of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method, it performs recognition until you close the console window or stop debugging.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Create an in-process speech recognizer for the en-US locale.  \\n      using (  \\n      SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(  \\n          new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Add a handler for the speech recognized event.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous, continuous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Recognized text: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: 'public class SpeechRecognitionEngine : IDisposable'\n  inheritance:\n  - System.Object\n  implements:\n  - System.IDisposable\n  inheritedMembers:\n  - System.Object.Equals(System.Object)\n  - System.Object.Equals(System.Object,System.Object)\n  - System.Object.GetHashCode\n  - System.Object.GetType\n  - System.Object.MemberwiseClone\n  - System.Object.ReferenceEquals(System.Object,System.Object)\n  - System.Object.ToString\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor\n  id: '#ctor'\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechRecognitionEngine()\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine()\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine()\n  type: Constructor\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class using the default speech recognizer for the system.\n  remarks: \"Before the speech recognizer can begin speech recognition, you must load at least one recognition grammar and configure the input for the recognizer.  \\n  \\n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \\n  \\n To configure the audio input, use one of the following methods:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>\"\n  syntax:\n    content: public SpeechRecognitionEngine ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.#ctor*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)\n  id: '#ctor(System.Globalization.CultureInfo)'\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechRecognitionEngine(CultureInfo)\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine(CultureInfo)\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine(CultureInfo)\n  type: Constructor\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class using the default speech recognizer for a specified locale.\n  remarks: \"Microsoft Windows and the System.Speech API accept all valid language-country codes. To perform speech recognition using the language specified in the `CultureInfo` argument, a speech recognition engine that supports that language-country code must be installed. The speech recognition engines that shipped with Microsoft Windows 7 work with the following language-country codes.  \\n  \\n-   en-GB. English (United Kingdom)  \\n  \\n-   en-US. English (United States)  \\n  \\n-   de-DE. German (Germany)  \\n  \\n-   es-ES. Spanish (Spain)  \\n  \\n-   fr-FR. French (France)  \\n  \\n-   ja-JP. Japanese (Japan)  \\n  \\n-   zh-CN. Chinese (China)  \\n  \\n-   zh-TW. Chinese (Taiwan)  \\n  \\n Two-letter language codes such as \\\"en\\\", \\\"fr\\\", or \\\"es\\\" are also permitted.  \\n  \\n Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.  \\n  \\n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \\n  \\n To configure the audio input, use one of the following methods:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition, and initializes a speech recognizer for the en-US locale.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Create an in-process speech recognizer for the en-US locale.  \\n      using (  \\n      SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(  \\n          new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Add a handler for the speech recognized event.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous, continuous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Recognized text: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);\n    parameters:\n    - id: culture\n      type: System.Globalization.CultureInfo\n      description: The locale that the speech recognizer must support.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.#ctor*\n  exceptions:\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: None of the installed speech recognizers support the specified locale, or <code>culture</code> is the invariant culture.\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>Culture</code> is `null`.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)\n  id: '#ctor(System.Speech.Recognition.RecognizerInfo)'\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechRecognitionEngine(RecognizerInfo)\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine(RecognizerInfo)\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine(RecognizerInfo)\n  type: Constructor\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> using the information in a <xref href=\"System.Speech.Recognition.RecognizerInfo\"></xref> object to specify the recognizer to use.\n  remarks: \"You can create an instance of this class for any of the installed speech recognizers. To get information about which recognizers are installed, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.  \\n  \\n Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.  \\n  \\n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \\n  \\n To configure the audio input, use one of the following methods:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition, and initializes a speech recognizer that supports the English language.  \\n  \\n```csharp  \\n using System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Select a speech recognizer that supports English.  \\n      RecognizerInfo info = null;  \\n      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  \\n      {  \\n        if (ri.Culture.TwoLetterISOLanguageName.Equals(\\\"en\\\"))  \\n        {  \\n          info = ri;  \\n          break;  \\n        }  \\n      }  \\n      if (info == null) return;  \\n  \\n      // Create the selected recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(info))  \\n      {  \\n  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Add a handler for the speech recognized event.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous, continuous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Recognized text: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);\n    parameters:\n    - id: recognizerInfo\n      type: System.Speech.Recognition.RecognizerInfo\n      description: The information for the specific speech recognizer.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.#ctor*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)\n  id: '#ctor(System.String)'\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechRecognitionEngine(String)\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine(String)\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine(String)\n  type: Constructor\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> class with a string parameter that specifies the name of the recognizer to use.\n  remarks: \"The token name of the recognizer is the value of the <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> property of the <xref:System.Speech.Recognition.RecognizerInfo> object returned by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property of the recognizer. To get a collection of all the installed recognizers, use the static <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.  \\n  \\n Before the speech recognizer can begin recognition, you must load at least one speech recognition grammar and configure the input for the recognizer.  \\n  \\n To load a grammar, call the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.  \\n  \\n To configure the audio input, use one of the following methods:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition, and creates an instance of the Speech Recognizer 8.0 for Windows (English - US).  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Create an instance of the Microsoft Speech Recognizer 8.0 for  \\n      // Windows (English - US).  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(\\\"MS-1033-80-DESK\\\"))  \\n      {  \\n  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Add a handler for the speech recognized event.  \\n        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous, continuous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Recognized text: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public SpeechRecognitionEngine (string recognizerId);\n    parameters:\n    - id: recognizerId\n      type: System.String\n      description: The token name of the speech recognizer to use.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.#ctor*\n  exceptions:\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: No speech recognizer with that token name is installed, or <code>recognizerId</code> is the empty string (\"\").\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>recognizerId</code> is `null`.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat\n  id: AudioFormat\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: AudioFormat\n  nameWithType: SpeechRecognitionEngine.AudioFormat\n  fullName: SpeechRecognitionEngine.AudioFormat\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the format of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.\n  remarks: \"To configure the audio input, use one of the following methods:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>\"\n  example:\n  - \"The example below uses <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> to obtain and display audio format data.  \\n  \\n```  \\nstatic void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   \\n{  \\n  \\n  if (recognitionEngine != null && label != null)   \\n  {  \\n    label.Text = String.Format(\\\"Encoding Format:         {0}\\\\n\\\" +  \\n          \\\"AverageBytesPerSecond    {1}\\\\n\\\" +  \\n          \\\"BitsPerSample            {2}\\\\n\\\" +  \\n          \\\"BlockAlign               {3}\\\\n\\\" +  \\n          \\\"ChannelCount             {4}\\\\n\\\" +  \\n          \\\"SamplesPerSecond         {5}\\\",  \\n          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  \\n          recognitionEngine.AudioFormat.AverageBytesPerSecond,  \\n          recognitionEngine.AudioFormat.BitsPerSample,  \\n          recognitionEngine.AudioFormat.BlockAlign,  \\n          recognitionEngine.AudioFormat.ChannelCount,  \\n          recognitionEngine.AudioFormat.SamplesPerSecond);  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }\n    return:\n      type: System.Speech.AudioFormat.SpeechAudioFormatInfo\n      description: The format of audio at the input to the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance, or `null` if the input is not configured or set to the null input.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel\n  id: AudioLevel\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: AudioLevel\n  nameWithType: SpeechRecognitionEngine.AudioLevel\n  fullName: SpeechRecognitionEngine.AudioLevel\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the level of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.\n  remarks: The value 0 represents silence, and 100 represents the maximum input volume.\n  syntax:\n    content: public int AudioLevel { get; }\n    return:\n      type: System.Int32\n      description: The audio level of the input to the speech recognizer, from 0 through 100.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated\n  id: AudioLevelUpdated\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: AudioLevelUpdated\n  nameWithType: SpeechRecognitionEngine.AudioLevelUpdated\n  fullName: SpeechRecognitionEngine.AudioLevelUpdated\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> reports the level of its audio input.\n  remarks: \"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.  \\n  \\n To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> property.  \\n  \\n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example adds a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> event to a <xref:System.Speech.Recognition.SpeechRecognitionEngine> object. The handler outputs the new audio level to the console.  \\n  \\n```  \\nprivate SpeechRecognitionEngine recognizer;  \\n  \\n// Initialize the SpeechRecognitionEngine object.   \\nprivate void Initialize()  \\n{  \\n  recognizer = new SpeechRecognitionEngine();  \\n  \\n  // Add an event handler for the AudioLevelUpdated event.  \\n  recognizer.AudioLevelUpdated +=   \\n   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  \\n  \\n  // Add other initialization code here.  \\n  \\n}  \\n  \\n// Write the audio level to the console when the AudioLevelUpdated event is raised.  \\nvoid recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  \\n{  \\n  Console.WriteLine(\\\"The audio level is now: {0}.\\\", e.AudioLevel);  \\n}  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.AudioLevelUpdatedEventArgs> AudioLevelUpdated;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.AudioLevelUpdatedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\n  id: AudioPosition\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: AudioPosition\n  nameWithType: SpeechRecognitionEngine.AudioPosition\n  fullName: SpeechRecognitionEngine.AudioPosition\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the current location in the audio stream being generated by the device that is providing input to the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.\n  remarks: The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the recognizer's position within its audio input. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.\n  example:\n  - \"In the following example, the in-process speech recognizer uses a dictation grammar to match speech input. A handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event writes to the console the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, and  <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> when the speech recognizer detects speech at its input.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognitionEngine recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an in-process speech recognition engine for US English.  \\n      using (recognizer = new SpeechRecognitionEngine(  \\n        new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Create a grammar for finding services in different cities.  \\n        Choices services = new Choices(new string[] { \\\"restaurants\\\", \\\"hotels\\\", \\\"gas stations\\\" });  \\n        Choices cities = new Choices(new string[] { \\\"Seattle\\\", \\\"Boston\\\", \\\"Dallas\\\" });  \\n  \\n        GrammarBuilder findServices = new GrammarBuilder(\\\"Find\\\");  \\n        findServices.Append(services);  \\n        findServices.Append(\\\"near\\\");  \\n        findServices.Append(cities);  \\n  \\n        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  \\n        Grammar servicesGrammar = new Grammar(findServices);  \\n        recognizer.LoadGrammarAsync(servicesGrammar);  \\n  \\n        // Add handlers for events.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  \\n  \\n        // Start asynchronous recognition.  \\n        recognizer.RecognizeAsync();  \\n        Console.WriteLine(\\\"Starting asynchronous recognition...\\\");  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Gather information about detected speech and write it to the console.  \\n    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Speech detected:\\\");  \\n      Console.WriteLine(\\\"  Audio level: \\\" + recognizer.AudioLevel);  \\n      Console.WriteLine(\\\"  Audio position at the event: \\\" + e.AudioPosition);  \\n      Console.WriteLine(\\\"  Current audio position: \\\" + recognizer.AudioPosition);  \\n      Console.WriteLine(\\\"  Current recognizer audio position: \\\" +   \\n        recognizer.RecognizerAudioPosition);  \\n    }  \\n  \\n    // Write the text of the recognition result to the console.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"\\\\nSpeech recognized: \\\" + e.Result.Text);  \\n  \\n      // Add event handler code here.  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public TimeSpan AudioPosition { get; }\n    return:\n      type: System.TimeSpan\n      description: The current location in the audio stream being generated by the input device.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred\n  id: AudioSignalProblemOccurred\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: AudioSignalProblemOccurred\n  nameWithType: SpeechRecognitionEngine.AudioSignalProblemOccurred\n  fullName: SpeechRecognitionEngine.AudioSignalProblemOccurred\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> detects a problem in the audio signal.\n  remarks: \"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  \\n  \\n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example defines an event handler that gathers information about an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> event.  \\n  \\n```  \\nprivate SpeechRecognitionEngine recognizer;  \\n  \\n// Initialize the speech recognition engine.  \\nprivate void Initialize()  \\n{  \\n  recognizer = new SpeechRecognitionEngine();  \\n  \\n  // Add a handler for the AudioSignalProblemOccurred event.  \\n  recognizer.AudioSignalProblemOccurred +=   \\n    new EventHandler<AudioSignalProblemOccurredEventArgs>(  \\n      recognizer_AudioSignalProblemOccurred);  \\n}  \\n  \\n// Gather information when the AudioSignalProblemOccurred event is raised.  \\nvoid recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  \\n{  \\n  StringBuilder details = new StringBuilder();  \\n  \\n  details.AppendLine(\\\"Audio signal problem information:\\\");  \\n  details.AppendFormat(  \\n    \\\" Audio level:               {0}\\\" + Environment.NewLine +  \\n    \\\" Audio position:            {1}\\\" + Environment.NewLine +  \\n    \\\" Audio signal problem:      {2}\\\" + Environment.NewLine +  \\n    \\\" Recognition engine audio position: {3}\\\" + Environment.NewLine,  \\n    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  \\n    e.recoEngineAudioPosition);  \\n  \\n  // Insert additional event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.AudioSignalProblemOccurredEventArgs> AudioSignalProblemOccurred;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.AudioSignalProblemOccurredEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioState\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState\n  id: AudioState\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: AudioState\n  nameWithType: SpeechRecognitionEngine.AudioState\n  fullName: SpeechRecognitionEngine.AudioState\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the state of the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.\n  remarks: The <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property represents the audio state with a member of the <xref:System.Speech.Recognition.AudioState> enumeration.\n  syntax:\n    content: public System.Speech.Recognition.AudioState AudioState { get; }\n    return:\n      type: System.Speech.Recognition.AudioState\n      description: The state of the audio input to the speech recognizer.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.AudioState*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged\n  id: AudioStateChanged\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: AudioStateChanged\n  nameWithType: SpeechRecognitionEngine.AudioStateChanged\n  fullName: SpeechRecognitionEngine.AudioStateChanged\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the state changes in the audio being received by the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.\n  remarks: \"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.  \\n  \\n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example uses a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> event to write the recognizer's new <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> to the console each time it changes, using a member of the <xref:System.Speech.Recognition.AudioState> enumeration.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize an in-process speech recognition engine.  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n         new SpeechRecognitionEngine(new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create and load a grammar.  \\n        Choices animals = new Choices(new string[] { \\\"cow\\\", \\\"pig\\\", \\\"goat\\\" });  \\n        GrammarBuilder farm = new GrammarBuilder(\\\"On this farm he had a\\\");  \\n        farm.Append(animals);  \\n        Grammar farmAnimals = new Grammar(farm);  \\n        farmAnimals.Name = \\\"Farm\\\";  \\n        recognizer.LoadGrammar(farmAnimals);  \\n  \\n        // Attach event handlers.  \\n        recognizer.AudioStateChanged +=  \\n          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n  \\n        // Set the input to the recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start recognition.  \\n        recognizer.RecognizeAsync();  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null && e.Result.Text != null)  \\n      {  \\n        Console.WriteLine();  \\n        Console.WriteLine(\\\"  Recognized text =  {0}\\\", e.Result.Text);  \\n        Console.WriteLine();  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  Recognized text not available.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Done.\\\");  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the AudioStateChanged event.  \\n    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"The new audio state is: \\\" + e.AudioState);  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.AudioStateChangedEventArgs> AudioStateChanged;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.AudioStateChangedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout\n  id: BabbleTimeout\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: BabbleTimeout\n  nameWithType: SpeechRecognitionEngine.BabbleTimeout\n  fullName: SpeechRecognitionEngine.BabbleTimeout\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets the time interval during which a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> accepts input containing only background noise, before finalizing recognition.\n  remarks: \"Each speech recognizer has an algorithm to distinguish between silence and speech. The recognizer classifies as background noise any non-silence input that does not match the initial rule of any of the recognizer's loaded and enabled speech recognition grammars. If the recognizer receives only background noise and silence within the babble timeout interval, then the recognizer finalizes that recognition operation.  \\n  \\n-   For asynchronous recognition operations, the recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, where the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=fullName> property is `true`, and the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName> property is `null`.  \\n  \\n-   For synchronous recognition operations and emulation, the recognizer returns `null`, instead of a valid <xref:System.Speech.Recognition.RecognitionResult>.  \\n  \\n If the babble timeout period is set to 0, the recognizer does not perform a babble timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition that sets the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties of a <xref:System.Speech.Recognition.SpeechRecognitionEngine> before initiating speech recognition. Handlers for the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> events output event information to the console to demonstrate how the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties of a <xref:System.Speech.Recognition.SpeechRecognitionEngine> affect recognition operations.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an in-process speech recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(  \\n          new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Load a Grammar object.  \\n        recognizer.LoadGrammar(CreateServicesGrammar(\\\"FindServices\\\"));  \\n  \\n        // Add event handlers.  \\n        recognizer.AudioStateChanged +=  \\n          new EventHandler<AudioStateChangedEventArgs>(  \\n            AudioStateChangedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  \\n        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  \\n        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  \\n        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  \\n  \\n        Console.WriteLine(\\\"BabbleTimeout: {0}\\\", recognizer.BabbleTimeout);  \\n        Console.WriteLine(\\\"InitialSilenceTimeout: {0}\\\", recognizer.InitialSilenceTimeout);  \\n        Console.WriteLine(\\\"EndSilenceTimeout: {0}\\\", recognizer.EndSilenceTimeout);  \\n        Console.WriteLine(\\\"EndSilenceTimeoutAmbiguous: {0}\\\", recognizer.EndSilenceTimeoutAmbiguous);  \\n        Console.WriteLine();  \\n  \\n        // Start asynchronous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Single);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Create a grammar and build it into a Grammar object.   \\n    static Grammar CreateServicesGrammar(string grammarName)  \\n    {  \\n  \\n      // Create a grammar for finding services in different cities.  \\n      Choices services = new Choices(new string[] { \\\"restaurants\\\", \\\"hotels\\\", \\\"gas stations\\\" });  \\n      Choices cities = new Choices(new string[] { \\\"Seattle\\\", \\\"Boston\\\", \\\"Dallas\\\" });  \\n  \\n      GrammarBuilder findServices = new GrammarBuilder(\\\"Find\\\");  \\n      findServices.Append(services);  \\n      findServices.Append(\\\"near\\\");  \\n      findServices.Append(cities);  \\n  \\n      // Create a Grammar object from the GrammarBuilder..  \\n      Grammar servicesGrammar = new Grammar(findServices);  \\n      servicesGrammar.Name = (\\\"FindServices\\\");  \\n      return servicesGrammar;  \\n    }  \\n  \\n    // Handle the AudioStateChanged event.  \\n    static void AudioStateChangedHandler(  \\n      object sender, AudioStateChangedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"AudioStateChanged ({0}): {1}\\\",  \\n        DateTime.Now.ToString(\\\"mm:ss.f\\\"), e.AudioState);  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"RecognizeCompleted ({0}):\\\",  \\n        DateTime.Now.ToString(\\\"mm:ss.f\\\"));  \\n  \\n      string resultText;  \\n      if (e.Result != null) { resultText = e.Result.Text; }  \\n      else { resultText = \\\"<null>\\\"; }  \\n  \\n      Console.WriteLine(  \\n        \\\" BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}\\\",  \\n        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\" Exception message: \\\", e.Error.Message);  \\n      }  \\n  \\n      // Start the next asynchronous recognition operation.  \\n      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public TimeSpan BabbleTimeout { get; set; }\n    return:\n      type: System.TimeSpan\n      description: The duration of the time interval.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout*\n  exceptions:\n  - type: System.ArgumentOutOfRangeException\n    commentId: T:System.ArgumentOutOfRangeException\n    description: This property is set to less than 0 seconds.\n  attributes: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)\n  id: Dispose(System.Boolean)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: Dispose(Boolean)\n  nameWithType: SpeechRecognitionEngine.Dispose(Boolean)\n  fullName: SpeechRecognitionEngine.Dispose(Boolean)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object and releases resources used during the session.\n  syntax:\n    content: protected virtual void Dispose (bool disposing);\n    parameters:\n    - id: disposing\n      type: System.Boolean\n      description: '`true` to release both managed and unmanaged resources; `false` to release only unmanaged resources.'\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.Dispose*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Dispose\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose\n  id: Dispose\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: Dispose()\n  nameWithType: SpeechRecognitionEngine.Dispose()\n  fullName: SpeechRecognitionEngine.Dispose()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object.\n  syntax:\n    content: public void Dispose ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.Dispose*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)\n  id: EmulateRecognize(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EmulateRecognize(String)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognize(String)\n  fullName: SpeechRecognitionEngine.EmulateRecognize(String)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition.\n  remarks: \"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.  \\n  \\n The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.\"\n  example:\n  - \"The code example below is part of a console application that demonstrates emulated input, the associated recognition results, and the associated events raised by the speech recognizer. The example generates the following output.  \\n  \\n```  \\nTestRecognize(\\\"Smith\\\")...  \\n SpeechDetected event raised.  \\n SpeechRecognized event raised.  \\n  Grammar = Smith; Text = Smith  \\n...Recognition result text = Smith  \\n  \\nTestRecognize(\\\"Jones\\\")...  \\n SpeechDetected event raised.  \\n SpeechRecognized event raised.  \\n  Grammar = Jones; Text = Jones  \\n...Recognition result text = Jones  \\n  \\nTestRecognize(\\\"Mister\\\")...  \\n SpeechDetected event raised.  \\n SpeechHypothesized event raised.  \\n  Grammar = Smith; Text = mister  \\n SpeechRecognitionRejected event raised.  \\n  Grammar = <not available>; Text =  \\n...No recognition result.  \\n  \\nTestRecognize(\\\"Mister Smith\\\")...  \\n SpeechDetected event raised.  \\n SpeechRecognized event raised.  \\n  Grammar = Smith; Text = mister Smith  \\n...Recognition result text = mister Smith  \\n  \\npress any key to exit...  \\n```  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace Sre_EmulateRecognize  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Create an in-process speech recognizer for the en-US locale.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Load grammars.  \\n        recognizer.LoadGrammar(CreateNameGrammar(\\\"Smith\\\"));  \\n        recognizer.LoadGrammar(CreateNameGrammar(\\\"Jones\\\"));  \\n  \\n        // Disable audio input to the recognizer.  \\n        recognizer.SetInputToNull();  \\n  \\n        // Add handlers for events raised by the EmulateRecognize method.  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(  \\n            SpeechDetectedHandler);  \\n        recognizer.SpeechHypothesized +=  \\n          new EventHandler<SpeechHypothesizedEventArgs>(  \\n            SpeechHypothesizedHandler);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(  \\n            SpeechRecognitionRejectedHandler);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n  \\n        // Start four synchronous emulated recognition operations.  \\n        TestRecognize(recognizer, \\\"Smith\\\");  \\n        TestRecognize(recognizer, \\\"Jones\\\");  \\n        TestRecognize(recognizer, \\\"Mister\\\");  \\n        TestRecognize(recognizer, \\\"Mister Smith\\\");  \\n      }  \\n  \\n      Console.WriteLine(\\\"press any key to exit...\\\");  \\n      Console.ReadKey(true);  \\n    }  \\n  \\n    // Create a simple name grammar.  \\n    // Set the grammar name to the surname.  \\n    private static Grammar CreateNameGrammar(string surname)  \\n    {  \\n      GrammarBuilder builder = new GrammarBuilder(\\\"mister\\\", 0, 1);  \\n      builder.Append(surname);  \\n  \\n      Grammar nameGrammar = new Grammar(builder);  \\n      nameGrammar.Name = surname;  \\n  \\n      return nameGrammar;  \\n    }  \\n  \\n    // Send emulated input to the recognizer for synchronous recognition.  \\n    private static void TestRecognize(  \\n      SpeechRecognitionEngine recognizer, string input)  \\n    {  \\n      Console.WriteLine(\\\"TestRecognize(\\\\\\\"{0}\\\\\\\")...\\\", input);  \\n      RecognitionResult result =  \\n        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  \\n      if (result != null)  \\n      {  \\n        Console.WriteLine(\\\"...Recognition result text = {0}\\\",  \\n          result.Text ?? \\\"<null>\\\");  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"...No recognition result.\\\");  \\n      }  \\n      Console.WriteLine();  \\n    }  \\n  \\n    static void SpeechDetectedHandler(  \\n      object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechDetected event raised.\\\");  \\n    }  \\n  \\n    // Handle events.  \\n    static void SpeechHypothesizedHandler(  \\n      object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechHypothesized event raised.\\\");  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"  Grammar = {0}; Text = {1}\\\",  \\n          e.Result.Grammar.Name ?? \\\"<none>\\\", e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  No recognition result available.\\\");  \\n      }  \\n    }  \\n  \\n    static void SpeechRecognitionRejectedHandler(  \\n      object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechRecognitionRejected event raised.\\\");  \\n      if (e.Result != null)  \\n      {  \\n        string grammarName;  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name ?? \\\"<none>\\\";  \\n        }  \\n        else  \\n        {  \\n          grammarName = \\\"<not available>\\\";  \\n        }  \\n        Console.WriteLine(\\\"  Grammar = {0}; Text = {1}\\\",  \\n          grammarName, e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  No recognition result available.\\\");  \\n      }  \\n    }  \\n  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechRecognized event raised.\\\");  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"  Grammar = {0}; Text = {1}\\\",  \\n          e.Result.Grammar.Name ?? \\\"<none>\\\", e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  No recognition result available.\\\");  \\n      }  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input for the recognition operation.\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize*\n  exceptions:\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The recognizer has no speech recognition grammars loaded.\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>inputText</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>inputText</code> is the empty string (\"\").\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  id: EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of specific words to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.\n  remarks: \"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.  \\n  \\n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizer always ignores the character width and never ignores the Kana type. The recognizer also ignores new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\"\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: wordUnits\n      type: System.Speech.Recognition.RecognizedWordUnit[]\n      description: An array of word units that contains the input for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize*\n  exceptions:\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The recognizer has no speech recognition grammars loaded.\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>wordUnits</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>wordUnits</code> contains one or more `null` elements.\n  - type: System.NotSupportedException\n    commentId: T:System.NotSupportedException\n    description: <code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  id: EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EmulateRecognize(String, CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognize(String, CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognize(String, CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the speech recognizer, using text in place of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.\n  remarks: \"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated.  \\n  \\n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizer always ignores the character width and never ignores the Kana type. The recognizer also ignores new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\"\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input phrase for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The result for the recognition operation, or `null` if the operation is not successful or the recognizer is not enabled.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize*\n  exceptions:\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The recognizer has no speech recognition grammars loaded.\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>inputText</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>inputText</code> is the empty string (\"\").\n  - type: System.NotSupportedException\n    commentId: T:System.NotSupportedException\n    description: <code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)\n  id: EmulateRecognizeAsync(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EmulateRecognizeAsync(String)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeAsync(String)\n  fullName: SpeechRecognitionEngine.EmulateRecognizeAsync(String)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition.\n  remarks: \"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \\n  \\n The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.\"\n  example:\n  - \"The code example below is part of a console application that demonstrates asynchronous emulated input, the associated recognition results, and the associated events raised by the speech recognizer. The example generates the following output.  \\n  \\n```  \\n  \\nTestRecognizeAsync(\\\"Smith\\\")...  \\n SpeechDetected event raised.  \\n SpeechRecognized event raised.  \\n  Grammar = Smith; Text = Smith  \\n EmulateRecognizeCompleted event raised.  \\n  Grammar = Smith; Text = Smith  \\n Done.  \\n  \\nTestRecognizeAsync(\\\"Jones\\\")...  \\n SpeechDetected event raised.  \\n SpeechRecognized event raised.  \\n  Grammar = Jones; Text = Jones  \\n EmulateRecognizeCompleted event raised.  \\n  Grammar = Jones; Text = Jones  \\n Done.  \\n  \\nTestRecognizeAsync(\\\"Mister\\\")...  \\n SpeechDetected event raised.  \\n SpeechHypothesized event raised.  \\n  Grammar = Smith; Text = mister  \\n SpeechRecognitionRejected event raised.  \\n  Grammar = <not available>; Text =  \\n EmulateRecognizeCompleted event raised.  \\n  No recognition result available.  \\n Done.  \\n  \\nTestRecognizeAsync(\\\"Mister Smith\\\")...  \\n SpeechDetected event raised.  \\n SpeechRecognized event raised.  \\n  Grammar = Smith; Text = mister Smith  \\n EmulateRecognizeCompleted event raised.  \\n  Grammar = Smith; Text = mister Smith  \\n Done.  \\n  \\npress any key to exit...  \\n```  \\n  \\n```csharp  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace SreEmulateRecognizeAsync  \\n{  \\n  class Program  \\n  {  \\n    // Indicate when an asynchronous operation is finished.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Load grammars.  \\n        recognizer.LoadGrammar(CreateNameGrammar(\\\"Smith\\\"));  \\n        recognizer.LoadGrammar(CreateNameGrammar(\\\"Jones\\\"));  \\n  \\n        // Configure the audio input.  \\n        recognizer.SetInputToNull();  \\n  \\n        // Add event handlers for the events raised by the  \\n        // EmulateRecognizeAsync method.  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(  \\n            SpeechDetectedHandler);  \\n        recognizer.SpeechHypothesized +=  \\n          new EventHandler<SpeechHypothesizedEventArgs>(  \\n            SpeechHypothesizedHandler);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(  \\n            SpeechRecognitionRejectedHandler);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.EmulateRecognizeCompleted +=  \\n          new EventHandler<EmulateRecognizeCompletedEventArgs>(  \\n            EmulateRecognizeCompletedHander);  \\n  \\n        // Start four asynchronous emulated recognition operations.  \\n        TestRecognizeAsync(recognizer, \\\"Smith\\\");  \\n        TestRecognizeAsync(recognizer, \\\"Jones\\\");  \\n        TestRecognizeAsync(recognizer, \\\"Mister\\\");  \\n        TestRecognizeAsync(recognizer, \\\"Mister Smith\\\");  \\n      }  \\n  \\n      Console.WriteLine(\\\"press any key to exit...\\\");  \\n      Console.ReadKey(true);  \\n    }  \\n  \\n    // Create a simple name grammar.  \\n    // Set the grammar name to the surname.  \\n    private static Grammar CreateNameGrammar(string surname)  \\n    {  \\n      GrammarBuilder builder = new GrammarBuilder(\\\"mister\\\", 0, 1);  \\n      builder.Append(surname);  \\n  \\n      Grammar nameGrammar = new Grammar(builder);  \\n      nameGrammar.Name = surname;  \\n  \\n      return nameGrammar;  \\n    }  \\n  \\n    // Send emulated input to the recognizer for asynchronous  \\n    // recognition.  \\n    private static void TestRecognizeAsync(  \\n      SpeechRecognitionEngine recognizer, string input)  \\n    {  \\n      completed = false;  \\n  \\n      Console.WriteLine(\\\"TestRecognizeAsync(\\\\\\\"{0}\\\\\\\")...\\\", input);  \\n      recognizer.EmulateRecognizeAsync(input);  \\n  \\n      // Wait for the operation to complete.  \\n      while (!completed)  \\n      {  \\n        Thread.Sleep(333);  \\n      }  \\n  \\n      Console.WriteLine(\\\" Done.\\\");  \\n      Console.WriteLine();  \\n    }  \\n  \\n    static void SpeechDetectedHandler(  \\n      object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechDetected event raised.\\\");  \\n    }  \\n  \\n    static void SpeechHypothesizedHandler(  \\n      object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechHypothesized event raised.\\\");  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"  Grammar = {0}; Text = {1}\\\",  \\n          e.Result.Grammar.Name ?? \\\"<none>\\\", e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  No recognition result available.\\\");  \\n      }  \\n    }  \\n  \\n    // Handle events.  \\n    static void SpeechRecognitionRejectedHandler(  \\n      object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechRecognitionRejected event raised.\\\");  \\n      if (e.Result != null)  \\n      {  \\n        string grammarName;  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name ?? \\\"<none>\\\";  \\n        }  \\n        else  \\n        {  \\n          grammarName = \\\"<not available>\\\";  \\n        }  \\n        Console.WriteLine(\\\"  Grammar = {0}; Text = {1}\\\",  \\n          grammarName, e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  No recognition result available.\\\");  \\n      }  \\n    }  \\n  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" SpeechRecognized event raised.\\\");  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"  Grammar = {0}; Text = {1}\\\",  \\n          e.Result.Grammar.Name ?? \\\"<none>\\\", e.Result.Text );  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  No recognition result available.\\\");  \\n      }  \\n    }  \\n  \\n    static void EmulateRecognizeCompletedHander(  \\n      object sender, EmulateRecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" EmulateRecognizeCompleted event raised.\\\");  \\n  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\"  {0} exception encountered: {1}:\\\",  \\n          e.Error.GetType().Name, e.Error.Message);  \\n      }  \\n      else if (e.Cancelled)  \\n      {  \\n        Console.WriteLine(\\\"  Operation cancelled.\\\");  \\n      }  \\n      else if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"  Grammar = {0}; Text = {1}\\\",  \\n          e.Result.Grammar.Name ?? \\\"<none>\\\", e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  No recognition result available.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void EmulateRecognizeAsync (string inputText);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input for the recognition operation.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync*\n  exceptions:\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>inputText</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>inputText</code> is the empty string (\"\").\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  id: EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of specific words to the speech recognizer, using an array of <xref href=\"System.Speech.Recognition.RecognizedWordUnit\"></xref> objects in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.\n  remarks: \"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \\n  \\n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treat punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\"\n  syntax:\n    content: public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: wordUnits\n      type: System.Speech.Recognition.RecognizedWordUnit[]\n      description: An array of word units that contains the input for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync*\n  exceptions:\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>wordUnits</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>wordUnits</code> contains one or more `null` elements.\n  - type: System.NotSupportedException\n    commentId: T:System.NotSupportedException\n    description: <code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  id: EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EmulateRecognizeAsync(String, CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeAsync(String, CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognizeAsync(String, CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the speech recognizer, using text in place of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.\n  remarks: \"The speech recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events as if the recognition operation is not emulated. When the recognizer completes the asynchronous recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \\n  \\n The recognizer uses `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treat punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\"\n  syntax:\n    content: public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input phrase for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync*\n  exceptions:\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The recognizer has no speech recognition grammars loaded, or the recognizer has an asynchronous recognition operation that is not yet complete.\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>inputText</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>inputText</code> is the empty string (\"\").\n  - type: System.NotSupportedException\n    commentId: T:System.NotSupportedException\n    description: <code>compareOptions</code> contains the <xref href=\"System.Globalization.CompareOptions.IgnoreNonSpace\"></xref>, <xref href=\"System.Globalization.CompareOptions.IgnoreSymbols\"></xref>, or <xref href=\"System.Globalization.CompareOptions.StringSort\"></xref> flag.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted\n  id: EmulateRecognizeCompleted\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EmulateRecognizeCompleted\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeCompleted\n  fullName: SpeechRecognitionEngine.EmulateRecognizeCompleted\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finalizes an asynchronous recognition operation of emulated input.\n  remarks: \"Each <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when it finalizes the asynchronous operation.  \\n  \\n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operation can raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.  \\n  \\n If emulated recognition was successful, you can access the recognition result using the either of the following:  \\n  \\n-   The <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event.  \\n  \\n-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property in the <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event.  \\n  \\n If emulated recognition was not successful, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event is not raised and the <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> will be null.  \\n  \\n <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> derives from <xref:System.ComponentModel.AsyncCompletedEventArgs>.  \\n  \\n <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>.  \\n  \\n When you create an <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example is part of a console application that loads a speech recognition grammar and demonstrates asynchronous emulated input, the associated recognition results, and the associated events raised by the speech recognizer.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace InProcessRecognizer  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether the asynchronous emulate recognition  \\n    // operation has completed.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an instance of an in-process recognizer.  \\n      using (SpeechRecognitionEngine recognizer =   \\n        new SpeechRecognitionEngine(new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Create and load a sample grammar.  \\n        Grammar testGrammar =  \\n          new Grammar(new GrammarBuilder(\\\"testing testing\\\"));  \\n        testGrammar.Name = \\\"Test Grammar\\\";  \\n        recognizer.LoadGrammar(testGrammar);  \\n  \\n        // Attach event handlers for recognition events.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  \\n        recognizer.EmulateRecognizeCompleted +=  \\n          new EventHandler<EmulateRecognizeCompletedEventArgs>(  \\n            EmulateRecognizeCompletedHandler);  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call mathches the grammar  \\n        // and generates a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing testing\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call does not match the grammar  \\n        // or generate a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing one two three\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"Result of 1st call to EmulateRecognizeAsync = {0}\\\",  \\n          e.Result.Text ?? \\\"<no text>\\\");  \\n        Console.WriteLine();  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No recognition result\\\");  \\n      }  \\n    }  \\n  \\n    // Handle the EmulateRecognizeCompleted event.  \\n    static void EmulateRecognizeCompletedHandler(  \\n      object sender, EmulateRecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Result == null)  \\n      {  \\n        Console.WriteLine(\\\"Result of 2nd call to EmulateRecognizeAsync = No result generated.\\\");  \\n      }  \\n  \\n      // Indicate the asynchronous operation is complete.  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> EmulateRecognizeCompleted;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.EmulateRecognizeCompletedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout\n  id: EndSilenceTimeout\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EndSilenceTimeout\n  nameWithType: SpeechRecognitionEngine.EndSilenceTimeout\n  fullName: SpeechRecognitionEngine.EndSilenceTimeout\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets the interval of silence that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> will accept at the end of unambiguous input before finalizing a recognition operation.\n  remarks: \"The speech recognizer uses this timeout interval when the recognition input is unambiguous. For example, for a speech recognition grammar that supports recognition of either \\\"new game please\\\" or \\\"new game\\\", \\\"new game please\\\" is an unambiguous input, and \\\"new game\\\" is an ambiguous input.  \\n  \\n This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 150 milliseconds.  \\n  \\n To set the timeout interval for ambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property.\"\n  syntax:\n    content: public TimeSpan EndSilenceTimeout { get; set; }\n    return:\n      type: System.TimeSpan\n      description: The duration of the interval of silence.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout*\n  exceptions:\n  - type: System.ArgumentOutOfRangeException\n    commentId: T:System.ArgumentOutOfRangeException\n    description: This property is set to less than 0 seconds or greater than 10 seconds.\n  attributes: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  id: EndSilenceTimeoutAmbiguous\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: EndSilenceTimeoutAmbiguous\n  nameWithType: SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  fullName: SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets the interval of silence that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> will accept at the end of ambiguous input before finalizing a recognition operation.\n  remarks: \"The speech recognizer uses this timeout interval when the recognition input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \\\"new game please\\\" or \\\"new game\\\", \\\"new game please\\\" is an unambiguous input, and \\\"new game\\\" is an ambiguous input.  \\n  \\n This property determines how long the speech recognition engine will wait for additional input before finalizing a recognition operation. The timeout interval can be from 0 seconds to 10 seconds, inclusive. The default is 500 milliseconds.  \\n  \\n To set the timeout interval for unambiguous input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property.\"\n  syntax:\n    content: public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }\n    return:\n      type: System.TimeSpan\n      description: The duration of the interval of silence.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous*\n  exceptions:\n  - type: System.ArgumentOutOfRangeException\n    commentId: T:System.ArgumentOutOfRangeException\n    description: This property is set to less than 0 seconds or greater than 10 seconds.\n  attributes: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Grammars\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars\n  id: Grammars\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: Grammars\n  nameWithType: SpeechRecognitionEngine.Grammars\n  fullName: SpeechRecognitionEngine.Grammars\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets a collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that are loaded in this <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance.\n  remarks: ''\n  example:\n  - \"The following example outputs information to the console for each speech recognition grammar that is currently loaded by a speech recognizer.  \\n  \\n> [!IMPORTANT]\\n>  Copy the grammar collection to avoid errors if the collection is modified while this method enumerates the elements of the collection.  \\n  \\n```csharp  \\n  \\nprivate static void ListGrammars(SpeechRecognitionEngine recognizer)  \\n{  \\n  string qualifier;  \\n  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  \\n  foreach (Grammar g in grammars)  \\n  {  \\n    qualifier = (g.Enabled) ? \\\"enabled\\\" : \\\"disabled\\\";  \\n  \\n    Console.WriteLine(\\\"Grammar {0} is loaded and is {1}.\\\",  \\n      g.Name, qualifier);  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public System.Collections.ObjectModel.ReadOnlyCollection<System.Speech.Recognition.Grammar> Grammars { get; }\n    return:\n      type: System.Collections.ObjectModel.ReadOnlyCollection{System.Speech.Recognition.Grammar}\n      description: The collection of <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.Grammars*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout\n  id: InitialSilenceTimeout\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: InitialSilenceTimeout\n  nameWithType: SpeechRecognitionEngine.InitialSilenceTimeout\n  fullName: SpeechRecognitionEngine.InitialSilenceTimeout\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets the time interval during which a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> accepts input containing only silence before finalizing recognition.\n  remarks: \"Each speech recognizer has an algorithm to distinguish between silence and speech. If the recognizer input is silence during the initial silence timeout period, then the recognizer finalizes that recognition operation.  \\n  \\n-   For asynchronous recognition operations and emulation, the recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, where the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=fullName> property is `true`, and the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=fullName> property is `null`.  \\n  \\n-   For synchronous recognition operations and emulation, the recognizer returns `null`, instead of a valid <xref:System.Speech.Recognition.RecognitionResult>.  \\n  \\n If the initial silence timeout interval is set to 0, the recognizer does not perform an initial silence timeout check. The timeout interval can be any non-negative value. The default is 0 seconds.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. The example sets the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties of a <xref:System.Speech.Recognition.SpeechRecognitionEngine> before initiating speech recognition. Handlers for the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> events output event information to the console to demonstrate how the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties of a <xref:System.Speech.Recognition.SpeechRecognitionEngine> properties affect recognition operations.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an in-process speech recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(  \\n          new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Load a Grammar object.  \\n        recognizer.LoadGrammar(CreateServicesGrammar(\\\"FindServices\\\"));  \\n  \\n        // Add event handlers.  \\n        recognizer.AudioStateChanged +=  \\n          new EventHandler<AudioStateChangedEventArgs>(  \\n            AudioStateChangedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  \\n        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  \\n        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  \\n        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  \\n  \\n        Console.WriteLine(\\\"BabbleTimeout: {0}\\\", recognizer.BabbleTimeout);  \\n        Console.WriteLine(\\\"InitialSilenceTimeout: {0}\\\", recognizer.InitialSilenceTimeout);  \\n        Console.WriteLine(\\\"EndSilenceTimeout: {0}\\\", recognizer.EndSilenceTimeout);  \\n        Console.WriteLine(\\\"EndSilenceTimeoutAmbiguous: {0}\\\", recognizer.EndSilenceTimeoutAmbiguous);  \\n        Console.WriteLine();  \\n  \\n        // Start asynchronous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Single);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Create a grammar and build it into a Grammar object.   \\n    static Grammar CreateServicesGrammar(string grammarName)  \\n    {  \\n  \\n      // Create a grammar for finding services in different cities.  \\n      Choices services = new Choices(new string[] { \\\"restaurants\\\", \\\"hotels\\\", \\\"gas stations\\\" });  \\n      Choices cities = new Choices(new string[] { \\\"Seattle\\\", \\\"Boston\\\", \\\"Dallas\\\" });  \\n  \\n      GrammarBuilder findServices = new GrammarBuilder(\\\"Find\\\");  \\n      findServices.Append(services);  \\n      findServices.Append(\\\"near\\\");  \\n      findServices.Append(cities);  \\n  \\n      // Create a Grammar object from the GrammarBuilder..  \\n      Grammar servicesGrammar = new Grammar(findServices);  \\n      servicesGrammar.Name = (\\\"FindServices\\\");  \\n      return servicesGrammar;  \\n    }  \\n  \\n    // Handle the AudioStateChanged event.  \\n    static void AudioStateChangedHandler(  \\n      object sender, AudioStateChangedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"AudioStateChanged ({0}): {1}\\\",  \\n        DateTime.Now.ToString(\\\"mm:ss.f\\\"), e.AudioState);  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"RecognizeCompleted ({0}):\\\",  \\n        DateTime.Now.ToString(\\\"mm:ss.f\\\"));  \\n  \\n      string resultText;  \\n      if (e.Result != null) { resultText = e.Result.Text; }  \\n      else { resultText = \\\"<null>\\\"; }  \\n  \\n      Console.WriteLine(  \\n        \\\" BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}\\\",  \\n        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\" Exception message: \\\", e.Error.Message);  \\n      }  \\n  \\n      // Start the next asynchronous recognition operation.  \\n      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public TimeSpan InitialSilenceTimeout { get; set; }\n    return:\n      type: System.TimeSpan\n      description: The duration of the interval of silence.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout*\n  exceptions:\n  - type: System.ArgumentOutOfRangeException\n    commentId: T:System.ArgumentOutOfRangeException\n    description: This property is set to less than 0 seconds.\n  attributes: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers\n  id: InstalledRecognizers\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: InstalledRecognizers()\n  nameWithType: SpeechRecognitionEngine.InstalledRecognizers()\n  fullName: SpeechRecognitionEngine.InstalledRecognizers()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Returns information for all of the installed speech recognizers on the current system.\n  remarks: To get information about the current recognizer, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> property.\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. The example uses the collection returned by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method to find a speech recognizer that supports the English language.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Select a speech recognizer that supports English.  \\n      RecognizerInfo info = null;  \\n      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  \\n      {  \\n        if (ri.Culture.TwoLetterISOLanguageName.Equals(\\\"en\\\"))  \\n        {  \\n          info = ri;  \\n          break;  \\n        }  \\n      }  \\n      if (info == null) return;  \\n  \\n      // Create the selected recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(info))  \\n      {  \\n  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Add a handler for the speech recognized event.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous, continuous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Recognized text: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public static System.Collections.ObjectModel.ReadOnlyCollection<System.Speech.Recognition.RecognizerInfo> InstalledRecognizers ();\n    parameters: []\n    return:\n      type: System.Collections.ObjectModel.ReadOnlyCollection{System.Speech.Recognition.RecognizerInfo}\n      description: A read-only collection of the <xref href=\"System.Speech.Recognition.RecognizerInfo\"></xref> objects that describe the installed recognizers.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)\n  id: LoadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: LoadGrammar(Grammar)\n  nameWithType: SpeechRecognitionEngine.LoadGrammar(Grammar)\n  fullName: SpeechRecognitionEngine.LoadGrammar(Grammar)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Synchronously loads a <xref href=\"System.Speech.Recognition.Grammar\"></xref> object.\n  remarks: \"The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  \\n  \\n If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \\n  \\n When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.  \\n  \\n To load a <xref:System.Speech.Recognition.Grammar> object asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. The example creates a <xref:System.Speech.Recognition.DictationGrammar> and loads it into a speech recognizer.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SpeechRecognitionApp  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Create an in-process speech recognizer for the en-US locale.  \\n      using (  \\n      SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(  \\n          new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Add a handler for the speech recognized event.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous, continuous speech recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Keep the console window open.  \\n        while (true)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Recognized text: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void LoadGrammar (System.Speech.Recognition.Grammar grammar);\n    parameters:\n    - id: grammar\n      type: System.Speech.Recognition.Grammar\n      description: The grammar object to load.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar*\n  exceptions:\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>Grammar</code> is `null`.\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: <code>Grammar</code> is not in a valid state.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  id: LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: LoadGrammarAsync(Grammar)\n  nameWithType: SpeechRecognitionEngine.LoadGrammarAsync(Grammar)\n  fullName: SpeechRecognitionEngine.LoadGrammarAsync(Grammar)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Asynchronously loads a speech recognition grammar.\n  remarks: \"When the recognizer completes loading a <xref:System.Speech.Recognition.Grammar> object, it raises a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> event. The recognizer throws an exception if the <xref:System.Speech.Recognition.Grammar> object is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. You cannot load the same <xref:System.Speech.Recognition.Grammar> object into multiple instances of <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Instead, create a new <xref:System.Speech.Recognition.Grammar> object for each <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  \\n  \\n If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \\n  \\n When you load a grammar, it is enabled by default. To disable a loaded grammar, use the <xref:System.Speech.Recognition.Grammar.Enabled%2A> property.  \\n  \\n To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> method.\"\n  syntax:\n    content: public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);\n    parameters:\n    - id: grammar\n      type: System.Speech.Recognition.Grammar\n      description: The speech recognition grammar to load.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync*\n  exceptions:\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>Grammar</code> is `null`.\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: <code>Grammar</code> is not in a valid state.\n  - type: System.OperationCanceledException\n    commentId: T:System.OperationCanceledException\n    description: The asynchronous operation was canceled.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted\n  id: LoadGrammarCompleted\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: LoadGrammarCompleted\n  nameWithType: SpeechRecognitionEngine.LoadGrammarCompleted\n  fullName: SpeechRecognitionEngine.LoadGrammarCompleted\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finishes the asynchronous loading of a <xref href=\"System.Speech.Recognition.Grammar\"></xref> object.\n  remarks: \"The recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> method initiates an asynchronous operation. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it completes the operation. To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> property.  \\n  \\n If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \\n  \\n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example creates an in-process speech recognizer, and then creates two types of grammars for recognizing specific words and for accepting free dictation. The example constructs a <xref:System.Speech.Recognition.Grammar> object from each of the completed speech recognition grammars, then asynchronously loads the <xref:System.Speech.Recognition.Grammar> objects to the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance. Handlers for the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events write to the console the name of the <xref:System.Speech.Recognition.Grammar> object that was used to perform the recognition and the text of the recognition result, respectively.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognitionEngine recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an in-process speech recognition engine and set its input.  \\n      recognizer = new SpeechRecognitionEngine();  \\n      recognizer.SetInputToDefaultAudioDevice();  \\n  \\n      // Add a handler for the LoadGrammarCompleted event.  \\n      recognizer.LoadGrammarCompleted +=  \\n        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n  \\n      // Add a handler for the SpeechRecognized event.  \\n      recognizer.SpeechRecognized +=  \\n        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n      // Create the \\\"yesno\\\" grammar.  \\n      Choices yesChoices = new Choices(new string[] { \\\"yes\\\", \\\"yup\\\", \\\"yeah\\\" });  \\n      SemanticResultValue yesValue =  \\n          new SemanticResultValue(yesChoices, (bool)true);  \\n      Choices noChoices = new Choices(new string[] { \\\"no\\\", \\\"nope\\\", \\\"neah\\\" });  \\n      SemanticResultValue noValue =  \\n          new SemanticResultValue(noChoices, (bool)false);  \\n      SemanticResultKey yesNoKey =  \\n          new SemanticResultKey(\\\"yesno\\\", new Choices(new GrammarBuilder[] { yesValue, noValue }));  \\n      Grammar yesnoGrammar = new Grammar(yesNoKey);  \\n      yesnoGrammar.Name = \\\"yesNo\\\";  \\n  \\n      // Create the \\\"done\\\" grammar.  \\n      Grammar doneGrammar =  \\n        new Grammar(new Choices(new string[] { \\\"done\\\", \\\"exit\\\", \\\"quit\\\", \\\"stop\\\" }));  \\n      doneGrammar.Name = \\\"Done\\\";  \\n  \\n      // Create a dictation grammar.  \\n      Grammar dictation = new DictationGrammar();  \\n      dictation.Name = \\\"Dictation\\\";  \\n  \\n      // Load grammars to the recognizer.  \\n      recognizer.LoadGrammarAsync(yesnoGrammar);  \\n      recognizer.LoadGrammarAsync(doneGrammar);  \\n      recognizer.LoadGrammarAsync(dictation);  \\n  \\n      // Start asynchronous, continuous recognition.  \\n      recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n      // Keep the console window open.  \\n      Console.ReadLine();  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.   \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      string grammarName = e.Grammar.Name;  \\n      bool grammarLoaded = e.Grammar.Loaded;  \\n  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\"LoadGrammar for {0} failed with a {1}.\\\",  \\n        grammarName, e.Error.GetType().Name);  \\n  \\n        // Add exception handling code here.  \\n      }  \\n  \\n      Console.WriteLine(\\\"Grammar {0} {1} loaded.\\\",  \\n      grammarName, (grammarLoaded) ? \\\"is\\\" : \\\"is not\\\");  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar({0}): {1}\\\", e.Result.Grammar.Name, e.Result.Text);  \\n  \\n      // Add event handler code here.  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.LoadGrammarCompletedEventArgs> LoadGrammarCompleted;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.LoadGrammarCompletedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\n  id: MaxAlternates\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: MaxAlternates\n  nameWithType: SpeechRecognitionEngine.MaxAlternates\n  fullName: SpeechRecognitionEngine.MaxAlternates\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets the maximum number of alternate recognition results that the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> returns for each recognition operation.\n  remarks: \"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent possible interpretations of the input.  \\n  \\n The default value for <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> is 10.\"\n  syntax:\n    content: public int MaxAlternates { get; set; }\n    return:\n      type: System.Int32\n      description: The number of alternate results to return.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates*\n  exceptions:\n  - type: System.ArgumentOutOfRangeException\n    commentId: T:System.ArgumentOutOfRangeException\n    description: <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\"></xref> is set to a value less than 0.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)\n  id: QueryRecognizerSetting(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: QueryRecognizerSetting(String)\n  nameWithType: SpeechRecognitionEngine.QueryRecognizerSetting(String)\n  fullName: SpeechRecognitionEngine.QueryRecognizerSetting(String)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Returns the values of settings for the recognizer.\n  remarks: \"Recognizer settings can contain string, 64-bit integer, or memory address data. The following table describes the settings that are defined for a Microsoft Speech API (SAPI)-compliant recognizer. The following settings must have the same range for each recognizer that supports the setting. A SAPI-compliant recognizer is not required to support these settings and can support other settings.  \\n  \\n|Name|Description|  \\n|----------|-----------------|  \\n|`ResourceUsage`|Specifies the recognizer's CPU consumption. The range is from 0 to 100. The default value is 50.|  \\n|`ResponseSpeed`|Indicates the length of silence at the end of unambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000 milliseconds (ms). This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> property.  Default = 150ms.|  \\n|`ComplexResponseSpeed`|Indicates the length of silence at the end of ambiguous input before the speech recognizer completes a recognition operation. The range is from 0 to 10,000ms. This setting corresponds to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property. Default = 500ms.|  \\n|`AdaptationOn`|Indicates whether adaptation of the acoustic model is ON (value = `1`) or OFF (value = `0`). The default value is `1` (ON).|  \\n|`PersistedBackgroundAdaptation`|Indicates whether background adaptation is ON (value = `1`) or OFF (value = `0`), and persists the setting in the registry. The default value is `1` (ON).|  \\n  \\n To update a setting for the recognizer, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods.\"\n  example:\n  - \"The following example is part of a console application that outputs the values for a number of the settings defined for the recognizer that supports the en-US locale. The example generates the following output.  \\n  \\n```  \\nSettings for recognizer MS-1033-80-DESK:  \\n  \\n  ResourceUsage                  is not supported by this recognizer.  \\n  ResponseSpeed                  = 150  \\n  ComplexResponseSpeed           = 500  \\n  AdaptationOn                   = 1  \\n  PersistedBackgroundAdaptation  = 1  \\n  \\nPress any key to exit...  \\n```  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace RecognizerSettings  \\n{  \\n  class Program  \\n  {  \\n    static readonly string[] settings = new string[] {  \\n      \\\"ResourceUsage\\\",  \\n      \\\"ResponseSpeed\\\",  \\n      \\\"ComplexResponseSpeed\\\",  \\n      \\\"AdaptationOn\\\",  \\n      \\\"PersistedBackgroundAdaptation\\\"  \\n    };  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        Console.WriteLine(\\\"Settings for recognizer {0}:\\\",  \\n          recognizer.RecognizerInfo.Name);  \\n        Console.WriteLine();  \\n  \\n        foreach (string setting in settings)  \\n        {  \\n          try  \\n          {  \\n            object value = recognizer.QueryRecognizerSetting(setting);  \\n            Console.WriteLine(\\\"  {0,-30} = {1}\\\", setting, value);  \\n          }  \\n          catch  \\n          {  \\n            Console.WriteLine(\\\"  {0,-30} is not supported by this recognizer.\\\",  \\n              setting);  \\n          }  \\n        }  \\n      }  \\n      Console.WriteLine();  \\n  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public object QueryRecognizerSetting (string settingName);\n    parameters:\n    - id: settingName\n      type: System.String\n      description: The name of the setting to return.\n    return:\n      type: System.Object\n      description: The value of the setting.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting*\n  exceptions:\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>settingName</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>settingName</code> is the empty string (\"\").\n  - type: System.Collections.Generic.KeyNotFoundException\n    commentId: T:System.Collections.Generic.KeyNotFoundException\n    description: The recognizer does not have a setting by that name.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Recognize\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize\n  id: Recognize\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: Recognize()\n  nameWithType: SpeechRecognitionEngine.Recognize()\n  fullName: SpeechRecognitionEngine.Recognize()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Performs a synchronous speech recognition operation.\n  remarks: \"This method performs a single recognition operation. The recognizer performs this operation against its loaded and enabled speech recognition grammars.  \\n  \\n During a call to this method, the recognizer can raise the following events:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \\n  \\n The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method.  \\n  \\n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> method returns a <xref:System.Speech.Recognition.RecognitionResult> object, or `null` if the operation is not successful.  \\n  \\n A synchronous recognition operation can fail for the following reasons:  \\n  \\n-   Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties.  \\n  \\n-   The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.  \\n  \\n To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. The example creates a <xref:System.Speech.Recognition.DictationGrammar>, loads it into an in-process speech recognizer, and performs one recognition operation.  \\n  \\n```  \\n  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SynchronousRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n      // Create an in-process speech recognizer for the en-US locale.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(  \\n          new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Modify the initial silence time-out value.  \\n        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  \\n  \\n        // Start synchronous speech recognition.  \\n        RecognitionResult result = recognizer.Recognize();  \\n  \\n        if (result != null)  \\n        {  \\n          Console.WriteLine(\\\"Recognized text = {0}\\\", result.Text);  \\n        }  \\n        else  \\n        {  \\n          Console.WriteLine(\\\"No recognition result available.\\\");  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to continue...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult Recognize ();\n    parameters: []\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The recognition result for the input, or `null` if the operation is not successful or the recognizer is not enabled.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.Recognize*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)\n  id: Recognize(System.TimeSpan)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: Recognize(TimeSpan)\n  nameWithType: SpeechRecognitionEngine.Recognize(TimeSpan)\n  fullName: SpeechRecognitionEngine.Recognize(TimeSpan)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Performs a synchronous speech recognition operation with a specified initial silence timeout period.\n  remarks: \"If the speech recognition engine detects speech within the time interval specified by `initialSilenceTimeout` argument, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> performs a single recognition operation and then terminates.  The `initialSilenceTimeout` parameter supersedes the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> property.  \\n  \\n During a call to this method, the recognizer can raise the following events:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \\n  \\n The recognizer does not raise the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event when using this method.  \\n  \\n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> method returns a <xref:System.Speech.Recognition.RecognitionResult> object, or `null` if the operation is not successful.  \\n  \\n A synchronous recognition operation can fail for the following reasons:  \\n  \\n-   Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or for the `initialSilenceTimeout` parameter.  \\n  \\n-   The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.  \\n  \\n To perform asynchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. The example creates a <xref:System.Speech.Recognition.DictationGrammar>, loads it into an in-process speech recognizer, and performs one recognition operation.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SynchronousRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n      // Create an in-process speech recognizer for the en-US locale.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(  \\n          new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Create and load a dictation grammar.  \\n        recognizer.LoadGrammar(new DictationGrammar());  \\n  \\n        // Configure input to the speech recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start synchronous speech recognition.  \\n        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  \\n  \\n        if (result != null)  \\n        {  \\n          Console.WriteLine(\\\"Recognized text = {0}\\\", result.Text);  \\n        }  \\n        else  \\n        {  \\n          Console.WriteLine(\\\"No recognition result available.\\\");  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to continue...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);\n    parameters:\n    - id: initialSilenceTimeout\n      type: System.TimeSpan\n      description: The interval of time a speech recognizer accepts input containing only silence before finalizing recognition.\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The recognition result for the input, or `null` if the operation is not successful or the recognizer is not enabled.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.Recognize*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync\n  id: RecognizeAsync\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizeAsync()\n  nameWithType: SpeechRecognitionEngine.RecognizeAsync()\n  fullName: SpeechRecognitionEngine.RecognizeAsync()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Performs a single, asynchronous speech recognition operation.\n  remarks: \"This method performs a single, asynchronous recognition operation. The recognizer performs the operation against its loaded and enabled speech recognition grammars.  \\n  \\n During a call to this method, the recognizer can raise the following events:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.  \\n  \\n To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation. If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`.  \\n  \\n To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic asynchronous speech recognition. The example creates a <xref:System.Speech.Recognition.DictationGrammar>, loads it into an in-process speech recognizer, and performs one asynchronous recognition operation. Event handlers are included to demonstrate the events that the recognizer raises during the operation.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace AsynchronousRecognition  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether asynchronous recognition is complete.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      // Create an in-process speech recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Create a grammar for choosing cities for a flight.  \\n        Choices cities = new Choices(new string[]   \\n        { \\\"Los Angeles\\\", \\\"New York\\\", \\\"Chicago\\\", \\\"San Francisco\\\", \\\"Miami\\\", \\\"Dallas\\\" });  \\n  \\n        GrammarBuilder gb = new GrammarBuilder();  \\n        gb.Append(\\\"I want to fly from\\\");  \\n        gb.Append(cities);  \\n        gb.Append(\\\"to\\\");  \\n        gb.Append(cities);  \\n  \\n        // Construct a Grammar object and load it to the recognizer.  \\n        Grammar cityChooser = new Grammar(gb);  \\n        cityChooser.Name = (\\\"City Chooser\\\");  \\n        recognizer.LoadGrammarAsync(cityChooser);  \\n  \\n        // Attach event handlers.  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(  \\n            SpeechDetectedHandler);  \\n        recognizer.SpeechHypothesized +=  \\n          new EventHandler<SpeechHypothesizedEventArgs>(  \\n            SpeechHypothesizedHandler);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(  \\n            SpeechRecognitionRejectedHandler);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Assign input to the recognizer and start an asynchronous  \\n        // recognition operation.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        completed = false;  \\n        Console.WriteLine(\\\"Starting asynchronous recognition...\\\");  \\n        recognizer.RecognizeAsync();  \\n  \\n        // Wait for the operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n        Console.WriteLine(\\\"Done.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechDetected event.  \\n    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechDetectedHandler:\\\");  \\n      Console.WriteLine(\\\" - AudioPosition = {0}\\\", e.AudioPosition);  \\n    }  \\n  \\n    // Handle the SpeechHypothesized event.  \\n    static void SpeechHypothesizedHandler(  \\n      object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechHypothesizedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognitionRejected event.  \\n    static void SpeechRecognitionRejectedHandler(  \\n      object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognitionRejectedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognizedHandler.\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In RecognizeCompletedHandler.\\\");  \\n  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Error occurred during recognition: {0}\\\", e.Error);  \\n        return;  \\n      }  \\n      if (e.InitialSilenceTimeout || e.BabbleTimeout)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - BabbleTimeout = {0}; InitialSilenceTimeout = {1}\\\",  \\n          e.BabbleTimeout, e.InitialSilenceTimeout);  \\n        return;  \\n      }  \\n      if (e.InputStreamEnded)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - AudioPosition = {0}; InputStreamEnded = {1}\\\",  \\n          e.AudioPosition, e.InputStreamEnded);  \\n      }  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Grammar = {0}; Text = {1}; Confidence = {2}\\\",  \\n          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  \\n        Console.WriteLine(\\\" - AudioPosition = {0}\\\", e.AudioPosition);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\" - No result.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void RecognizeAsync ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)\n  id: RecognizeAsync(System.Speech.Recognition.RecognizeMode)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizeAsync(RecognizeMode)\n  nameWithType: SpeechRecognitionEngine.RecognizeAsync(RecognizeMode)\n  fullName: SpeechRecognitionEngine.RecognizeAsync(RecognizeMode)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Performs one or more asynchronous speech recognition operations.\n  remarks: \"If `mode` is <xref:System.Speech.Recognition.RecognizeMode.Multiple>, the recognizer continues performing asynchronous recognition operations until the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method is called.  \\n  \\n During a call to this method, the recognizer can raise the following events:  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Raised when the recognizer detects input that it can identify as speech.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Raised when input creates an ambiguous match with one of the active grammars.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Raised when the recognizer finalizes a recognition operation.  \\n  \\n-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Raised when a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operation finishes.  \\n  \\n To retrieve the result of an asynchronous recognition operation, attach an event handler to the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. The recognizer raises this event whenever it successfully completes a synchronous or asynchronous recognition operation. If recognition was not successful, the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> property on <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object, which you can access in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, will be `null`.  \\n  \\n An asynchronous recognition operation can fail for the following reasons:  \\n  \\n-   Speech is not detected before the timeout intervals expire for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> properties.  \\n  \\n-   The recognition engine detects speech but finds no matches in any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects.  \\n  \\n To perform synchronous recognition, use one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> methods.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates basic asynchronous speech recognition. The example creates a <xref:System.Speech.Recognition.DictationGrammar>, loads it into an in-process speech recognizer, and performs multiple asynchronous recognition operations. The asynchronous operations are cancelled after 30 seconds. Event handlers are included to demonstrate the events that the recognizer raises during the operation.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace AsynchronousRecognition  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether asynchronous recognition is complete.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      // Create an in-process speech recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Create a grammar for choosing cities for a flight.  \\n        Choices cities = new Choices(new string[] { \\\"Los Angeles\\\", \\\"New York\\\", \\\"Chicago\\\", \\\"San Francisco\\\", \\\"Miami\\\", \\\"Dallas\\\" });  \\n  \\n        GrammarBuilder gb = new GrammarBuilder();  \\n        gb.Append(\\\"I want to fly from\\\");  \\n        gb.Append(cities);  \\n        gb.Append(\\\"to\\\");  \\n        gb.Append(cities);  \\n  \\n        // Construct a Grammar object and load it to the recognizer.  \\n        Grammar cityChooser = new Grammar(gb);  \\n        cityChooser.Name = (\\\"City Chooser\\\");  \\n        recognizer.LoadGrammarAsync(cityChooser);  \\n  \\n        // Attach event handlers.  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(  \\n            SpeechDetectedHandler);  \\n        recognizer.SpeechHypothesized +=  \\n          new EventHandler<SpeechHypothesizedEventArgs>(  \\n            SpeechHypothesizedHandler);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(  \\n            SpeechRecognitionRejectedHandler);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Assign input to the recognizer and start asynchronous  \\n        // recognition.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        completed = false;  \\n        Console.WriteLine(\\\"Starting asynchronous recognition...\\\");  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Wait 30 seconds, and then cancel asynchronous recognition.  \\n        Thread.Sleep(TimeSpan.FromSeconds(30));  \\n        recognizer.RecognizeAsyncCancel();  \\n  \\n        // Wait for the operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n        Console.WriteLine(\\\"Done.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechDetected event.  \\n    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechDetectedHandler:\\\");  \\n      Console.WriteLine(\\\" - AudioPosition = {0}\\\", e.AudioPosition);  \\n    }  \\n  \\n    // Handle the SpeechHypothesized event.  \\n    static void SpeechHypothesizedHandler(  \\n      object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechHypothesizedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognitionRejected event.  \\n    static void SpeechRecognitionRejectedHandler(  \\n      object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognitionRejectedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognizedHandler.\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In RecognizeCompletedHandler.\\\");  \\n  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Error occurred during recognition: {0}\\\", e.Error);  \\n        return;  \\n      }  \\n      if (e.InitialSilenceTimeout || e.BabbleTimeout)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - BabbleTimeout = {0}; InitialSilenceTimeout = {1}\\\",  \\n          e.BabbleTimeout, e.InitialSilenceTimeout);  \\n        return;  \\n      }  \\n      if (e.InputStreamEnded)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - AudioPosition = {0}; InputStreamEnded = {1}\\\",  \\n          e.AudioPosition, e.InputStreamEnded);  \\n      }  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Grammar = {0}; Text = {1}; Confidence = {2}\\\",  \\n          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  \\n        Console.WriteLine(\\\" - AudioPosition = {0}\\\", e.AudioPosition);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\" - No result.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);\n    parameters:\n    - id: mode\n      type: System.Speech.Recognition.RecognizeMode\n      description: Indicates whether to perform one or multiple recognition operations.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel\n  id: RecognizeAsyncCancel\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizeAsyncCancel()\n  nameWithType: SpeechRecognitionEngine.RecognizeAsyncCancel()\n  fullName: SpeechRecognitionEngine.RecognizeAsyncCancel()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Terminates asynchronous recognition without waiting for the current recognition operation to complete.\n  remarks: \"This method immediately finalizes asynchronous recognition. If the current asynchronous recognition operation is receiving input, the input is truncated and the operation completes with the existing input. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is canceled, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`. This method cancels asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \\n  \\n To stop asynchronous recognition without truncating the input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates the use of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> method. The example creates and loads a speech recognition grammar, initiates a continuing asynchronous recognition operation, and then pauses 2 seconds before it cancels the operation. The recognizer receives input from the file, c:\\\\temp\\\\audioinput\\\\sample.wav. Event handlers are included to demonstrate the events that the recognizer raises during the operation.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace AsynchronousRecognition  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether asynchronous recognition is complete.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      // Create an in-process speech recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Create and load a dictation grammar.  \\n        Grammar dictation = new DictationGrammar();  \\n        dictation.Name = \\\"Dictation Grammar\\\";  \\n  \\n        recognizer.LoadGrammar(dictation);  \\n  \\n        // Attach event handlers.  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(  \\n            SpeechDetectedHandler);  \\n        recognizer.SpeechHypothesized +=  \\n          new EventHandler<SpeechHypothesizedEventArgs>(  \\n            SpeechHypothesizedHandler);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(  \\n            SpeechRecognitionRejectedHandler);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Begin asynchronous recognition from pre-recorded input.  \\n        recognizer.SetInputToWaveFile(@\\\"c:\\\\temp\\\\audioinput\\\\sample.wav\\\");  \\n  \\n        completed = false;  \\n        Console.WriteLine(\\\"Begin continuing asynchronous recognition...\\\");  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Wait 2 seconds and then cancel the recognition operation.  \\n        Thread.Sleep(TimeSpan.FromSeconds(2));  \\n        recognizer.RecognizeAsyncCancel();  \\n  \\n        // Wait for the operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        Console.WriteLine(\\\"Done.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechDetected event.  \\n    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechDetectedHandler:\\\");  \\n      Console.WriteLine(\\\" - AudioPosition = {0}\\\", e.AudioPosition);  \\n    }  \\n  \\n    // Handle the SpeechHypothesized event.  \\n    static void SpeechHypothesizedHandler(  \\n      object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechHypothesizedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognitionRejected event.  \\n    static void SpeechRecognitionRejectedHandler(  \\n      object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognitionRejectedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognizedHandler.\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In RecognizeCompletedHandler.\\\");  \\n  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Error occurred during recognition: {0}\\\", e.Error);  \\n        return;  \\n      }  \\n      if (e.Cancelled)  \\n      {  \\n        Console.WriteLine(\\\" - asynchronous operation canceled.\\\");  \\n      }  \\n      if (e.InitialSilenceTimeout || e.BabbleTimeout)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - BabbleTimeout = {0}; InitialSilenceTimeout = {1}\\\",  \\n          e.BabbleTimeout, e.InitialSilenceTimeout);  \\n        return;  \\n      }  \\n      if (e.InputStreamEnded)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - AudioPosition = {0}; InputStreamEnded = {1}\\\",  \\n          e.AudioPosition, e.InputStreamEnded);  \\n      }  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Grammar = {0}; Text = {1}; Confidence = {2}\\\",  \\n          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\" - No result.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public void RecognizeAsyncCancel ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop\n  id: RecognizeAsyncStop\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizeAsyncStop()\n  nameWithType: SpeechRecognitionEngine.RecognizeAsyncStop()\n  fullName: SpeechRecognitionEngine.RecognizeAsyncStop()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Stops asynchronous recognition after the current recognition operation completes.\n  remarks: \"This method finalizes asynchronous recognition without truncating input. If the current asynchronous recognition operation is receiving input, the recognizer continues accepting input until the current recognition operation is completed. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> event when an asynchronous operation is stopped, and sets the <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> property of the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> to `true`. This method stops asynchronous operations initiated by the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \\n  \\n To immediately cancel asynchronous recognition with only the existing input, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> method.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates the use of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> method. The example creates and loads a speech recognition grammar, initiates a continuing asynchronous recognition operation, and then pauses 2 seconds before it stops the operation. The recognizer receives input from the file, c:\\\\temp\\\\audioinput\\\\sample.wav. Event handlers are included to demonstrate the events that the recognizer raises during the operation.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace AsynchronousRecognition  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether asynchronous recognition is complete.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      // Create an in-process speech recognizer.  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        // Create and load a dictation grammar.  \\n        Grammar dictation = new DictationGrammar();  \\n        dictation.Name = \\\"Dictation Grammar\\\";  \\n  \\n        recognizer.LoadGrammar(dictation);  \\n  \\n        // Attach event handlers.  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(  \\n            SpeechDetectedHandler);  \\n        recognizer.SpeechHypothesized +=  \\n          new EventHandler<SpeechHypothesizedEventArgs>(  \\n            SpeechHypothesizedHandler);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(  \\n            SpeechRecognitionRejectedHandler);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Begin asynchronous recognition from pre-recorded input.  \\n        recognizer.SetInputToWaveFile(@\\\"c:\\\\temp\\\\audioinput\\\\sample.wav\\\");  \\n  \\n        completed = false;  \\n        Console.WriteLine(\\\"Begin continuing asynchronous recognition...\\\");  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Wait 2 seconds and then stop the recognition operation.  \\n        Thread.Sleep(TimeSpan.FromSeconds(2));  \\n        recognizer.RecognizeAsyncStop();  \\n  \\n        // Wait for the operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        Console.WriteLine(\\\"Done.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechDetected event.  \\n    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechDetectedHandler:\\\");  \\n      Console.WriteLine(\\\" - AudioPosition = {0}\\\", e.AudioPosition);  \\n    }  \\n  \\n    // Handle the SpeechHypothesized event.  \\n    static void SpeechHypothesizedHandler(  \\n      object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechHypothesizedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognitionRejected event.  \\n    static void SpeechRecognitionRejectedHandler(  \\n      object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognitionRejectedHandler:\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In SpeechRecognizedHandler.\\\");  \\n  \\n      string grammarName = \\\"<not available>\\\";  \\n      string resultText = \\\"<not available>\\\";  \\n      if (e.Result != null)  \\n      {  \\n        if (e.Result.Grammar != null)  \\n        {  \\n          grammarName = e.Result.Grammar.Name;  \\n        }  \\n        resultText = e.Result.Text;  \\n      }  \\n  \\n      Console.WriteLine(\\\" - Grammar Name = {0}; Result Text = {1}\\\",  \\n        grammarName, resultText);  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\" In RecognizeCompletedHandler.\\\");  \\n  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Error occurred during recognition: {0}\\\", e.Error);  \\n        return;  \\n      }  \\n      if (e.Cancelled)  \\n      {  \\n        Console.WriteLine(\\\" - asynchronous operation canceled.\\\");  \\n      }  \\n      if (e.InitialSilenceTimeout || e.BabbleTimeout)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - BabbleTimeout = {0}; InitialSilenceTimeout = {1}\\\",  \\n          e.BabbleTimeout, e.InitialSilenceTimeout);  \\n        return;  \\n      }  \\n      if (e.InputStreamEnded)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - AudioPosition = {0}; InputStreamEnded = {1}\\\",  \\n          e.AudioPosition, e.InputStreamEnded);  \\n      }  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\" - Grammar = {0}; Text = {1}; Confidence = {2}\\\",  \\n          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\" - No result.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void RecognizeAsyncStop ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted\n  id: RecognizeCompleted\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizeCompleted\n  nameWithType: SpeechRecognitionEngine.RecognizeCompleted\n  fullName: SpeechRecognitionEngine.RecognizeCompleted\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> finalizes an asynchronous recognition operation.\n  remarks: \"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> method initiates an asynchronous recognition operation. When the recognizer finalizes the asynchronous operation, it raises this event.  \\n  \\n Using the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event, you can access the <xref:System.Speech.Recognition.RecognitionResult> in the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> object. If recognition was not successful, <xref:System.Speech.Recognition.RecognitionResult> will be `null`. To determine whether a timeout or an interruption in audio input caused recognition to fail, you can access the properties for <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, or <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  \\n  \\n See the <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> class for more information.  \\n  \\n To obtain details on the best rejected recognition candidates, attach a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event.  \\n  \\n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example recognizes phrases such as \\\"Display the list of artists in the jazz category\\\" or \\\"Display albums gospel\\\". The example uses a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> event to display information about the results of recognition in the console.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize an in-process speech recognition engine.  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n         new SpeechRecognitionEngine())  \\n      {  \\n  \\n        //  Create lists of alternative choices.  \\n        Choices listTypes = new Choices(new string[] { \\\"albums\\\", \\\"artists\\\" });  \\n        Choices genres = new Choices(new string[] {   \\n          \\\"blues\\\", \\\"classical\\\", \\\"gospel\\\", \\\"jazz\\\", \\\"rock\\\" });  \\n  \\n        //  Create a GrammarBuilder object and assemble the grammar components.  \\n        GrammarBuilder mediaMenu = new GrammarBuilder(\\\"Display\\\");  \\n        mediaMenu.Append(\\\"the list of\\\", 0, 1);  \\n        mediaMenu.Append(listTypes);  \\n        mediaMenu.Append(\\\"in the\\\", 0, 1);  \\n        mediaMenu.Append(genres);  \\n        mediaMenu.Append(\\\"category.\\\", 0, 1);  \\n  \\n        //  Build a Grammar object from the GrammarBuilder.  \\n        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  \\n        mediaMenuGrammar.Name = \\\"Media Chooser\\\";  \\n  \\n        // Attach event handlers.  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  \\n        recognizer.LoadGrammarCompleted +=   \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n  \\n        // Load the grammar object to the recognizer.  \\n        recognizer.LoadGrammarAsync(mediaMenuGrammar);  \\n  \\n        // Set the input to the recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous, continuous recognition.  \\n        recognizer.RecognizeAsync();  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(  \\n          \\\"RecognizeCompleted, error occurred during recognition: {0}\\\", e.Error);  \\n        return;  \\n      }  \\n  \\n      if (e.InitialSilenceTimeout || e.BabbleTimeout)  \\n      {  \\n        Console.WriteLine(  \\n          \\\"RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).\\\",  \\n          e.BabbleTimeout, e.InitialSilenceTimeout);  \\n        return;  \\n      }  \\n  \\n      if (e.InputStreamEnded)  \\n      {  \\n        Console.WriteLine(  \\n          \\\"RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).\\\",  \\n          e.AudioPosition, e.InputStreamEnded);  \\n      }  \\n  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"RecognizeCompleted:\\\");  \\n        Console.WriteLine(\\\"  Grammar: \\\" + e.Result.Grammar.Name);  \\n        Console.WriteLine(\\\"  Recognized text: \\\" + e.Result.Text);  \\n        Console.WriteLine(\\\"  Confidence score: \\\" + e.Result.Confidence);  \\n        Console.WriteLine(\\\"  Audio position: \\\" + e.AudioPosition);  \\n      }  \\n  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"RecognizeCompleted: No result.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded:  \\\" + e.Grammar.Name);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.RecognizeCompletedEventArgs> RecognizeCompleted;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.RecognizeCompletedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition\n  id: RecognizerAudioPosition\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizerAudioPosition\n  nameWithType: SpeechRecognitionEngine.RecognizerAudioPosition\n  fullName: SpeechRecognitionEngine.RecognizerAudioPosition\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the current location of the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> in the audio input that it is processing.\n  remarks: \"The audio position is specific to each speech recognizer. The zero value of an input stream is established when it is enabled.  \\n  \\n The <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property references the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object's position within its audio input. By contrast, the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property references the input device's position in its generated audio stream. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> property.\"\n  syntax:\n    content: public TimeSpan RecognizerAudioPosition { get; }\n    return:\n      type: System.TimeSpan\n      description: The position of the recognizer in the audio input that it is processing.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo\n  commentId: P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo\n  id: RecognizerInfo\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizerInfo\n  nameWithType: SpeechRecognitionEngine.RecognizerInfo\n  fullName: SpeechRecognitionEngine.RecognizerInfo\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets information about the current instance of <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref>.\n  remarks: To get information about all of the installed speech recognizers for the current system, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> method.\n  example:\n  - \"The following example gets a partial list of data for the current in-process speech recognition engine. For more information, see <xref:System.Speech.Recognition.RecognizerInfo>.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace RecognitionEngine  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  \\n      {  \\n        Console.WriteLine(\\\"Information for the current speech recognition engine:\\\");  \\n        Console.WriteLine(\\\"  Name: {0}\\\", recognizer.RecognizerInfo.Name);  \\n        Console.WriteLine(\\\"  Culture: {0}\\\", recognizer.RecognizerInfo.Culture.ToString());  \\n        Console.WriteLine(\\\"  Description: {0}\\\", recognizer.RecognizerInfo.Description);  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }\n    return:\n      type: System.Speech.Recognition.RecognizerInfo\n      description: Information about the current speech recognizer.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached\n  id: RecognizerUpdateReached\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RecognizerUpdateReached\n  nameWithType: SpeechRecognitionEngine.RecognizerUpdateReached\n  fullName: SpeechRecognitionEngine.RecognizerUpdateReached\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when a running <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> pauses to accept modifications.\n  remarks: \"Applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine> before modifying its settings or its <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises this event when it is ready to accept modifications.  \\n  \\n For example, while the <xref:System.Speech.Recognition.SpeechRecognitionEngine> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects, and modify values for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> properties. For more information, see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.  \\n  \\n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example shows a console application that loads and unloads <xref:System.Speech.Recognition.Grammar> objects. The application uses the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method to request the speech recognition engine to pause so it can receive an update. The application then loads or unloads a <xref:System.Speech.Recognition.Grammar> object.  \\n  \\n At each update, a handler for <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event writes the name and status of the currently loaded <xref:System.Speech.Recognition.Grammar> objects to the console. As grammars are loaded and unloaded, the application first recognizes the names of farm animals, then the names of farm animals and the names of fruits, then only the names of fruits.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Collections.Generic;  \\nusing System.Threading;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognitionEngine recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an in-process speech recognition engine and configure its input.  \\n      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Create the first grammar - Farm.  \\n        Choices animals = new Choices(new string[] { \\\"cow\\\", \\\"pig\\\", \\\"goat\\\" });  \\n        GrammarBuilder farm = new GrammarBuilder(animals);  \\n        Grammar farmAnimals = new Grammar(farm);  \\n        farmAnimals.Name = \\\"Farm\\\";  \\n  \\n        // Create the second grammar - Fruit.  \\n        Choices fruit = new Choices(new string[] { \\\"apples\\\", \\\"peaches\\\", \\\"oranges\\\" });  \\n        GrammarBuilder favorite = new GrammarBuilder(fruit);  \\n        Grammar favoriteFruit = new Grammar(favorite);  \\n        favoriteFruit.Name = \\\"Fruit\\\";  \\n  \\n        // Attach event handlers.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.RecognizerUpdateReached +=  \\n          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  \\n  \\n        // Load the Farm grammar.  \\n        recognizer.LoadGrammar(farmAnimals);  \\n  \\n        // Start asynchronous, continuous recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n        Console.WriteLine(\\\"Starting asynchronous, continuous recognition\\\");  \\n        Console.WriteLine(\\\"  Farm grammar is loaded and enabled.\\\");  \\n  \\n        // Pause to recognize farm animals.  \\n        Thread.Sleep(7000);  \\n        Console.WriteLine();  \\n  \\n        // Request an update and load the Fruit grammar.  \\n        recognizer.RequestRecognizerUpdate();  \\n        recognizer.LoadGrammarAsync(favoriteFruit);  \\n        Thread.Sleep(7000);  \\n  \\n        // Request an update and unload the Farm grammar.  \\n        recognizer.RequestRecognizerUpdate();  \\n        recognizer.UnloadGrammar(farmAnimals);  \\n        Thread.Sleep(7000);  \\n      }  \\n  \\n      // Keep the console window open.  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // At the update, get the names and enabled status of the currently loaded grammars.  \\n    public static void recognizer_RecognizerUpdateReached(  \\n      object sender, RecognizerUpdateReachedEventArgs e)  \\n    {  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Update reached:\\\");  \\n      Thread.Sleep(1000);  \\n  \\n      string qualifier;  \\n      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  \\n      foreach (Grammar g in grammars)  \\n      {  \\n        qualifier = (g.Enabled) ? \\\"enabled\\\" : \\\"disabled\\\";  \\n        Console.WriteLine(\\\"  {0} grammar is loaded and {1}.\\\",  \\n        g.Name, qualifier);  \\n      }  \\n    }  \\n  \\n    // Write the text of the recognized phrase to the console.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"    Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  \\n    // Write a message to the console when recognition fails.  \\n    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"    Recognition attempt failed\\\");  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.RecognizerUpdateReachedEventArgs> RecognizerUpdateReached;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.RecognizerUpdateReachedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  id: RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RequestRecognizerUpdate(Object, TimeSpan)\n  nameWithType: SpeechRecognitionEngine.RequestRecognizerUpdate(Object, TimeSpan)\n  fullName: SpeechRecognitionEngine.RequestRecognizerUpdate(Object, TimeSpan)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Requests that the recognizer pauses to update its state and provides an offset and a user token for the associated event.\n  remarks: \"The recognizer does not initiate the recognizer update request until the recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> equals the current <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  \\n  \\n When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.\"\n  syntax:\n    content: public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);\n    parameters:\n    - id: userToken\n      type: System.Object\n      description: User-defined information that contains information for the operation.\n    - id: audioPositionAheadToRaiseUpdate\n      type: System.TimeSpan\n      description: The offset from the current <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\"></xref> to delay the request.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)\n  id: RequestRecognizerUpdate(System.Object)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RequestRecognizerUpdate(Object)\n  nameWithType: SpeechRecognitionEngine.RequestRecognizerUpdate(Object)\n  fullName: SpeechRecognitionEngine.RequestRecognizerUpdate(Object)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Requests that the recognizer pauses to update its state and provides a user token for the associated event.\n  remarks: \"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.  \\n  \\n To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.\"\n  syntax:\n    content: public void RequestRecognizerUpdate (object userToken);\n    parameters:\n    - id: userToken\n      type: System.Object\n      description: User-defined information that contains information for the operation.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate\n  id: RequestRecognizerUpdate\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: RequestRecognizerUpdate()\n  nameWithType: SpeechRecognitionEngine.RequestRecognizerUpdate()\n  fullName: SpeechRecognitionEngine.RequestRecognizerUpdate()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Requests that the recognizer pauses to update its state.\n  remarks: \"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> is `null`.  \\n  \\n To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method.\"\n  example:\n  - \"The following example shows a console application that loads and unloads <xref:System.Speech.Recognition.Grammar> objects. The application uses the <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> method to request the speech recognition engine to pause so it can receive an update. The application then loads or unloads a <xref:System.Speech.Recognition.Grammar> object.  \\n  \\n At each update, a handler for <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> event writes the name and status of the currently loaded <xref:System.Speech.Recognition.Grammar> objects to the console. As grammars are loaded and unloaded, the application first recognizes the names of farm animals, then the names of farm animals and the names of fruits, then only the names of fruits.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Collections.Generic;  \\nusing System.Threading;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognitionEngine recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an in-process speech recognition engine and configure its input.  \\n      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Create the first grammar - Farm.  \\n        Choices animals = new Choices(new string[] { \\\"cow\\\", \\\"pig\\\", \\\"goat\\\" });  \\n        GrammarBuilder farm = new GrammarBuilder(animals);  \\n        Grammar farmAnimals = new Grammar(farm);  \\n        farmAnimals.Name = \\\"Farm\\\";  \\n  \\n        // Create the second grammar - Fruit.  \\n        Choices fruit = new Choices(new string[] { \\\"apples\\\", \\\"peaches\\\", \\\"oranges\\\" });  \\n        GrammarBuilder favorite = new GrammarBuilder(fruit);  \\n        Grammar favoriteFruit = new Grammar(favorite);  \\n        favoriteFruit.Name = \\\"Fruit\\\";  \\n  \\n        // Attach event handlers.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.RecognizerUpdateReached +=  \\n          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  \\n  \\n        // Load the Farm grammar.  \\n        recognizer.LoadGrammar(farmAnimals);  \\n  \\n        // Start asynchronous, continuous recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n        Console.WriteLine(\\\"Starting asynchronous, continuous recognition\\\");  \\n        Console.WriteLine(\\\"  Farm grammar is loaded and enabled.\\\");  \\n  \\n        // Pause to recognize farm animals.  \\n        Thread.Sleep(7000);  \\n        Console.WriteLine();  \\n  \\n        // Request an update and load the Fruit grammar.  \\n        recognizer.RequestRecognizerUpdate();  \\n        recognizer.LoadGrammarAsync(favoriteFruit);  \\n        Thread.Sleep(7000);  \\n  \\n        // Request an update and unload the Farm grammar.  \\n        recognizer.RequestRecognizerUpdate();  \\n        recognizer.UnloadGrammar(farmAnimals);  \\n        Thread.Sleep(7000);  \\n      }  \\n  \\n      // Keep the console window open.  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // At the update, get the names and enabled status of the currently loaded grammars.  \\n    public static void recognizer_RecognizerUpdateReached(  \\n      object sender, RecognizerUpdateReachedEventArgs e)  \\n    {  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Update reached:\\\");  \\n      Thread.Sleep(1000);  \\n  \\n      string qualifier;  \\n      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  \\n      foreach (Grammar g in grammars)  \\n      {  \\n        qualifier = (g.Enabled) ? \\\"enabled\\\" : \\\"disabled\\\";  \\n        Console.WriteLine(\\\"  {0} grammar is loaded and {1}.\\\",  \\n        g.Name, qualifier);  \\n      }  \\n    }  \\n  \\n    // Write the text of the recognized phrase to the console.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"    Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  \\n    // Write a message to the console when recognition fails.  \\n    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"    Recognition attempt failed\\\");  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void RequestRecognizerUpdate ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)\n  id: SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SetInputToAudioStream(Stream, SpeechAudioFormatInfo)\n  nameWithType: SpeechRecognitionEngine.SetInputToAudioStream(Stream, SpeechAudioFormatInfo)\n  fullName: SpeechRecognitionEngine.SetInputToAudioStream(Stream, SpeechAudioFormatInfo)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from an audio stream.\n  remarks: If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. The example uses input from an audio file, example.wav, that contains the phrases, \\\"testing testing one two three\\\" and \\\"mister cooper\\\", separated by a pause. The example generates the following output.  \\n  \\n```  \\n  \\nStarting asynchronous recognition...  \\n  Recognized text =  Testing testing 123  \\n  Recognized text =  Mr. Cooper  \\n  End of stream encountered.  \\nDone.  \\n  \\nPress any key to exit...  \\n```  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Globalization;  \\nusing System.IO;  \\nusing System.Speech.AudioFormat;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace InputExamples  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether asynchronous recognition is complete.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create and load a grammar.  \\n        Grammar dictation = new DictationGrammar();  \\n        dictation.Name = \\\"Dictation Grammar\\\";  \\n  \\n        recognizer.LoadGrammar(dictation);  \\n  \\n        // Configure the input to the recognizer.  \\n        recognizer.SetInputToAudioStream(  \\n          File.OpenRead(@\\\"c:\\\\temp\\\\audioinput\\\\example.wav\\\"),  \\n          new SpeechAudioFormatInfo(  \\n            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  \\n  \\n        // Attach event handlers.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Perform recognition of the whole file.  \\n        Console.WriteLine(\\\"Starting asynchronous recognition...\\\");  \\n        completed = false;  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n        Console.WriteLine(\\\"Done.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null && e.Result.Text != null)  \\n      {  \\n        Console.WriteLine(\\\"  Recognized text =  {0}\\\", e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  Recognized text not available.\\\");  \\n      }  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\"  Error encountered, {0}: {1}\\\",  \\n          e.Error.GetType().Name, e.Error.Message);  \\n      }  \\n      if (e.Cancelled)  \\n      {  \\n        Console.WriteLine(\\\"  Operation cancelled.\\\");  \\n      }  \\n      if (e.InputStreamEnded)  \\n      {  \\n        Console.WriteLine(\\\"  End of stream encountered.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);\n    parameters:\n    - id: audioSource\n      type: System.IO.Stream\n      description: The audio input stream.\n    - id: audioFormat\n      type: System.Speech.AudioFormat.SpeechAudioFormatInfo\n      description: The format of the audio input.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice\n  id: SetInputToDefaultAudioDevice\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SetInputToDefaultAudioDevice()\n  nameWithType: SpeechRecognitionEngine.SetInputToDefaultAudioDevice()\n  fullName: SpeechRecognitionEngine.SetInputToDefaultAudioDevice()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from the default audio device.\n  remarks: ''\n  example:\n  - \"The following example shows part of a console application that demonstrates basic speech recognition. The example uses output from the default audio device, performs multiple, asynchronous recognition operations, and exits when a user utters the phrase, \\\"exit\\\".  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace DefaultInput  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether asynchronous recognition has finished.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create and load the exit grammar.  \\n        Grammar exitGrammar = new Grammar(new GrammarBuilder(\\\"exit\\\"));  \\n        exitGrammar.Name = \\\"Exit Grammar\\\";  \\n        recognizer.LoadGrammar(exitGrammar);  \\n  \\n        // Create and load the dictation grammar.  \\n        Grammar dictation = new DictationGrammar();  \\n        dictation.Name = \\\"Dictation Grammar\\\";  \\n        recognizer.LoadGrammar(dictation);  \\n  \\n        // Attach event handlers to the recognizer.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.RecognizeCompleted +=  \\n          new EventHandler<RecognizeCompletedEventArgs>(  \\n            RecognizeCompletedHandler);  \\n  \\n        // Assign input to the recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Begin asynchronous recognition.  \\n        Console.WriteLine(\\\"Starting recognition...\\\");  \\n        completed = false;  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Wait for recognition to finish.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n        Console.WriteLine(\\\"Done.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"  Speech recognized:\\\");  \\n      string grammarName = \\\"<not available>\\\";  \\n      if (e.Result.Grammar.Name != null &&  \\n        !e.Result.Grammar.Name.Equals(string.Empty))  \\n      {  \\n        grammarName = e.Result.Grammar.Name;  \\n      }  \\n      Console.WriteLine(\\\"    {0,-17} - {1}\\\",  \\n        grammarName, e.Result.Text);  \\n  \\n      if (grammarName.Equals(\\\"Exit Grammar\\\"))  \\n      {  \\n        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  \\n      }  \\n    }  \\n  \\n    static void RecognizeCompletedHandler(  \\n      object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"  Recognition completed.\\\");  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public void SetInputToDefaultAudioDevice ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull\n  id: SetInputToNull\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SetInputToNull()\n  nameWithType: SpeechRecognitionEngine.SetInputToNull()\n  fullName: SpeechRecognitionEngine.SetInputToNull()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Disables the input to the speech recognizer.\n  remarks: Configure the <xref:System.Speech.Recognition.SpeechRecognitionEngine> object for no input when using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods, or when taking a recognition engine temporarily off line.\n  syntax:\n    content: public void SetInputToNull ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)\n  id: SetInputToWaveFile(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SetInputToWaveFile(String)\n  nameWithType: SpeechRecognitionEngine.SetInputToWaveFile(String)\n  fullName: SpeechRecognitionEngine.SetInputToWaveFile(String)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from a Waveform audio format (.wav) file.\n  remarks: If the recognizer reaches the end of the input file during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.\n  example:\n  - \"The following example performs recognition on the audio in a .wav file and writes the recognized text to the console.  \\n  \\n```  \\nusing System;  \\nusing System.IO;  \\nusing System.Speech.Recognition;  \\nusing System.Speech.AudioFormat;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n  \\n    // Initialize an in-process speech recognition engine.  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n         new SpeechRecognitionEngine())  \\n      {  \\n  \\n        // Create and load a grammar.  \\n        Grammar dictation = new DictationGrammar();  \\n        dictation.Name = \\\"Dictation Grammar\\\";  \\n  \\n        recognizer.LoadGrammar(dictation);  \\n  \\n        // Configure the input to the recognizer.  \\nrecognizer.SetInputToWaveFile(@\\\"c:\\\\temp\\\\SampleWAVInput.wav\\\");  \\n  \\n        // Attach event handlers for the results of recognition.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.RecognizeCompleted +=   \\n          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  \\n  \\n        // Perform recognition on the entire file.  \\n        Console.WriteLine(\\\"Starting asynchronous recognition...\\\");  \\n        completed = false;  \\n        recognizer.RecognizeAsync();  \\n  \\n        // Keep the console window open.  \\n        while (!completed)  \\n        {  \\n          Console.ReadLine();  \\n        }  \\n        Console.WriteLine(\\\"Done.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null && e.Result.Text != null)  \\n      {  \\n        Console.WriteLine(\\\"  Recognized text =  {0}\\\", e.Result.Text);  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  Recognized text not available.\\\");  \\n      }  \\n    }  \\n  \\n    // Handle the RecognizeCompleted event.  \\n    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\"  Error encountered, {0}: {1}\\\",  \\n        e.Error.GetType().Name, e.Error.Message);  \\n      }  \\n      if (e.Cancelled)  \\n      {  \\n        Console.WriteLine(\\\"  Operation cancelled.\\\");  \\n      }  \\n      if (e.InputStreamEnded)  \\n      {  \\n        Console.WriteLine(\\\"  End of stream encountered.\\\");  \\n      }  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void SetInputToWaveFile (string path);\n    parameters:\n    - id: path\n      type: System.String\n      description: The path of the file to use as input.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)\n  id: SetInputToWaveStream(System.IO.Stream)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SetInputToWaveStream(Stream)\n  nameWithType: SpeechRecognitionEngine.SetInputToWaveStream(Stream)\n  fullName: SpeechRecognitionEngine.SetInputToWaveStream(Stream)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Configures the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> object to receive input from a stream that contains Waveform audio format (.wav) data.\n  remarks: If the recognizer reaches the end of the input stream during a recognition operation, the recognition operation finalizes with the available input. Any subsequent recognition operations can generate an exception, unless you update the input to the recognizer.\n  syntax:\n    content: public void SetInputToWaveStream (System.IO.Stream audioSource);\n    parameters:\n    - id: audioSource\n      type: System.IO.Stream\n      description: The stream containing the audio data.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected\n  id: SpeechDetected\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechDetected\n  nameWithType: SpeechRecognitionEngine.SpeechDetected\n  fullName: SpeechRecognitionEngine.SpeechDetected\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> detects input that it can identify as speech.\n  remarks: \"Each speech recognizer has an algorithm to distinguish between silence and speech. When the <xref:System.Speech.Recognition.SpeechRecognitionEngine> performs a speech recognition operation, it raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event when its algorithm identifies the input as speech. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. The <xref:System.Speech.Recognition.SpeechRecognitionEngine> raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event before it raises any of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, or <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> events.  \\n  \\n For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \\n  \\n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example is part of a console application for choosing origin and destination cities for a flight. The application recognizes phrases such as \\\"I want to fly from Miami to Chicago.\\\"  The example uses the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> event to report the <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> each time speech is detected.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize an in-process speech recognition engine.  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n         new SpeechRecognitionEngine())  \\n      {  \\n  \\n        // Create a grammar.  \\n        Choices cities = new Choices(new string[] {   \\n          \\\"Los Angeles\\\", \\\"New York\\\", \\\"Chicago\\\", \\\"San Francisco\\\", \\\"Miami\\\", \\\"Dallas\\\" });  \\n  \\n        GrammarBuilder gb = new GrammarBuilder();  \\n        gb.Append(\\\"I would like to fly from\\\");  \\n        gb.Append(cities);  \\n        gb.Append(\\\"to\\\");  \\n        gb.Append(cities);  \\n  \\n        // Create a Grammar object and load it to the recognizer.  \\n        Grammar g = new Grammar(gb);  \\n        g.Name = (\\\"City Chooser\\\");  \\n        recognizer.LoadGrammarAsync(g);  \\n  \\n        // Attach event handlers.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n        recognizer.SpeechDetected +=  \\n          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Set the input to the recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start recognition.  \\n        recognizer.RecognizeAsync();  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechDetected event.  \\n    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"  Speech detected at AudioPosition = {0}\\\", e.AudioPosition);  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"  Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechDetectedEventArgs> SpeechDetected;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechDetectedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized\n  id: SpeechHypothesized\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechHypothesized\n  nameWithType: SpeechRecognitionEngine.SpeechHypothesized\n  fullName: SpeechRecognitionEngine.SpeechHypothesized\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> has recognized a word or words that may be a component of multiple complete phrases in a grammar.\n  remarks: \"The <xref:System.Speech.Recognition.SpeechRecognitionEngine> generates numerous <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> events as it attempts to identify an input phrase. You can access the text of partially recognized phrases in the <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> object in the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> event. Typically, handling these events is useful only for debugging.  \\n  \\n <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> derives from <xref:System.Speech.Recognition.RecognitionEventArgs>.  \\n  \\n For more information see the <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> property and the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> methods.  \\n  \\n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example recognizes phrases such as \\\"Display the list of artists in the jazz category\\\". The example uses the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> event to display incomplete phrase fragments in the console as they are recognized.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize an in-process speech recognition engine.  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n         new SpeechRecognitionEngine())  \\n      {  \\n  \\n        // Create a grammar.  \\n        //  Create lists of alternative choices.  \\n        Choices listTypes = new Choices(new string[] { \\\"albums\\\", \\\"artists\\\" });  \\n        Choices genres = new Choices(new string[] {   \\n          \\\"blues\\\", \\\"classical\\\", \\\"gospel\\\", \\\"jazz\\\", \\\"rock\\\" });  \\n  \\n        //  Create a GrammarBuilder object and assemble the grammar components.  \\n        GrammarBuilder mediaMenu = new GrammarBuilder(\\\"Display the list of\\\");  \\n        mediaMenu.Append(listTypes);  \\n        mediaMenu.Append(\\\"in the\\\");  \\n        mediaMenu.Append(genres);  \\n        mediaMenu.Append(\\\"category.\\\");  \\n  \\n        //  Build a Grammar object from the GrammarBuilder.  \\n        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  \\n        mediaMenuGrammar.Name = \\\"Media Chooser\\\";  \\n  \\n        // Attach event handlers.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.SpeechHypothesized +=  \\n          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  \\n  \\n        // Load the grammar object to the recognizer.  \\n        recognizer.LoadGrammarAsync(mediaMenuGrammar);  \\n  \\n        // Set the input to the recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start asynchronous recognition.  \\n        recognizer.RecognizeAsync();  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechHypothesized event.  \\n    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech hypothesized: \\\" + e.Result.Text);  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n      Console.WriteLine();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine();   \\n      Console.WriteLine(\\\"Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechHypothesizedEventArgs> SpeechHypothesized;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechHypothesizedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected\n  id: SpeechRecognitionRejected\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechRecognitionRejected\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionRejected\n  fullName: SpeechRecognitionEngine.SpeechRecognitionRejected\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> receives input that does not match any of its loaded and enabled <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects.\n  remarks: \"The recognizer raises this event if it determines that input does not match with sufficient confidence any of its loaded and enabled <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object. You can use the handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event to retrieve recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> that were rejected and their <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> scores.  \\n  \\n If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods. You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.  \\n  \\n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example recognizes phrases such as \\\"Display the list of artists in the jazz category\\\" or \\\"Display albums gospel\\\". The example uses a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event to display a notification in the console when the speech input cannot be matched to the contents of the grammar with sufficient <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> to produce a successful recognition. The handler also displays recognition result <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> that were rejected because of low confidence scores.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize an in-process speech recognition engine.  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n         new SpeechRecognitionEngine(new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n  \\n        // Create a grammar.  \\n        //  Create lists of alternative choices.  \\n        Choices listTypes = new Choices(new string[] { \\\"albums\\\", \\\"artists\\\" });  \\n        Choices genres = new Choices(new string[] {   \\n          \\\"blues\\\", \\\"classical\\\", \\\"gospel\\\", \\\"jazz\\\", \\\"rock\\\" });  \\n  \\n        //  Create a GrammarBuilder object and assemble the grammar components.  \\n        GrammarBuilder mediaMenu = new GrammarBuilder(\\\"Display\\\");  \\n        mediaMenu.Append(\\\"the list of\\\", 0, 1);  \\n        mediaMenu.Append(listTypes);  \\n        mediaMenu.Append(\\\"in the\\\", 0, 1);  \\n        mediaMenu.Append(genres);  \\n        mediaMenu.Append(\\\"category\\\", 0, 1);  \\n  \\n        //  Build a Grammar object from the GrammarBuilder.  \\n        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  \\n        mediaMenuGrammar.Name = \\\"Media Chooser\\\";  \\n  \\n        // Attach event handlers.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.SpeechRecognitionRejected +=  \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  \\n  \\n        // Load the grammar object to the recognizer.  \\n        recognizer.LoadGrammarAsync(mediaMenuGrammar);  \\n  \\n        // Set the input to the recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start recognition.  \\n        recognizer.RecognizeAsync(RecognizeMode.Multiple);  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognitionRejected event.  \\n    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech input was rejected.\\\");  \\n      foreach (RecognizedPhrase phrase in e.Result.Alternates)  \\n      {  \\n      Console.WriteLine(\\\"  Rejected phrase: \\\" + phrase.Text);  \\n      Console.WriteLine(\\\"  Confidence score: \\\" + phrase.Confidence);  \\n      }  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech recognized: \\\" + e.Result.Text);  \\n      Console.WriteLine(\\\"  Confidence score: \\\" + e.Result.Confidence);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> SpeechRecognitionRejected;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechRecognitionRejectedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\n  commentId: E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\n  id: SpeechRecognized\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: SpeechRecognized\n  nameWithType: SpeechRecognitionEngine.SpeechRecognized\n  fullName: SpeechRecognitionEngine.SpeechRecognized\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Raised when the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> receives input that matches any of its loaded and enabled <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects.\n  remarks: \"You can initiate a recognition operation using the one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> or <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> methods. The recognizer raises the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event if it determines that input matches one of its loaded <xref:System.Speech.Recognition.Grammar> objects with a sufficient level of confidence to constitute recognition. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object. Handlers of <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> events can obtain the recognized phrase as well as a list of recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> with lower confidence scores.  \\n  \\n If your application is using a <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, you can modify the confidence level at which speech input is accepted or rejected with one of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> methods.  You can modify how the speech recognition responds to non-speech input using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, and <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> properties.  \\n  \\n When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise its <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. Any tasks specific to a particular grammar should always be performed by a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event.  \\n  \\n When you create a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> delegate, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example is part of a console application that creates speech recognition grammar, constructs a <xref:System.Speech.Recognition.Grammar> object, and loads it into the <xref:System.Speech.Recognition.SpeechRecognitionEngine> to perform recognition. The example demonstrates speech input to a <xref:System.Speech.Recognition.SpeechRecognitionEngine>, the associated recognition results, and the associated events raised by the speech recognizer.  \\n  \\n Spoken input such as \\\"I want to fly from Chicago to Miami\\\" will trigger a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event. Speaking the phrase \\\"Fly me from Houston to Chicago \\\" will not trigger a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event.  \\n  \\n The example uses a handler for the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event to display successfully recognized phrases and the semantics they contain in the console.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize an in-process speech recognition engine.  \\n    {  \\n      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  \\n      {  \\n  \\n        // Create SemanticResultValue objects that contain cities and airport codes.  \\n        SemanticResultValue chicago = new SemanticResultValue(\\\"Chicago\\\", \\\"ORD\\\");  \\n        SemanticResultValue boston = new SemanticResultValue(\\\"Boston\\\", \\\"BOS\\\");  \\n        SemanticResultValue miami = new SemanticResultValue(\\\"Miami\\\", \\\"MIA\\\");  \\n        SemanticResultValue dallas = new SemanticResultValue(\\\"Dallas\\\", \\\"DFW\\\");  \\n  \\n        // Create a Choices object and add the SemanticResultValue objects, using  \\n        // implicit conversion from SemanticResultValue to GrammarBuilder  \\n        Choices cities = new Choices();  \\n        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  \\n  \\n        // Build the phrase and add SemanticResultKeys.  \\n        GrammarBuilder chooseCities = new GrammarBuilder();  \\n        chooseCities.Append(\\\"I want to fly from\\\");  \\n        chooseCities.Append(new SemanticResultKey(\\\"origin\\\", cities));  \\n        chooseCities.Append(\\\"to\\\");  \\n        chooseCities.Append(new SemanticResultKey(\\\"destination\\\", cities));  \\n  \\n        // Build a Grammar object from the GrammarBuilder.  \\n        Grammar bookFlight = new Grammar(chooseCities);  \\n        bookFlight.Name = \\\"Book Flight\\\";  \\n  \\n        // Add a handler for the LoadGrammarCompleted event.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n  \\n        // Add a handler for the SpeechRecognized event.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Load the grammar object to the recognizer.  \\n        recognizer.LoadGrammarAsync(bookFlight);  \\n  \\n        // Set the input to the recognizer.  \\n        recognizer.SetInputToDefaultAudioDevice();  \\n  \\n        // Start recognition.  \\n        recognizer.RecognizeAsync();  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n      Console.WriteLine();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech recognized:  \\\" + e.Result.Text);  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Semantic results:\\\");  \\n      Console.WriteLine(\\\"  The flight origin is \\\" + e.Result.Semantics[\\\"origin\\\"].Value);  \\n      Console.WriteLine(\\\"  The flight destination is \\\" + e.Result.Semantics[\\\"destination\\\"].Value);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechRecognizedEventArgs> SpeechRecognized;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechRecognizedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars\n  id: UnloadAllGrammars\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: UnloadAllGrammars()\n  nameWithType: SpeechRecognitionEngine.UnloadAllGrammars()\n  fullName: SpeechRecognitionEngine.UnloadAllGrammars()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Unloads all <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects from the recognizer.\n  remarks: \"If the recognizer is currently loading a <xref:System.Speech.Recognition.Grammar> asynchronously, this method waits until the <xref:System.Speech.Recognition.Grammar> is loaded, before it unloads all of the <xref:System.Speech.Recognition.Grammar> objects from the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  \\n  \\n To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> method.\"\n  example:\n  - \"The following example shows part of a console application that demonstrates the synchronous loading and unloading of speech recognition grammars.  \\n  \\n```  \\nLoading grammars...  \\nLoaded grammars:  \\n - Grammar1  \\n - Grammar2  \\n - Grammar3  \\n  \\nUnloading Grammar1...  \\nLoaded grammars:  \\n - Grammar2  \\n - Grammar3  \\n  \\nUnloading all grammars...  \\nNo grammars loaded.  \\n  \\nPress any key to exit...  \\n```  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Collections.Generic;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace UnloadGrammars  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        Console.WriteLine(\\\"Loading grammars...\\\");  \\n  \\n        // Create and load a number of grammars.  \\n        Grammar grammar1 = new Grammar(new GrammarBuilder(\\\"first grammar\\\"));  \\n        grammar1.Name = \\\"Grammar1\\\";  \\n        recognizer.LoadGrammar(grammar1);  \\n  \\n        Grammar grammar2 = new Grammar(new GrammarBuilder(\\\"second grammar\\\"));  \\n        grammar2.Name = \\\"Grammar2\\\";  \\n        recognizer.LoadGrammar(grammar2);  \\n  \\n        Grammar grammar3 = new Grammar(new GrammarBuilder(\\\"third grammar\\\"));  \\n        grammar3.Name = \\\"Grammar3\\\";  \\n        recognizer.LoadGrammar(grammar3);  \\n  \\n        // List the recognizer's loaded grammars.  \\n        ListGrammars(recognizer);  \\n  \\n        // Unload one grammar and list the loaded grammars.  \\n        Console.WriteLine(\\\"Unloading Grammar1...\\\");  \\n        recognizer.UnloadGrammar(grammar1);  \\n        ListGrammars(recognizer);  \\n  \\n        // Unload all grammars and list the loaded grammars.  \\n        Console.WriteLine(\\\"Unloading all grammars...\\\");  \\n        recognizer.UnloadAllGrammars();  \\n        ListGrammars(recognizer);  \\n      }  \\n  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    private static void ListGrammars(SpeechRecognitionEngine recognizer)  \\n    {  \\n      // Make a copy of the recognizer's grammar collection.  \\n      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  \\n  \\n      if (loadedGrammars.Count > 0)  \\n      {  \\n        Console.WriteLine(\\\"Loaded grammars:\\\");  \\n        foreach (Grammar g in recognizer.Grammars)  \\n        {  \\n          Console.WriteLine(\\\" - {0}\\\", g.Name);  \\n        }  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No grammars loaded.\\\");  \\n      }  \\n      Console.WriteLine();  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public void UnloadAllGrammars ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)\n  id: UnloadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: UnloadGrammar(Grammar)\n  nameWithType: SpeechRecognitionEngine.UnloadGrammar(Grammar)\n  fullName: SpeechRecognitionEngine.UnloadGrammar(Grammar)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Unloads a specified <xref href=\"System.Speech.Recognition.Grammar\"></xref> object from the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> instance.\n  remarks: If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> to pause the <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance before loading, unloading,  enabling, or disabling a <xref:System.Speech.Recognition.Grammar> object. To unload all <xref:System.Speech.Recognition.Grammar> objects, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> method.\n  example:\n  - \"The following example shows part of a console application that demonstrates the synchronous loading and unloading of speech recognition grammars.  \\n  \\n```  \\nLoading grammars...  \\nLoaded grammars:  \\n - Grammar1  \\n - Grammar2  \\n - Grammar3  \\n  \\nUnloading Grammar1...  \\nLoaded grammars:  \\n - Grammar2  \\n - Grammar3  \\n  \\nUnloading all grammars...  \\nNo grammars loaded.  \\n  \\nPress any key to exit...  \\n```  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Collections.Generic;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace UnloadGrammars  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        Console.WriteLine(\\\"Loading grammars...\\\");  \\n  \\n        // Create and load a number of grammars.  \\n        Grammar grammar1 = new Grammar(new GrammarBuilder(\\\"first grammar\\\"));  \\n        grammar1.Name = \\\"Grammar1\\\";  \\n        recognizer.LoadGrammar(grammar1);  \\n  \\n        Grammar grammar2 = new Grammar(new GrammarBuilder(\\\"second grammar\\\"));  \\n        grammar2.Name = \\\"Grammar2\\\";  \\n        recognizer.LoadGrammar(grammar2);  \\n  \\n        Grammar grammar3 = new Grammar(new GrammarBuilder(\\\"third grammar\\\"));  \\n        grammar3.Name = \\\"Grammar3\\\";  \\n        recognizer.LoadGrammar(grammar3);  \\n  \\n        // List the recognizer's loaded grammars.  \\n        ListGrammars(recognizer);  \\n  \\n        // Unload one grammar and list the loaded grammars.  \\n        Console.WriteLine(\\\"Unloading Grammar1...\\\");  \\n        recognizer.UnloadGrammar(grammar1);  \\n        ListGrammars(recognizer);  \\n  \\n        // Unload all grammars and list the loaded grammars.  \\n        Console.WriteLine(\\\"Unloading all grammars...\\\");  \\n        recognizer.UnloadAllGrammars();  \\n        ListGrammars(recognizer);  \\n      }  \\n  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    private static void ListGrammars(SpeechRecognitionEngine recognizer)  \\n    {  \\n      // Make a copy of the recognizer's grammar collection.  \\n      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  \\n  \\n      if (loadedGrammars.Count > 0)  \\n      {  \\n        Console.WriteLine(\\\"Loaded grammars:\\\");  \\n        foreach (Grammar g in recognizer.Grammars)  \\n        {  \\n          Console.WriteLine(\\\" - {0}\\\", g.Name);  \\n        }  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No grammars loaded.\\\");  \\n      }  \\n      Console.WriteLine();  \\n    }  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);\n    parameters:\n    - id: grammar\n      type: System.Speech.Recognition.Grammar\n      description: The grammar object to unload.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar*\n  exceptions:\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>Grammar</code> is `null`.\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The grammar is not loaded in this recognizer, or this recognizer is currently loading the grammar asynchronously.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)\n  id: UpdateRecognizerSetting(System.String,System.Int32)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: UpdateRecognizerSetting(String, Int32)\n  nameWithType: SpeechRecognitionEngine.UpdateRecognizerSetting(String, Int32)\n  fullName: SpeechRecognitionEngine.UpdateRecognizerSetting(String, Int32)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Updates the specified setting for the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine\"></xref> with the specified integer value.\n  remarks: With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings. See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings.\n  example:\n  - \"The following example is part of a console application that outputs the values for a number of the settings defined for the recognizer that supports the en-US locale. The example updates the confidence level settings, and then queries the recognizer to check the updated values. The example generates the following output.  \\n  \\n```  \\nSettings for recognizer MS-1033-80-DESK:  \\n  \\n  ResourceUsage                  is not supported by this recognizer.  \\n  ResponseSpeed                  = 150  \\n  ComplexResponseSpeed           = 500  \\n  AdaptationOn                   = 1  \\n  PersistedBackgroundAdaptation  = 1  \\n  \\nUpdated settings:  \\n  \\n  ResourceUsage                  is not supported by this recognizer.  \\n  ResponseSpeed                  = 200  \\n  ComplexResponseSpeed           = 300  \\n  AdaptationOn                   = 0  \\n  PersistedBackgroundAdaptation  = 0  \\n  \\nPress any key to exit...  \\n```  \\n  \\n```csharp  \\nusing System;  \\nusing System.Globalization;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace RecognizerSettings  \\n{  \\n  class Program  \\n  {  \\n    static readonly string[] settings = new string[] {  \\n      \\\"ResourceUsage\\\",  \\n      \\\"ResponseSpeed\\\",  \\n      \\\"ComplexResponseSpeed\\\",  \\n      \\\"AdaptationOn\\\",  \\n      \\\"PersistedBackgroundAdaptation\\\",  \\n    };  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognitionEngine recognizer =  \\n        new SpeechRecognitionEngine(new System.Globalization.CultureInfo(\\\"en-US\\\")))  \\n      {  \\n        Console.WriteLine(\\\"Settings for recognizer {0}:\\\",  \\n          recognizer.RecognizerInfo.Name);  \\n        Console.WriteLine();  \\n  \\n        // List the current settings.  \\n        ListSettings(recognizer);  \\n  \\n        // Change some of the settings.  \\n        recognizer.UpdateRecognizerSetting(\\\"ResponseSpeed\\\", 200);  \\n        recognizer.UpdateRecognizerSetting(\\\"ComplexResponseSpeed\\\", 300);  \\n        recognizer.UpdateRecognizerSetting(\\\"AdaptationOn\\\", 1);  \\n        recognizer.UpdateRecognizerSetting(\\\"PersistedBackgroundAdaptation\\\", 0);  \\n  \\n        Console.WriteLine(\\\"Updated settings:\\\");  \\n        Console.WriteLine();  \\n  \\n        // List the updated settings.  \\n        ListSettings(recognizer);  \\n      }  \\n  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    private static void ListSettings(SpeechRecognitionEngine recognizer)  \\n    {  \\n      foreach (string setting in settings)  \\n      {  \\n        try  \\n        {  \\n          object value = recognizer.QueryRecognizerSetting(setting);  \\n          Console.WriteLine(\\\"  {0,-30} = {1}\\\", setting, value);  \\n        }  \\n        catch  \\n        {  \\n          Console.WriteLine(\\\"  {0,-30} is not supported by this recognizer.\\\",  \\n            setting);  \\n        }  \\n      }  \\n      Console.WriteLine();  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void UpdateRecognizerSetting (string settingName, int updatedValue);\n    parameters:\n    - id: settingName\n      type: System.String\n      description: The name of the setting to update.\n    - id: updatedValue\n      type: System.Int32\n      description: The new value for the setting.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting*\n  exceptions:\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>settingName</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>settingName</code> is the empty string (\"\").\n  - type: System.Collections.Generic.KeyNotFoundException\n    commentId: T:System.Collections.Generic.KeyNotFoundException\n    description: The recognizer does not have a setting by that name.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)\n  id: UpdateRecognizerSetting(System.String,System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  langs:\n  - csharp\n  name: UpdateRecognizerSetting(String, String)\n  nameWithType: SpeechRecognitionEngine.UpdateRecognizerSetting(String, String)\n  fullName: SpeechRecognitionEngine.UpdateRecognizerSetting(String, String)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Updates the specified speech recognition engine setting with the specified string value.\n  remarks: With the exception of `PersistedBackgroundAdaptation`, property values set using the <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> method remain in effect only for the current instance of <xref:System.Speech.Recognition.SpeechRecognitionEngine>, after which they revert to their default settings. See <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> for descriptions of supported settings.\n  syntax:\n    content: public void UpdateRecognizerSetting (string settingName, string updatedValue);\n    parameters:\n    - id: settingName\n      type: System.String\n      description: The name of the setting to update.\n    - id: updatedValue\n      type: System.String\n      description: The new value for the setting.\n  overload: System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting*\n  exceptions:\n  - type: System.ArgumentNullException\n    commentId: T:System.ArgumentNullException\n    description: <code>settingName</code> is `null`.\n  - type: System.ArgumentException\n    commentId: T:System.ArgumentException\n    description: <code>settingName</code> is the empty string (\"\").\n  - type: System.Collections.Generic.KeyNotFoundException\n    commentId: T:System.Collections.Generic.KeyNotFoundException\n    description: The recognizer does not have a setting by that name.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\nreferences:\n- uid: System.Object\n  parent: System\n  isExternal: false\n  name: Object\n  nameWithType: Object\n  fullName: System.Object\n- uid: System.ArgumentException\n  parent: System\n  isExternal: false\n  name: ArgumentException\n  nameWithType: ArgumentException\n  fullName: System.ArgumentException\n- uid: System.ArgumentNullException\n  parent: System\n  isExternal: false\n  name: ArgumentNullException\n  nameWithType: ArgumentNullException\n  fullName: System.ArgumentNullException\n- uid: System.ArgumentOutOfRangeException\n  parent: System\n  isExternal: false\n  name: ArgumentOutOfRangeException\n  nameWithType: ArgumentOutOfRangeException\n  fullName: System.ArgumentOutOfRangeException\n- uid: System.InvalidOperationException\n  parent: System\n  isExternal: false\n  name: InvalidOperationException\n  nameWithType: InvalidOperationException\n  fullName: System.InvalidOperationException\n- uid: System.NotSupportedException\n  parent: System\n  isExternal: false\n  name: NotSupportedException\n  nameWithType: NotSupportedException\n  fullName: System.NotSupportedException\n- uid: System.OperationCanceledException\n  parent: System\n  isExternal: false\n  name: OperationCanceledException\n  nameWithType: OperationCanceledException\n  fullName: System.OperationCanceledException\n- uid: System.Collections.Generic.KeyNotFoundException\n  parent: System.Collections.Generic\n  isExternal: false\n  name: KeyNotFoundException\n  nameWithType: KeyNotFoundException\n  fullName: System.Collections.Generic.KeyNotFoundException\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechRecognitionEngine()\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine()\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechRecognitionEngine(CultureInfo)\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine(CultureInfo)\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine(CultureInfo)\n- uid: System.Globalization.CultureInfo\n  parent: System.Globalization\n  isExternal: false\n  name: CultureInfo\n  nameWithType: CultureInfo\n  fullName: System.Globalization.CultureInfo\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechRecognitionEngine(RecognizerInfo)\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine(RecognizerInfo)\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine(RecognizerInfo)\n- uid: System.Speech.Recognition.RecognizerInfo\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizerInfo\n  nameWithType: RecognizerInfo\n  fullName: System.Speech.Recognition.RecognizerInfo\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechRecognitionEngine(String)\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine(String)\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine(String)\n- uid: System.String\n  parent: System\n  isExternal: false\n  name: String\n  nameWithType: String\n  fullName: System.String\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioFormat\n  nameWithType: SpeechRecognitionEngine.AudioFormat\n  fullName: SpeechRecognitionEngine.AudioFormat\n- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo\n  parent: System.Speech.AudioFormat\n  isExternal: false\n  name: SpeechAudioFormatInfo\n  nameWithType: SpeechAudioFormatInfo\n  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioLevel\n  nameWithType: SpeechRecognitionEngine.AudioLevel\n  fullName: SpeechRecognitionEngine.AudioLevel\n- uid: System.Int32\n  parent: System\n  isExternal: false\n  name: Int32\n  nameWithType: Int32\n  fullName: System.Int32\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioLevelUpdated\n  nameWithType: SpeechRecognitionEngine.AudioLevelUpdated\n  fullName: SpeechRecognitionEngine.AudioLevelUpdated\n- uid: System.EventHandler`1\n  name: EventHandler<TEventArgs>\n  nameWithType: EventHandler<TEventArgs>\n  fullName: System.EventHandler<TEventArgs>\n- uid: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n  name: AudioLevelUpdatedEventArgs\n  nameWithType: AudioLevelUpdatedEventArgs\n  fullName: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.AudioLevelUpdatedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<AudioLevelUpdatedEventArgs>\n  nameWithType: EventHandler<AudioLevelUpdatedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.AudioLevelUpdatedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n    name: AudioLevelUpdatedEventArgs\n    nameWithType: AudioLevelUpdatedEventArgs\n    fullName: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioPosition\n  nameWithType: SpeechRecognitionEngine.AudioPosition\n  fullName: SpeechRecognitionEngine.AudioPosition\n- uid: System.TimeSpan\n  parent: System\n  isExternal: false\n  name: TimeSpan\n  nameWithType: TimeSpan\n  fullName: System.TimeSpan\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioSignalProblemOccurred\n  nameWithType: SpeechRecognitionEngine.AudioSignalProblemOccurred\n  fullName: SpeechRecognitionEngine.AudioSignalProblemOccurred\n- uid: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n  name: AudioSignalProblemOccurredEventArgs\n  nameWithType: AudioSignalProblemOccurredEventArgs\n  fullName: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.AudioSignalProblemOccurredEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<AudioSignalProblemOccurredEventArgs>\n  nameWithType: EventHandler<AudioSignalProblemOccurredEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n    name: AudioSignalProblemOccurredEventArgs\n    nameWithType: AudioSignalProblemOccurredEventArgs\n    fullName: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioState\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioState\n  nameWithType: SpeechRecognitionEngine.AudioState\n  fullName: SpeechRecognitionEngine.AudioState\n- uid: System.Speech.Recognition.AudioState\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: AudioState\n  nameWithType: AudioState\n  fullName: System.Speech.Recognition.AudioState\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioStateChanged\n  nameWithType: SpeechRecognitionEngine.AudioStateChanged\n  fullName: SpeechRecognitionEngine.AudioStateChanged\n- uid: System.Speech.Recognition.AudioStateChangedEventArgs\n  name: AudioStateChangedEventArgs\n  nameWithType: AudioStateChangedEventArgs\n  fullName: System.Speech.Recognition.AudioStateChangedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.AudioStateChangedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<AudioStateChangedEventArgs>\n  nameWithType: EventHandler<AudioStateChangedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.AudioStateChangedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.AudioStateChangedEventArgs\n    name: AudioStateChangedEventArgs\n    nameWithType: AudioStateChangedEventArgs\n    fullName: System.Speech.Recognition.AudioStateChangedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: BabbleTimeout\n  nameWithType: SpeechRecognitionEngine.BabbleTimeout\n  fullName: SpeechRecognitionEngine.BabbleTimeout\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Dispose(Boolean)\n  nameWithType: SpeechRecognitionEngine.Dispose(Boolean)\n  fullName: SpeechRecognitionEngine.Dispose(Boolean)\n- uid: System.Boolean\n  parent: System\n  isExternal: false\n  name: Boolean\n  nameWithType: Boolean\n  fullName: System.Boolean\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Dispose\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Dispose()\n  nameWithType: SpeechRecognitionEngine.Dispose()\n  fullName: SpeechRecognitionEngine.Dispose()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognize(String)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognize(String)\n  fullName: SpeechRecognitionEngine.EmulateRecognize(String)\n- uid: System.Speech.Recognition.RecognitionResult\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognitionResult\n  nameWithType: RecognitionResult\n  fullName: System.Speech.Recognition.RecognitionResult\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n- uid: System.Speech.Recognition.RecognizedWordUnit\n  name: RecognizedWordUnit\n  nameWithType: RecognizedWordUnit\n  fullName: System.Speech.Recognition.RecognizedWordUnit\n- uid: System.Speech.Recognition.RecognizedWordUnit[]\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizedWordUnit[]\n  nameWithType: RecognizedWordUnit[]\n  fullName: System.Speech.Recognition.RecognizedWordUnit[]\n  spec.csharp:\n  - uid: System.Speech.Recognition.RecognizedWordUnit\n    name: RecognizedWordUnit\n    nameWithType: RecognizedWordUnit\n    fullName: System.Speech.Recognition.RecognizedWordUnit\n  - name: '[]'\n    nameWithType: '[]'\n    fullName: '[]'\n- uid: System.Globalization.CompareOptions\n  parent: System.Globalization\n  isExternal: false\n  name: CompareOptions\n  nameWithType: CompareOptions\n  fullName: System.Globalization.CompareOptions\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognize(String, CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognize(String, CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognize(String, CompareOptions)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognizeAsync(String)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeAsync(String)\n  fullName: SpeechRecognitionEngine.EmulateRecognizeAsync(String)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognizeAsync(String, CompareOptions)\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeAsync(String, CompareOptions)\n  fullName: SpeechRecognitionEngine.EmulateRecognizeAsync(String, CompareOptions)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognizeCompleted\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeCompleted\n  fullName: SpeechRecognitionEngine.EmulateRecognizeCompleted\n- uid: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n  name: EmulateRecognizeCompletedEventArgs\n  nameWithType: EmulateRecognizeCompletedEventArgs\n  fullName: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.EmulateRecognizeCompletedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<EmulateRecognizeCompletedEventArgs>\n  nameWithType: EventHandler<EmulateRecognizeCompletedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n    name: EmulateRecognizeCompletedEventArgs\n    nameWithType: EmulateRecognizeCompletedEventArgs\n    fullName: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EndSilenceTimeout\n  nameWithType: SpeechRecognitionEngine.EndSilenceTimeout\n  fullName: SpeechRecognitionEngine.EndSilenceTimeout\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EndSilenceTimeoutAmbiguous\n  nameWithType: SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  fullName: SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Grammars\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Grammars\n  nameWithType: SpeechRecognitionEngine.Grammars\n  fullName: SpeechRecognitionEngine.Grammars\n- uid: System.Collections.ObjectModel.ReadOnlyCollection`1\n  name: ReadOnlyCollection<T>\n  nameWithType: ReadOnlyCollection<T>\n  fullName: System.Collections.ObjectModel.ReadOnlyCollection<T>\n- uid: System.Speech.Recognition.Grammar\n  name: Grammar\n  nameWithType: Grammar\n  fullName: System.Speech.Recognition.Grammar\n- uid: System.Collections.ObjectModel.ReadOnlyCollection{System.Speech.Recognition.Grammar}\n  parent: System.Collections.ObjectModel\n  isExternal: false\n  name: ReadOnlyCollection<Grammar>\n  nameWithType: ReadOnlyCollection<Grammar>\n  fullName: System.Collections.ObjectModel.ReadOnlyCollection<System.Speech.Recognition.Grammar>\n  spec.csharp:\n  - uid: System.Collections.ObjectModel.ReadOnlyCollection`1\n    name: ReadOnlyCollection\n    nameWithType: ReadOnlyCollection\n    fullName: System.Collections.ObjectModel.ReadOnlyCollection\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.Grammar\n    name: Grammar\n    nameWithType: Grammar\n    fullName: System.Speech.Recognition.Grammar\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: InitialSilenceTimeout\n  nameWithType: SpeechRecognitionEngine.InitialSilenceTimeout\n  fullName: SpeechRecognitionEngine.InitialSilenceTimeout\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: InstalledRecognizers()\n  nameWithType: SpeechRecognitionEngine.InstalledRecognizers()\n  fullName: SpeechRecognitionEngine.InstalledRecognizers()\n- uid: System.Collections.ObjectModel.ReadOnlyCollection{System.Speech.Recognition.RecognizerInfo}\n  parent: System.Collections.ObjectModel\n  isExternal: false\n  name: ReadOnlyCollection<RecognizerInfo>\n  nameWithType: ReadOnlyCollection<RecognizerInfo>\n  fullName: System.Collections.ObjectModel.ReadOnlyCollection<System.Speech.Recognition.RecognizerInfo>\n  spec.csharp:\n  - uid: System.Collections.ObjectModel.ReadOnlyCollection`1\n    name: ReadOnlyCollection\n    nameWithType: ReadOnlyCollection\n    fullName: System.Collections.ObjectModel.ReadOnlyCollection\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.RecognizerInfo\n    name: RecognizerInfo\n    nameWithType: RecognizerInfo\n    fullName: System.Speech.Recognition.RecognizerInfo\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: LoadGrammar(Grammar)\n  nameWithType: SpeechRecognitionEngine.LoadGrammar(Grammar)\n  fullName: SpeechRecognitionEngine.LoadGrammar(Grammar)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: LoadGrammarAsync(Grammar)\n  nameWithType: SpeechRecognitionEngine.LoadGrammarAsync(Grammar)\n  fullName: SpeechRecognitionEngine.LoadGrammarAsync(Grammar)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: LoadGrammarCompleted\n  nameWithType: SpeechRecognitionEngine.LoadGrammarCompleted\n  fullName: SpeechRecognitionEngine.LoadGrammarCompleted\n- uid: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n  name: LoadGrammarCompletedEventArgs\n  nameWithType: LoadGrammarCompletedEventArgs\n  fullName: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.LoadGrammarCompletedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<LoadGrammarCompletedEventArgs>\n  nameWithType: EventHandler<LoadGrammarCompletedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.LoadGrammarCompletedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n    name: LoadGrammarCompletedEventArgs\n    nameWithType: LoadGrammarCompletedEventArgs\n    fullName: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: MaxAlternates\n  nameWithType: SpeechRecognitionEngine.MaxAlternates\n  fullName: SpeechRecognitionEngine.MaxAlternates\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: QueryRecognizerSetting(String)\n  nameWithType: SpeechRecognitionEngine.QueryRecognizerSetting(String)\n  fullName: SpeechRecognitionEngine.QueryRecognizerSetting(String)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Recognize\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Recognize()\n  nameWithType: SpeechRecognitionEngine.Recognize()\n  fullName: SpeechRecognitionEngine.Recognize()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Recognize(TimeSpan)\n  nameWithType: SpeechRecognitionEngine.Recognize(TimeSpan)\n  fullName: SpeechRecognitionEngine.Recognize(TimeSpan)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeAsync()\n  nameWithType: SpeechRecognitionEngine.RecognizeAsync()\n  fullName: SpeechRecognitionEngine.RecognizeAsync()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeAsync(RecognizeMode)\n  nameWithType: SpeechRecognitionEngine.RecognizeAsync(RecognizeMode)\n  fullName: SpeechRecognitionEngine.RecognizeAsync(RecognizeMode)\n- uid: System.Speech.Recognition.RecognizeMode\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizeMode\n  nameWithType: RecognizeMode\n  fullName: System.Speech.Recognition.RecognizeMode\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeAsyncCancel()\n  nameWithType: SpeechRecognitionEngine.RecognizeAsyncCancel()\n  fullName: SpeechRecognitionEngine.RecognizeAsyncCancel()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeAsyncStop()\n  nameWithType: SpeechRecognitionEngine.RecognizeAsyncStop()\n  fullName: SpeechRecognitionEngine.RecognizeAsyncStop()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeCompleted\n  nameWithType: SpeechRecognitionEngine.RecognizeCompleted\n  fullName: SpeechRecognitionEngine.RecognizeCompleted\n- uid: System.Speech.Recognition.RecognizeCompletedEventArgs\n  name: RecognizeCompletedEventArgs\n  nameWithType: RecognizeCompletedEventArgs\n  fullName: System.Speech.Recognition.RecognizeCompletedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.RecognizeCompletedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<RecognizeCompletedEventArgs>\n  nameWithType: EventHandler<RecognizeCompletedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.RecognizeCompletedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.RecognizeCompletedEventArgs\n    name: RecognizeCompletedEventArgs\n    nameWithType: RecognizeCompletedEventArgs\n    fullName: System.Speech.Recognition.RecognizeCompletedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizerAudioPosition\n  nameWithType: SpeechRecognitionEngine.RecognizerAudioPosition\n  fullName: SpeechRecognitionEngine.RecognizerAudioPosition\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizerInfo\n  nameWithType: SpeechRecognitionEngine.RecognizerInfo\n  fullName: SpeechRecognitionEngine.RecognizerInfo\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizerUpdateReached\n  nameWithType: SpeechRecognitionEngine.RecognizerUpdateReached\n  fullName: SpeechRecognitionEngine.RecognizerUpdateReached\n- uid: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n  name: RecognizerUpdateReachedEventArgs\n  nameWithType: RecognizerUpdateReachedEventArgs\n  fullName: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.RecognizerUpdateReachedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<RecognizerUpdateReachedEventArgs>\n  nameWithType: EventHandler<RecognizerUpdateReachedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.RecognizerUpdateReachedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n    name: RecognizerUpdateReachedEventArgs\n    nameWithType: RecognizerUpdateReachedEventArgs\n    fullName: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RequestRecognizerUpdate(Object, TimeSpan)\n  nameWithType: SpeechRecognitionEngine.RequestRecognizerUpdate(Object, TimeSpan)\n  fullName: SpeechRecognitionEngine.RequestRecognizerUpdate(Object, TimeSpan)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RequestRecognizerUpdate(Object)\n  nameWithType: SpeechRecognitionEngine.RequestRecognizerUpdate(Object)\n  fullName: SpeechRecognitionEngine.RequestRecognizerUpdate(Object)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RequestRecognizerUpdate()\n  nameWithType: SpeechRecognitionEngine.RequestRecognizerUpdate()\n  fullName: SpeechRecognitionEngine.RequestRecognizerUpdate()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToAudioStream(Stream, SpeechAudioFormatInfo)\n  nameWithType: SpeechRecognitionEngine.SetInputToAudioStream(Stream, SpeechAudioFormatInfo)\n  fullName: SpeechRecognitionEngine.SetInputToAudioStream(Stream, SpeechAudioFormatInfo)\n- uid: System.IO.Stream\n  parent: System.IO\n  isExternal: false\n  name: Stream\n  nameWithType: Stream\n  fullName: System.IO.Stream\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToDefaultAudioDevice()\n  nameWithType: SpeechRecognitionEngine.SetInputToDefaultAudioDevice()\n  fullName: SpeechRecognitionEngine.SetInputToDefaultAudioDevice()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToNull()\n  nameWithType: SpeechRecognitionEngine.SetInputToNull()\n  fullName: SpeechRecognitionEngine.SetInputToNull()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToWaveFile(String)\n  nameWithType: SpeechRecognitionEngine.SetInputToWaveFile(String)\n  fullName: SpeechRecognitionEngine.SetInputToWaveFile(String)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToWaveStream(Stream)\n  nameWithType: SpeechRecognitionEngine.SetInputToWaveStream(Stream)\n  fullName: SpeechRecognitionEngine.SetInputToWaveStream(Stream)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechDetected\n  nameWithType: SpeechRecognitionEngine.SpeechDetected\n  fullName: SpeechRecognitionEngine.SpeechDetected\n- uid: System.Speech.Recognition.SpeechDetectedEventArgs\n  name: SpeechDetectedEventArgs\n  nameWithType: SpeechDetectedEventArgs\n  fullName: System.Speech.Recognition.SpeechDetectedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechDetectedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechDetectedEventArgs>\n  nameWithType: EventHandler<SpeechDetectedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechDetectedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechDetectedEventArgs\n    name: SpeechDetectedEventArgs\n    nameWithType: SpeechDetectedEventArgs\n    fullName: System.Speech.Recognition.SpeechDetectedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechHypothesized\n  nameWithType: SpeechRecognitionEngine.SpeechHypothesized\n  fullName: SpeechRecognitionEngine.SpeechHypothesized\n- uid: System.Speech.Recognition.SpeechHypothesizedEventArgs\n  name: SpeechHypothesizedEventArgs\n  nameWithType: SpeechHypothesizedEventArgs\n  fullName: System.Speech.Recognition.SpeechHypothesizedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechHypothesizedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechHypothesizedEventArgs>\n  nameWithType: EventHandler<SpeechHypothesizedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechHypothesizedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechHypothesizedEventArgs\n    name: SpeechHypothesizedEventArgs\n    nameWithType: SpeechHypothesizedEventArgs\n    fullName: System.Speech.Recognition.SpeechHypothesizedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechRecognitionRejected\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionRejected\n  fullName: SpeechRecognitionEngine.SpeechRecognitionRejected\n- uid: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n  name: SpeechRecognitionRejectedEventArgs\n  nameWithType: SpeechRecognitionRejectedEventArgs\n  fullName: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechRecognitionRejectedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechRecognitionRejectedEventArgs>\n  nameWithType: EventHandler<SpeechRecognitionRejectedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechRecognitionRejectedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n    name: SpeechRecognitionRejectedEventArgs\n    nameWithType: SpeechRecognitionRejectedEventArgs\n    fullName: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechRecognized\n  nameWithType: SpeechRecognitionEngine.SpeechRecognized\n  fullName: SpeechRecognitionEngine.SpeechRecognized\n- uid: System.Speech.Recognition.SpeechRecognizedEventArgs\n  name: SpeechRecognizedEventArgs\n  nameWithType: SpeechRecognizedEventArgs\n  fullName: System.Speech.Recognition.SpeechRecognizedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechRecognizedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechRecognizedEventArgs>\n  nameWithType: EventHandler<SpeechRecognizedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechRecognizedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechRecognizedEventArgs\n    name: SpeechRecognizedEventArgs\n    nameWithType: SpeechRecognizedEventArgs\n    fullName: System.Speech.Recognition.SpeechRecognizedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: UnloadAllGrammars()\n  nameWithType: SpeechRecognitionEngine.UnloadAllGrammars()\n  fullName: SpeechRecognitionEngine.UnloadAllGrammars()\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: UnloadGrammar(Grammar)\n  nameWithType: SpeechRecognitionEngine.UnloadGrammar(Grammar)\n  fullName: SpeechRecognitionEngine.UnloadGrammar(Grammar)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: UpdateRecognizerSetting(String, Int32)\n  nameWithType: SpeechRecognitionEngine.UpdateRecognizerSetting(String, Int32)\n  fullName: SpeechRecognitionEngine.UpdateRecognizerSetting(String, Int32)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: UpdateRecognizerSetting(String, String)\n  nameWithType: SpeechRecognitionEngine.UpdateRecognizerSetting(String, String)\n  fullName: SpeechRecognitionEngine.UpdateRecognizerSetting(String, String)\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.#ctor*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SpeechRecognitionEngine\n  nameWithType: SpeechRecognitionEngine.SpeechRecognitionEngine\n  fullName: SpeechRecognitionEngine.SpeechRecognitionEngine\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognize\n  nameWithType: SpeechRecognitionEngine.EmulateRecognize\n  fullName: SpeechRecognitionEngine.EmulateRecognize\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EmulateRecognizeAsync\n  nameWithType: SpeechRecognitionEngine.EmulateRecognizeAsync\n  fullName: SpeechRecognitionEngine.EmulateRecognizeAsync\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Recognize*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Recognize\n  nameWithType: SpeechRecognitionEngine.Recognize\n  fullName: SpeechRecognitionEngine.Recognize\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeAsync\n  nameWithType: SpeechRecognitionEngine.RecognizeAsync\n  fullName: SpeechRecognitionEngine.RecognizeAsync\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RequestRecognizerUpdate\n  nameWithType: SpeechRecognitionEngine.RequestRecognizerUpdate\n  fullName: SpeechRecognitionEngine.RequestRecognizerUpdate\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: UpdateRecognizerSetting\n  nameWithType: SpeechRecognitionEngine.UpdateRecognizerSetting\n  fullName: SpeechRecognitionEngine.UpdateRecognizerSetting\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioFormat\n  nameWithType: SpeechRecognitionEngine.AudioFormat\n  fullName: SpeechRecognitionEngine.AudioFormat\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioLevel\n  nameWithType: SpeechRecognitionEngine.AudioLevel\n  fullName: SpeechRecognitionEngine.AudioLevel\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioPosition\n  nameWithType: SpeechRecognitionEngine.AudioPosition\n  fullName: SpeechRecognitionEngine.AudioPosition\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.AudioState*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: AudioState\n  nameWithType: SpeechRecognitionEngine.AudioState\n  fullName: SpeechRecognitionEngine.AudioState\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: BabbleTimeout\n  nameWithType: SpeechRecognitionEngine.BabbleTimeout\n  fullName: SpeechRecognitionEngine.BabbleTimeout\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Dispose*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Dispose\n  nameWithType: SpeechRecognitionEngine.Dispose\n  fullName: SpeechRecognitionEngine.Dispose\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EndSilenceTimeout\n  nameWithType: SpeechRecognitionEngine.EndSilenceTimeout\n  fullName: SpeechRecognitionEngine.EndSilenceTimeout\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: EndSilenceTimeoutAmbiguous\n  nameWithType: SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  fullName: SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.Grammars*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: Grammars\n  nameWithType: SpeechRecognitionEngine.Grammars\n  fullName: SpeechRecognitionEngine.Grammars\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: InitialSilenceTimeout\n  nameWithType: SpeechRecognitionEngine.InitialSilenceTimeout\n  fullName: SpeechRecognitionEngine.InitialSilenceTimeout\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: InstalledRecognizers\n  nameWithType: SpeechRecognitionEngine.InstalledRecognizers\n  fullName: SpeechRecognitionEngine.InstalledRecognizers\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: LoadGrammar\n  nameWithType: SpeechRecognitionEngine.LoadGrammar\n  fullName: SpeechRecognitionEngine.LoadGrammar\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: LoadGrammarAsync\n  nameWithType: SpeechRecognitionEngine.LoadGrammarAsync\n  fullName: SpeechRecognitionEngine.LoadGrammarAsync\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: MaxAlternates\n  nameWithType: SpeechRecognitionEngine.MaxAlternates\n  fullName: SpeechRecognitionEngine.MaxAlternates\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: QueryRecognizerSetting\n  nameWithType: SpeechRecognitionEngine.QueryRecognizerSetting\n  fullName: SpeechRecognitionEngine.QueryRecognizerSetting\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeAsyncCancel\n  nameWithType: SpeechRecognitionEngine.RecognizeAsyncCancel\n  fullName: SpeechRecognitionEngine.RecognizeAsyncCancel\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizeAsyncStop\n  nameWithType: SpeechRecognitionEngine.RecognizeAsyncStop\n  fullName: SpeechRecognitionEngine.RecognizeAsyncStop\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizerAudioPosition\n  nameWithType: SpeechRecognitionEngine.RecognizerAudioPosition\n  fullName: SpeechRecognitionEngine.RecognizerAudioPosition\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: RecognizerInfo\n  nameWithType: SpeechRecognitionEngine.RecognizerInfo\n  fullName: SpeechRecognitionEngine.RecognizerInfo\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToAudioStream\n  nameWithType: SpeechRecognitionEngine.SetInputToAudioStream\n  fullName: SpeechRecognitionEngine.SetInputToAudioStream\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToDefaultAudioDevice\n  nameWithType: SpeechRecognitionEngine.SetInputToDefaultAudioDevice\n  fullName: SpeechRecognitionEngine.SetInputToDefaultAudioDevice\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToNull\n  nameWithType: SpeechRecognitionEngine.SetInputToNull\n  fullName: SpeechRecognitionEngine.SetInputToNull\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToWaveFile\n  nameWithType: SpeechRecognitionEngine.SetInputToWaveFile\n  fullName: SpeechRecognitionEngine.SetInputToWaveFile\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: SetInputToWaveStream\n  nameWithType: SpeechRecognitionEngine.SetInputToWaveStream\n  fullName: SpeechRecognitionEngine.SetInputToWaveStream\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: UnloadAllGrammars\n  nameWithType: SpeechRecognitionEngine.UnloadAllGrammars\n  fullName: SpeechRecognitionEngine.UnloadAllGrammars\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar*\n  parent: System.Speech.Recognition.SpeechRecognitionEngine\n  isExternal: false\n  name: UnloadGrammar\n  nameWithType: SpeechRecognitionEngine.UnloadGrammar\n  fullName: SpeechRecognitionEngine.UnloadGrammar\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognitionEngine.xml\n- uid: System.Object.Equals(System.Object)\n  parent: System.Object\n  isExternal: false\n  name: Equals(Object)\n  nameWithType: Object.Equals(Object)\n  fullName: Object.Equals(Object)\n- uid: System.Object.Equals(System.Object,System.Object)\n  parent: System.Object\n  isExternal: false\n  name: Equals(Object, Object)\n  nameWithType: Object.Equals(Object, Object)\n  fullName: Object.Equals(Object, Object)\n- uid: System.Object.GetHashCode\n  parent: System.Object\n  isExternal: false\n  name: GetHashCode()\n  nameWithType: Object.GetHashCode()\n  fullName: Object.GetHashCode()\n- uid: System.Object.GetType\n  parent: System.Object\n  isExternal: false\n  name: GetType()\n  nameWithType: Object.GetType()\n  fullName: Object.GetType()\n- uid: System.Object.MemberwiseClone\n  parent: System.Object\n  isExternal: false\n  name: MemberwiseClone()\n  nameWithType: Object.MemberwiseClone()\n  fullName: Object.MemberwiseClone()\n- uid: System.Object.ReferenceEquals(System.Object,System.Object)\n  parent: System.Object\n  isExternal: false\n  name: ReferenceEquals(Object, Object)\n  nameWithType: Object.ReferenceEquals(Object, Object)\n  fullName: Object.ReferenceEquals(Object, Object)\n- uid: System.Object.ToString\n  parent: System.Object\n  isExternal: false\n  name: ToString()\n  nameWithType: Object.ToString()\n  fullName: Object.ToString()\n- uid: System.IDisposable\n  parent: System\n  isExternal: false\n  name: IDisposable\n  nameWithType: IDisposable\n  fullName: System.IDisposable\n"}