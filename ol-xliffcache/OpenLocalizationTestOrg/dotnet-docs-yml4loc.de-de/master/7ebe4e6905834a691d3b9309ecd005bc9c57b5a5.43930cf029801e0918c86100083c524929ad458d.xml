{"nodes":[{"content":"Represents audio input that is associated with a <xref href=\"System.Speech.Recognition.RecognitionResult\"></xref>.","nodes":[{"pos":[0,114],"content":"Represents audio input that is associated with a <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.RecognitionResult\"&gt;&lt;/xref&gt;</ph>.","source":"Represents audio input that is associated with a <xref href=\"System.Speech.Recognition.RecognitionResult\"></xref>."}],"pos":[879,994],"yaml":true},{"content":"A speech recognizer generates information about the audio input as part of the recognition operation. To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>.  \n  \n A recognition result can be produced by the following events and methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes:  \n  \n-   Events:  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>  \n  \n-   Methods:  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>  \n  \n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>  \n  \n> [!IMPORTANT]\n>  A recognition result produced by emulated speech recognition does not contain recognized audio. For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception. For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes.","nodes":[{"pos":[0,358],"content":"A speech recognizer generates information about the audio input as part of the recognition operation. To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>.","nodes":[{"content":"A speech recognizer generates information about the audio input as part of the recognition operation. To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>.","pos":[0,358],"nodes":[{"content":"A speech recognizer generates information about the audio input as part of the recognition operation.","pos":[0,101]},{"content":"To access the recognized audio, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</ph> property or the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt;</ph> method of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph>.","pos":[102,358],"source":" To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>."}]}]},{"pos":[365,564],"content":"A recognition result can be produced by the following events and methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes:","nodes":[{"content":"A recognition result can be produced by the following events and methods of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> classes:","pos":[0,199],"source":"A recognition result can be produced by the following events and methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes:"}]},{"pos":[574,581],"content":"Events:","nodes":[{"content":"Events:","pos":[0,7]}]},{"pos":[595,793],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName&gt;</ph>","pos":[0,198],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>"}]},{"pos":[807,1019],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName&gt;</ph>","pos":[0,212],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>"}]},{"pos":[1033,1227],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName&gt;</ph>","pos":[0,194],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>"}]},{"pos":[1241,1453],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName&gt;</ph>","pos":[0,212],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>"}]},{"pos":[1467,1567],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>","nodes":[]},{"pos":[1577,1585],"content":"Methods:","nodes":[{"content":"Methods:","pos":[0,8]}]},{"pos":[1599,1799],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName&gt;</ph>","pos":[0,200],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>"}]},{"pos":[1813,2023],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>","nodes":[{"content":"<ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName&gt;</ph>","pos":[0,210],"source":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>"}]},{"pos":[2037,2131],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>","nodes":[]},{"pos":[2145,2244],"content":"<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>","nodes":[]},{"pos":[2252,2839],"content":"[!IMPORTANT]\n A recognition result produced by emulated speech recognition does not contain recognized audio. For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception. For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes.","leadings":["","> "],"nodes":[{"content":" A recognition result produced by emulated speech recognition does not contain recognized audio. For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception. For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes.","pos":[13,585],"nodes":[{"content":"A recognition result produced by emulated speech recognition does not contain recognized audio.","pos":[1,96]},{"content":"For such a recognition result, its <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</ph> property returns <ph id=\"ph2\">`null`</ph> and its <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt;</ph> method throws an exception.","pos":[97,326],"source":" For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception."},{"content":"For more information about emulated speech recognition, see the <ph id=\"ph1\">`EmulateRecognize`</ph> and <ph id=\"ph2\">`EmulateRecognizeAsync`</ph> methods of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> classes.","pos":[327,572],"source":" For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes."}]}]}],"pos":[1005,3874],"yaml":true,"extradata":"MT"},{"content":"Gets the location in the input audio stream for the start of the recognized audio.","nodes":[{"pos":[0,82],"content":"Gets the location in the input audio stream for the start of the recognized audio.","nodes":[{"content":"Gets the location in the input audio stream for the start of the recognized audio.","pos":[0,82]}]}],"pos":[6408,6491],"yaml":true},{"content":"This property references the position at the beginning of the recognized phrase in the input device's generated audio stream. By contrast, the `RecognizerAudioPosition` property of the <xref:System.Speech.Recognition.SpeechRecognitionEngine> and <xref:System.Speech.Recognition.SpeechRecognizer> classes reference the recognizer's position within its audio input. These positions can be different. For more information, see [Using Speech Recognition Events](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482).  \n  \n The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation.","nodes":[{"pos":[0,528],"content":"This property references the position at the beginning of the recognized phrase in the input device's generated audio stream. By contrast, the `RecognizerAudioPosition` property of the <xref:System.Speech.Recognition.SpeechRecognitionEngine> and <xref:System.Speech.Recognition.SpeechRecognizer> classes reference the recognizer's position within its audio input. These positions can be different. For more information, see [Using Speech Recognition Events](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482).","nodes":[{"content":"This property references the position at the beginning of the recognized phrase in the input device's generated audio stream.","pos":[0,125]},{"content":"By contrast, the <ph id=\"ph1\">`RecognizerAudioPosition`</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> and <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> classes reference the recognizer's position within its audio input.","pos":[126,363],"source":" By contrast, the `RecognizerAudioPosition` property of the <xref:System.Speech.Recognition.SpeechRecognitionEngine> and <xref:System.Speech.Recognition.SpeechRecognizer> classes reference the recognizer's position within its audio input."},{"content":"These positions can be different.","pos":[364,397]},{"content":"For more information, see <bpt id=\"p1\">[</bpt>Using Speech Recognition Events<ept id=\"p1\">](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)</ept>.","pos":[398,528],"source":" For more information, see [Using Speech Recognition Events](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)."}]},{"pos":[535,673],"content":"The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;</ph> property gets the system time at the start of the recognition operation.","pos":[0,138],"source":"The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation."}]}],"pos":[6502,7180],"yaml":true,"extradata":"MT"},{"content":"The location in the input audio stream for the start of the recognized audio.","nodes":[{"pos":[0,77],"content":"The location in the input audio stream for the start of the recognized audio.","nodes":[{"content":"The location in the input audio stream for the start of the recognized audio.","pos":[0,77]}]}],"pos":[8398,8476],"yaml":true},{"content":"Gets the duration of the input audio stream for the recognized audio.","nodes":[{"pos":[0,69],"content":"Gets the duration of the input audio stream for the recognized audio.","nodes":[{"content":"Gets the duration of the input audio stream for the recognized audio.","pos":[0,69]}]}],"pos":[9488,9558],"yaml":true},{"content":"The duration within the input audio stream for the recognized audio.","nodes":[{"pos":[0,68],"content":"The duration within the input audio stream for the recognized audio.","nodes":[{"content":"The duration within the input audio stream for the recognized audio.","pos":[0,68]}]}],"pos":[10785,10854],"yaml":true},{"content":"Gets the format of the audio processed by a recognition engine.","nodes":[{"pos":[0,63],"content":"Gets the format of the audio processed by a recognition engine.","nodes":[{"content":"Gets the format of the audio processed by a recognition engine.","pos":[0,63]}]}],"pos":[11849,11913],"yaml":true},{"content":"The format of the audio processed by the speech recognizer.","nodes":[{"pos":[0,59],"content":"The format of the audio processed by the speech recognizer.","nodes":[{"content":"The format of the audio processed by the speech recognizer.","pos":[0,59]}]}],"pos":[13209,13269],"yaml":true},{"content":"Selects and returns a section of the current recognized audio as binary data.","nodes":[{"pos":[0,77],"content":"Selects and returns a section of the current recognized audio as binary data.","nodes":[{"content":"Selects and returns a section of the current recognized audio as binary data.","pos":[0,77]}]}],"pos":[14431,14509],"yaml":true},{"content":"The starting point of the audio data to be returned.","nodes":[{"pos":[0,52],"content":"The starting point of the audio data to be returned.","nodes":[{"content":"The starting point of the audio data to be returned.","pos":[0,52]}]}],"pos":[17514,17567],"yaml":true},{"content":"The length of the segment to be returned.","nodes":[{"pos":[0,41],"content":"The length of the segment to be returned.","nodes":[{"content":"The length of the segment to be returned.","pos":[0,41]}]}],"pos":[17633,17675],"yaml":true},{"content":"Returns a subsection of the recognized audio, as defined by <code>audioPosition</code> and <code>duration</code>.","nodes":[{"pos":[0,113],"content":"Returns a subsection of the recognized audio, as defined by <bpt id=\"p1\">&lt;code&gt;</bpt><ph id=\"ph1\">audioPosition</ph><ept id=\"p1\">&lt;/code&gt;</ept> and <bpt id=\"p2\">&lt;code&gt;</bpt><ph id=\"ph2\">duration</ph><ept id=\"p2\">&lt;/code&gt;</ept>.","source":"Returns a subsection of the recognized audio, as defined by <code>audioPosition</code> and <code>duration</code>."}],"pos":[17760,17874],"yaml":true},{"content":"<code>audioPosition</code> and <code>duration</code> define a segment of audio outside the range of the current segment.","nodes":[{"pos":[0,120],"content":"<ph id=\"ph1\">&lt;code&gt;audioPosition&lt;/code&gt;</ph> and <ph id=\"ph2\">&lt;code&gt;duration&lt;/code&gt;</ph> define a segment of audio outside the range of the current segment.","source":"<code>audioPosition</code> and <code>duration</code> define a segment of audio outside the range of the current segment."}],"pos":[18066,18187],"yaml":true},{"content":"The current recognized audio contains no data.","nodes":[{"pos":[0,46],"content":"The current recognized audio contains no data.","nodes":[{"content":"The current recognized audio contains no data.","pos":[0,46]}]}],"pos":[18297,18344],"yaml":true},{"content":"Gets the system time at the start of the recognition operation.","nodes":[{"pos":[0,63],"content":"Gets the system time at the start of the recognition operation.","nodes":[{"content":"Gets the system time at the start of the recognition operation.","pos":[0,63]}]}],"pos":[19276,19340],"yaml":true},{"content":"The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.  \n  \n The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream.","nodes":[{"pos":[0,200],"content":"The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;</ph> property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.","pos":[0,200],"source":"The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations."}]},{"pos":[207,349],"content":"The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A&gt;</ph> property gets the location in the input device's generated audio stream.","pos":[0,142],"source":"The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream."}]}],"pos":[19351,19705],"yaml":true,"extradata":"MT"},{"content":"The system time at the start of the recognition operation.","nodes":[{"pos":[0,58],"content":"The system time at the start of the recognition operation.","nodes":[{"content":"The system time at the start of the recognition operation.","pos":[0,58]}]}],"pos":[20919,20978],"yaml":true},{"content":"Writes the entire audio to a stream as raw data.","nodes":[{"pos":[0,48],"content":"Writes the entire audio to a stream as raw data.","nodes":[{"content":"Writes the entire audio to a stream as raw data.","pos":[0,48]}]}],"pos":[22122,22171],"yaml":true},{"content":"Audio data is written to `outputStream` in binary form. No header information is included.  \n  \n The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the Wave format, but does not include the Wave header. To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method.","nodes":[{"pos":[0,90],"content":"Audio data is written to `outputStream` in binary form. No header information is included.","nodes":[{"content":"Audio data is written to <ph id=\"ph1\">`outputStream`</ph> in binary form.","pos":[0,55],"source":"Audio data is written to `outputStream` in binary form."},{"content":"No header information is included.","pos":[56,90]}]},{"pos":[97,352],"content":"The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the Wave format, but does not include the Wave header. To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method.","nodes":[{"content":"The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the Wave format, but does not include the Wave header. To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method.","pos":[0,255],"nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;</ph> method uses the Wave format, but does not include the Wave header.","pos":[0,141],"source":"The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the Wave format, but does not include the Wave header."},{"content":"To include the Wave header, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A&gt;</ph> method.","pos":[142,255],"source":" To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method."}]}]}],"pos":[22182,22539],"yaml":true,"extradata":"MT"},{"content":"The stream that will receive the audio data.","nodes":[{"pos":[0,44],"content":"The stream that will receive the audio data.","nodes":[{"content":"The stream that will receive the audio data.","pos":[0,44]}]}],"pos":[22713,22758],"yaml":true},{"content":"Writes audio to a stream in Wave format.","nodes":[{"pos":[0,40],"content":"Writes audio to a stream in Wave format.","nodes":[{"content":"Writes audio to a stream in Wave format.","pos":[0,40]}]}],"pos":[23905,23946],"yaml":true},{"content":"Audio data is written to `outputStream` in Wave format, which includes a resource interchange file format (RIFF) header.  \n  \n The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the same binary format, but does not include the Wave header.","nodes":[{"pos":[0,120],"content":"Audio data is written to <ph id=\"ph1\">`outputStream`</ph> in Wave format, which includes a resource interchange file format (RIFF) header.","source":"Audio data is written to `outputStream` in Wave format, which includes a resource interchange file format (RIFF) header."},{"pos":[127,275],"content":"The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the same binary format, but does not include the Wave header.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;</ph> method uses the same binary format, but does not include the Wave header.","pos":[0,148],"source":"The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the same binary format, but does not include the Wave header."}]}],"pos":[23957,24237],"yaml":true,"extradata":"MT"},{"content":"The stream that will receive the audio data.","nodes":[{"pos":[0,44],"content":"The stream that will receive the audio data.","nodes":[{"content":"The stream that will receive the audio data.","pos":[0,44]}]}],"pos":[27188,27233],"yaml":true}],"content":"### YamlMime:ManagedReference\nitems:\n- uid: System.Speech.Recognition.RecognizedAudio\n  commentId: T:System.Speech.Recognition.RecognizedAudio\n  id: RecognizedAudio\n  children:\n  - System.Speech.Recognition.RecognizedAudio.AudioPosition\n  - System.Speech.Recognition.RecognizedAudio.Duration\n  - System.Speech.Recognition.RecognizedAudio.Format\n  - System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)\n  - System.Speech.Recognition.RecognizedAudio.StartTime\n  - System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)\n  - System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)\n  langs:\n  - csharp\n  name: RecognizedAudio\n  nameWithType: RecognizedAudio\n  fullName: System.Speech.Recognition.RecognizedAudio\n  type: Class\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Represents audio input that is associated with a <xref href=\"System.Speech.Recognition.RecognitionResult\"></xref>.\n  remarks: \"A speech recognizer generates information about the audio input as part of the recognition operation. To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>.  \\n  \\n A recognition result can be produced by the following events and methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes:  \\n  \\n-   Events:  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>  \\n  \\n-   Methods:  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>  \\n  \\n> [!IMPORTANT]\\n>  A recognition result produced by emulated speech recognition does not contain recognized audio. For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception. For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes.\"\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>, or <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```csharp  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public class RecognizedAudio\n  inheritance:\n  - System.Object\n  implements: []\n  inheritedMembers:\n  - System.Object.Equals(System.Object)\n  - System.Object.Equals(System.Object,System.Object)\n  - System.Object.GetHashCode\n  - System.Object.GetType\n  - System.Object.MemberwiseClone\n  - System.Object.ReferenceEquals(System.Object,System.Object)\n  - System.Object.ToString\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition\n  commentId: P:System.Speech.Recognition.RecognizedAudio.AudioPosition\n  id: AudioPosition\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: AudioPosition\n  nameWithType: RecognizedAudio.AudioPosition\n  fullName: RecognizedAudio.AudioPosition\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the location in the input audio stream for the start of the recognized audio.\n  remarks: \"This property references the position at the beginning of the recognized phrase in the input device's generated audio stream. By contrast, the `RecognizerAudioPosition` property of the <xref:System.Speech.Recognition.SpeechRecognitionEngine> and <xref:System.Speech.Recognition.SpeechRecognizer> classes reference the recognizer's position within its audio input. These positions can be different. For more information, see [Using Speech Recognition Events](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482).  \\n  \\n The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation.\"\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```csharp  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public TimeSpan AudioPosition { get; }\n    return:\n      type: System.TimeSpan\n      description: The location in the input audio stream for the start of the recognized audio.\n  overload: System.Speech.Recognition.RecognizedAudio.AudioPosition*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.RecognizedAudio.Duration\n  commentId: P:System.Speech.Recognition.RecognizedAudio.Duration\n  id: Duration\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: Duration\n  nameWithType: RecognizedAudio.Duration\n  fullName: RecognizedAudio.Duration\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the duration of the input audio stream for the recognized audio.\n  remarks: ''\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```csharp  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public TimeSpan Duration { get; }\n    return:\n      type: System.TimeSpan\n      description: The duration within the input audio stream for the recognized audio.\n  overload: System.Speech.Recognition.RecognizedAudio.Duration*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.RecognizedAudio.Format\n  commentId: P:System.Speech.Recognition.RecognizedAudio.Format\n  id: Format\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: Format\n  nameWithType: RecognizedAudio.Format\n  fullName: RecognizedAudio.Format\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the format of the audio processed by a recognition engine.\n  remarks: ''\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```csharp  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }\n    return:\n      type: System.Speech.AudioFormat.SpeechAudioFormatInfo\n      description: The format of the audio processed by the speech recognizer.\n  overload: System.Speech.Recognition.RecognizedAudio.Format*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)\n  commentId: M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)\n  id: GetRange(System.TimeSpan,System.TimeSpan)\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: GetRange(TimeSpan, TimeSpan)\n  nameWithType: RecognizedAudio.GetRange(TimeSpan, TimeSpan)\n  fullName: RecognizedAudio.GetRange(TimeSpan, TimeSpan)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Selects and returns a section of the current recognized audio as binary data.\n  remarks: ''\n  example:\n  - \"The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \\n  \\n```  \\nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \\n{  \\n  GrammarBuilder builder = new GrammarBuilder();  \\n  builder.Append(\\\"My name is\\\");  \\n  builder.AppendWildcard();  \\n  \\n  Grammar nameGrammar = new Grammar(builder);  \\n  nameGrammar.Name = \\\"Name Grammar\\\";  \\n  nameGrammar.SpeechRecognized +=  \\n    new EventHandler<SpeechRecognizedEventArgs>(  \\n      NameSpeechRecognized);  \\n  \\n  recognizer.LoadGrammar(nameGrammar);  \\n}  \\n  \\n// Handle the SpeechRecognized event of the name grammar.  \\nprivate static void NameSpeechRecognized(  \\n  object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  Console.WriteLine(\\\"Grammar ({0}) recognized speech: {1}\\\",  \\n    e.Result.Grammar.Name, e.Result.Text);  \\n  \\n  try  \\n  {  \\n  \\n    // The name phrase starts after the first three words.  \\n    if (e.Result.Words.Count < 4)  \\n    {  \\n  \\n      // Add code to check for an alternate that contains the wildcard.  \\n      return;  \\n    }  \\n  \\n    RecognizedAudio audio = e.Result.Audio;  \\n    TimeSpan start = e.Result.Words[3].AudioPosition;  \\n    TimeSpan duration = audio.Duration - start;  \\n  \\n    // Add code to verify and persist the audio.  \\n    string path = @\\\"C:\\\\temp\\\\nameAudio.wav\\\";  \\n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \\n    {  \\n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \\n      nameAudio.WriteToWaveStream(outputStream);  \\n      outputStream.Close();  \\n    }  \\n  \\n    Thread testThread =  \\n      new Thread(new ParameterizedThreadStart(TestAudio));  \\n    testThread.Start(path);  \\n  }  \\n  catch (Exception ex)  \\n  {  \\n    Console.WriteLine(\\\"Exception thrown while processing audio:\\\");  \\n    Console.WriteLine(ex.ToString());  \\n  }  \\n}  \\n  \\n// Use the speech synthesizer to play back the .wav file  \\n// that was created in the SpeechRecognized event handler.  \\n  \\nprivate static void TestAudio(object item)  \\n{  \\n  string path = item as string;  \\n  if (path != null && File.Exists(path))  \\n  {  \\n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \\n    PromptBuilder builder = new PromptBuilder();  \\n    builder.AppendText(\\\"Hello\\\");  \\n    builder.AppendAudio(path);  \\n    synthesizer.Speak(builder);  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);\n    parameters:\n    - id: audioPosition\n      type: System.TimeSpan\n      description: The starting point of the audio data to be returned.\n    - id: duration\n      type: System.TimeSpan\n      description: The length of the segment to be returned.\n    return:\n      type: System.Speech.Recognition.RecognizedAudio\n      description: Returns a subsection of the recognized audio, as defined by <code>audioPosition</code> and <code>duration</code>.\n  overload: System.Speech.Recognition.RecognizedAudio.GetRange*\n  exceptions:\n  - type: System.ArgumentOutOfRangeException\n    commentId: T:System.ArgumentOutOfRangeException\n    description: <code>audioPosition</code> and <code>duration</code> define a segment of audio outside the range of the current segment.\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The current recognized audio contains no data.\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.RecognizedAudio.StartTime\n  commentId: P:System.Speech.Recognition.RecognizedAudio.StartTime\n  id: StartTime\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: StartTime\n  nameWithType: RecognizedAudio.StartTime\n  fullName: RecognizedAudio.StartTime\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the system time at the start of the recognition operation.\n  remarks: \"The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.  \\n  \\n The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream.\"\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```csharp  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public DateTime StartTime { get; }\n    return:\n      type: System.DateTime\n      description: The system time at the start of the recognition operation.\n  overload: System.Speech.Recognition.RecognizedAudio.StartTime*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)\n  commentId: M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)\n  id: WriteToAudioStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: WriteToAudioStream(Stream)\n  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)\n  fullName: RecognizedAudio.WriteToAudioStream(Stream)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Writes the entire audio to a stream as raw data.\n  remarks: \"Audio data is written to `outputStream` in binary form. No header information is included.  \\n  \\n The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the Wave format, but does not include the Wave header. To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method.\"\n  syntax:\n    content: public void WriteToAudioStream (System.IO.Stream outputStream);\n    parameters:\n    - id: outputStream\n      type: System.IO.Stream\n      description: The stream that will receive the audio data.\n  overload: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)\n  commentId: M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)\n  id: WriteToWaveStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: WriteToWaveStream(Stream)\n  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)\n  fullName: RecognizedAudio.WriteToWaveStream(Stream)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Writes audio to a stream in Wave format.\n  remarks: \"Audio data is written to `outputStream` in Wave format, which includes a resource interchange file format (RIFF) header.  \\n  \\n The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the same binary format, but does not include the Wave header.\"\n  example:\n  - \"The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \\n  \\n```  \\nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \\n{  \\n  GrammarBuilder builder = new GrammarBuilder();  \\n  builder.Append(\\\"My name is\\\");  \\n  builder.AppendWildcard();  \\n  \\n  Grammar nameGrammar = new Grammar(builder);  \\n  nameGrammar.Name = \\\"Name Grammar\\\";  \\n  nameGrammar.SpeechRecognized +=  \\n    new EventHandler<SpeechRecognizedEventArgs>(  \\n      NameSpeechRecognized);  \\n  \\n  recognizer.LoadGrammar(nameGrammar);  \\n}  \\n  \\n// Handle the SpeechRecognized event of the name grammar.  \\nprivate static void NameSpeechRecognized(  \\n  object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  Console.WriteLine(\\\"Grammar ({0}) recognized speech: {1}\\\",  \\n    e.Result.Grammar.Name, e.Result.Text);  \\n  \\n  try  \\n  {  \\n    // The name phrase starts after the first three words.  \\n    if (e.Result.Words.Count < 4)  \\n    {  \\n  \\n      // Add code to check for an alternate that contains the   \\nwildcard.  \\n      return;  \\n    }  \\n  \\n    RecognizedAudio audio = e.Result.Audio;  \\n    TimeSpan start = e.Result.Words[3].AudioPosition;  \\n    TimeSpan duration = audio.Duration - start;  \\n  \\n    // Add code to verify and persist the audio.  \\n    string path = @\\\"C:\\\\temp\\\\nameAudio.wav\\\";  \\n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \\n    {  \\n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \\n      nameAudio.WriteToWaveStream(outputStream);  \\n      outputStream.Close();  \\n    }  \\n  \\n    Thread testThread =  \\n      new Thread(new ParameterizedThreadStart(TestAudio));  \\n    testThread.Start(path);  \\n  }  \\n  catch (Exception ex)  \\n  {  \\n    Console.WriteLine(\\\"Exception thrown while processing audio:\\\");  \\n    Console.WriteLine(ex.ToString());  \\n  }  \\n}  \\n  \\n// Use the speech synthesizer to play back the .wav file  \\n// that was created in the SpeechRecognized event handler.  \\n  \\nprivate static void TestAudio(object item)  \\n{  \\n  string path = item as string;  \\n  if (path != null && File.Exists(path))  \\n  {  \\n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \\n    PromptBuilder builder = new PromptBuilder();  \\n    builder.AppendText(\\\"Hello\\\");  \\n    builder.AppendAudio(path);  \\n    synthesizer.Speak(builder);  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public void WriteToWaveStream (System.IO.Stream outputStream);\n    parameters:\n    - id: outputStream\n      type: System.IO.Stream\n      description: The stream that will receive the audio data.\n  overload: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\nreferences:\n- uid: System.Object\n  parent: System\n  isExternal: false\n  name: Object\n  nameWithType: Object\n  fullName: System.Object\n- uid: System.ArgumentOutOfRangeException\n  parent: System\n  isExternal: false\n  name: ArgumentOutOfRangeException\n  nameWithType: ArgumentOutOfRangeException\n  fullName: System.ArgumentOutOfRangeException\n- uid: System.InvalidOperationException\n  parent: System\n  isExternal: false\n  name: InvalidOperationException\n  nameWithType: InvalidOperationException\n  fullName: System.InvalidOperationException\n- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: AudioPosition\n  nameWithType: RecognizedAudio.AudioPosition\n  fullName: RecognizedAudio.AudioPosition\n- uid: System.TimeSpan\n  parent: System\n  isExternal: false\n  name: TimeSpan\n  nameWithType: TimeSpan\n  fullName: System.TimeSpan\n- uid: System.Speech.Recognition.RecognizedAudio.Duration\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Duration\n  nameWithType: RecognizedAudio.Duration\n  fullName: RecognizedAudio.Duration\n- uid: System.Speech.Recognition.RecognizedAudio.Format\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Format\n  nameWithType: RecognizedAudio.Format\n  fullName: RecognizedAudio.Format\n- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo\n  parent: System.Speech.AudioFormat\n  isExternal: false\n  name: SpeechAudioFormatInfo\n  nameWithType: SpeechAudioFormatInfo\n  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo\n- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: GetRange(TimeSpan, TimeSpan)\n  nameWithType: RecognizedAudio.GetRange(TimeSpan, TimeSpan)\n  fullName: RecognizedAudio.GetRange(TimeSpan, TimeSpan)\n- uid: System.Speech.Recognition.RecognizedAudio\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizedAudio\n  nameWithType: RecognizedAudio\n  fullName: System.Speech.Recognition.RecognizedAudio\n- uid: System.Speech.Recognition.RecognizedAudio.StartTime\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: StartTime\n  nameWithType: RecognizedAudio.StartTime\n  fullName: RecognizedAudio.StartTime\n- uid: System.DateTime\n  parent: System\n  isExternal: false\n  name: DateTime\n  nameWithType: DateTime\n  fullName: System.DateTime\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToAudioStream(Stream)\n  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)\n  fullName: RecognizedAudio.WriteToAudioStream(Stream)\n- uid: System.IO.Stream\n  parent: System.IO\n  isExternal: false\n  name: Stream\n  nameWithType: Stream\n  fullName: System.IO.Stream\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToWaveStream(Stream)\n  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)\n  fullName: RecognizedAudio.WriteToWaveStream(Stream)\n- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: AudioPosition\n  nameWithType: RecognizedAudio.AudioPosition\n  fullName: RecognizedAudio.AudioPosition\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n- uid: System.Speech.Recognition.RecognizedAudio.Duration*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Duration\n  nameWithType: RecognizedAudio.Duration\n  fullName: RecognizedAudio.Duration\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n- uid: System.Speech.Recognition.RecognizedAudio.Format*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Format\n  nameWithType: RecognizedAudio.Format\n  fullName: RecognizedAudio.Format\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n- uid: System.Speech.Recognition.RecognizedAudio.GetRange*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: GetRange\n  nameWithType: RecognizedAudio.GetRange\n  fullName: RecognizedAudio.GetRange\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n- uid: System.Speech.Recognition.RecognizedAudio.StartTime*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: StartTime\n  nameWithType: RecognizedAudio.StartTime\n  fullName: RecognizedAudio.StartTime\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToAudioStream\n  nameWithType: RecognizedAudio.WriteToAudioStream\n  fullName: RecognizedAudio.WriteToAudioStream\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToWaveStream\n  nameWithType: RecognizedAudio.WriteToWaveStream\n  fullName: RecognizedAudio.WriteToWaveStream\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/RecognizedAudio.xml\n- uid: System.Object.Equals(System.Object)\n  parent: System.Object\n  isExternal: false\n  name: Equals(Object)\n  nameWithType: Object.Equals(Object)\n  fullName: Object.Equals(Object)\n- uid: System.Object.Equals(System.Object,System.Object)\n  parent: System.Object\n  isExternal: false\n  name: Equals(Object, Object)\n  nameWithType: Object.Equals(Object, Object)\n  fullName: Object.Equals(Object, Object)\n- uid: System.Object.GetHashCode\n  parent: System.Object\n  isExternal: false\n  name: GetHashCode()\n  nameWithType: Object.GetHashCode()\n  fullName: Object.GetHashCode()\n- uid: System.Object.GetType\n  parent: System.Object\n  isExternal: false\n  name: GetType()\n  nameWithType: Object.GetType()\n  fullName: Object.GetType()\n- uid: System.Object.MemberwiseClone\n  parent: System.Object\n  isExternal: false\n  name: MemberwiseClone()\n  nameWithType: Object.MemberwiseClone()\n  fullName: Object.MemberwiseClone()\n- uid: System.Object.ReferenceEquals(System.Object,System.Object)\n  parent: System.Object\n  isExternal: false\n  name: ReferenceEquals(Object, Object)\n  nameWithType: Object.ReferenceEquals(Object, Object)\n  fullName: Object.ReferenceEquals(Object, Object)\n- uid: System.Object.ToString\n  parent: System.Object\n  isExternal: false\n  name: ToString()\n  nameWithType: Object.ToString()\n  fullName: Object.ToString()\n"}