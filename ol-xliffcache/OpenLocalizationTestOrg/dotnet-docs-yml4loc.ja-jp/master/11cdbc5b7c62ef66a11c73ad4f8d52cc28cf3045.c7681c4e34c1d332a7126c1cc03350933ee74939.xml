{"nodes":[{"content":"Provides access to the shared speech recognition service available on the Windows desktop.","nodes":[{"pos":[0,90],"content":"Provides access to the shared speech recognition service available on the Windows desktop.","nodes":[{"content":"Provides access to the shared speech recognition service available on the Windows desktop.","pos":[0,90]}]}],"pos":[3310,3401],"yaml":true},{"content":"Applications use the shared recognizer to access Windows Speech Recognition. Use the <xref:System.Speech.Recognition.SpeechRecognizer> object to add to the Windows speech user experience.  \n  \n This class provides control over various aspects of the speech recognition process:  \n  \n-   To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>.  \n  \n-   To get information about current speech recognition operations, subscribe to the <xref:System.Speech.Recognition.SpeechRecognizer>’s <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events.  \n  \n-   To view or modify the number of alternate results the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.  \n  \n-   To access or monitor the state of the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged>, and <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> events.  \n  \n-   To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. The shared recognizer uses more than one thread to perform tasks.  \n  \n-   To emulate input to the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods.  \n  \n The configuration of Windows Speech Recognition is managed by the use of the **Speech Properties** dialog in the **Control Panel**. This interface is used to select the default desktop speech recognition engine and language, the audio input device, and the sleep behavior of speech recognition. If the configuration of Windows Speech Recognition is changed while the application is running, (for instance, if speech recognition is disabled or the input language is changed), the change affects all <xref:System.Speech.Recognition.SpeechRecognizer> objects.  \n  \n To create an in-process speech recognizer that is independent of Windows Speech Recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine> class.  \n  \n> [!NOTE]\n>  Always call <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.","nodes":[{"pos":[0,187],"content":"Applications use the shared recognizer to access Windows Speech Recognition. Use the <xref:System.Speech.Recognition.SpeechRecognizer> object to add to the Windows speech user experience.","nodes":[{"content":"Applications use the shared recognizer to access Windows Speech Recognition. Use the <xref:System.Speech.Recognition.SpeechRecognizer> object to add to the Windows speech user experience.","pos":[0,187],"nodes":[{"content":"Applications use the shared recognizer to access Windows Speech Recognition.","pos":[0,76]},{"content":"Use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> object to add to the Windows speech user experience.","pos":[77,187],"source":" Use the <xref:System.Speech.Recognition.SpeechRecognizer> object to add to the Windows speech user experience."}]}]},{"pos":[194,277],"content":"This class provides control over various aspects of the speech recognition process:","nodes":[{"content":"This class provides control over various aspects of the speech recognition process:","pos":[0,83]}]},{"pos":[287,677],"content":"To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>.","nodes":[{"content":"To manage speech recognition grammars, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A&gt;</ph>, and <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A&gt;</ph>.","pos":[0,390],"source":"To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>."}]},{"pos":[687,1111],"content":"To get information about current speech recognition operations, subscribe to the <xref:System.Speech.Recognition.SpeechRecognizer>’s <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events.","nodes":[{"content":"To get information about current speech recognition operations, subscribe to the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph>’s <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized&gt;</ph> events.","pos":[0,424],"source":"To get information about current speech recognition operations, subscribe to the <xref:System.Speech.Recognition.SpeechRecognizer>’s <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events."}]},{"pos":[1121,1386],"content":"To view or modify the number of alternate results the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.","nodes":[{"content":"To view or modify the number of alternate results the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.","pos":[0,265],"nodes":[{"content":"To view or modify the number of alternate results the recognizer returns, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A&gt;</ph> property.","pos":[0,158],"source":"To view or modify the number of alternate results the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> property."},{"content":"The recognizer returns recognition results in a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object.","pos":[159,265],"source":" The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object."}]}]},{"pos":[1396,2254],"content":"To access or monitor the state of the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged>, and <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> events.","nodes":[{"content":"To access or monitor the state of the shared recognizer, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A&gt;</ph>, <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A&gt;</ph>, <ph id=\"ph5\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A&gt;</ph>, <ph id=\"ph6\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A&gt;</ph>, and <ph id=\"ph7\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.State%2A&gt;</ph> properties and the <ph id=\"ph8\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated&gt;</ph>, <ph id=\"ph9\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred&gt;</ph>, <ph id=\"ph10\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged&gt;</ph>, and <ph id=\"ph11\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.StateChanged&gt;</ph> events.","pos":[0,858],"source":"To access or monitor the state of the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged>, and <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> events."}]},{"pos":[2264,2464],"content":"To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. The shared recognizer uses more than one thread to perform tasks.","nodes":[{"content":"To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. The shared recognizer uses more than one thread to perform tasks.","pos":[0,200],"nodes":[{"content":"To synchronize changes to the recognizer, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[0,134],"source":"To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method."},{"content":"The shared recognizer uses more than one thread to perform tasks.","pos":[135,200]}]}]},{"pos":[2474,2682],"content":"To emulate input to the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods.","nodes":[{"content":"To emulate input to the shared recognizer, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A&gt;</ph> methods.","pos":[0,208],"source":"To emulate input to the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods."}]},{"pos":[2689,3245],"content":"The configuration of Windows Speech Recognition is managed by the use of the **Speech Properties** dialog in the **Control Panel**. This interface is used to select the default desktop speech recognition engine and language, the audio input device, and the sleep behavior of speech recognition. If the configuration of Windows Speech Recognition is changed while the application is running, (for instance, if speech recognition is disabled or the input language is changed), the change affects all <xref:System.Speech.Recognition.SpeechRecognizer> objects.","nodes":[{"content":"The configuration of Windows Speech Recognition is managed by the use of the <bpt id=\"p1\">**</bpt>Speech Properties<ept id=\"p1\">**</ept> dialog in the <bpt id=\"p2\">**</bpt>Control Panel<ept id=\"p2\">**</ept>.","pos":[0,131],"source":"The configuration of Windows Speech Recognition is managed by the use of the **Speech Properties** dialog in the **Control Panel**."},{"content":"This interface is used to select the default desktop speech recognition engine and language, the audio input device, and the sleep behavior of speech recognition.","pos":[132,294]},{"content":"If the configuration of Windows Speech Recognition is changed while the application is running, (for instance, if speech recognition is disabled or the input language is changed), the change affects all <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> objects.","pos":[295,556],"source":" If the configuration of Windows Speech Recognition is changed while the application is running, (for instance, if speech recognition is disabled or the input language is changed), the change affects all <xref:System.Speech.Recognition.SpeechRecognizer> objects."}]},{"pos":[3252,3416],"content":"To create an in-process speech recognizer that is independent of Windows Speech Recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine> class.","nodes":[{"content":"To create an in-process speech recognizer that is independent of Windows Speech Recognition, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> class.","pos":[0,164],"source":"To create an in-process speech recognizer that is independent of Windows Speech Recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine> class."}]},{"pos":[3424,3704],"content":"[!NOTE]\n Always call <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.","leadings":["","> "],"nodes":[{"content":" Always call <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.","pos":[8,278],"nodes":[{"content":"Always call <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A&gt;</ph> before you release your last reference to the speech recognizer.","pos":[1,138],"source":" Always call <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> before you release your last reference to the speech recognizer."},{"content":"Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's <ph id=\"ph1\">`Finalize`</ph> method.","pos":[139,270],"source":" Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method."}]}]}],"pos":[3412,7140],"yaml":true,"extradata":"MT"},{"content":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> class.","nodes":[{"pos":[0,104],"content":"Initializes a new instance of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer\"&gt;&lt;/xref&gt;</ph> class.","source":"Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> class."}],"pos":[11948,12053],"yaml":true},{"content":"Each <xref:System.Speech.Recognition.SpeechRecognizer> object maintains a separate set of speech recognition grammars.","nodes":[{"pos":[0,118],"content":"Each <xref:System.Speech.Recognition.SpeechRecognizer> object maintains a separate set of speech recognition grammars.","nodes":[{"content":"Each <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> object maintains a separate set of speech recognition grammars.","pos":[0,118],"source":"Each <xref:System.Speech.Recognition.SpeechRecognizer> object maintains a separate set of speech recognition grammars."}]}],"pos":[12064,12183],"yaml":true,"extradata":"MT"},{"content":"Gets the format of the audio being received by the speech recognizer.","nodes":[{"pos":[0,69],"content":"Gets the format of the audio being received by the speech recognizer.","nodes":[{"content":"Gets the format of the audio being received by the speech recognizer.","pos":[0,69]}]}],"pos":[16698,16768],"yaml":true},{"content":"The audio input format for the speech recognizer, or `null` if the input to the recognizer is not configured.","nodes":[{"pos":[0,109],"content":"The audio input format for the speech recognizer, or <ph id=\"ph1\">`null`</ph> if the input to the recognizer is not configured.","source":"The audio input format for the speech recognizer, or `null` if the input to the recognizer is not configured."}],"pos":[16958,17068],"yaml":true},{"content":"Gets the level of the audio being received by the speech recognizer.","nodes":[{"pos":[0,68],"content":"Gets the level of the audio being received by the speech recognizer.","nodes":[{"content":"Gets the level of the audio being received by the speech recognizer.","pos":[0,68]}]}],"pos":[18097,18166],"yaml":true},{"content":"The audio level of the input to the speech recognizer, from 0 through 100.","nodes":[{"pos":[0,74],"content":"The audio level of the input to the speech recognizer, from 0 through 100.","nodes":[{"content":"The audio level of the input to the speech recognizer, from 0 through 100.","pos":[0,74]}]}],"pos":[18276,18351],"yaml":true},{"content":"Occurs when the shared recognizer reports the level of its audio input.","nodes":[{"pos":[0,71],"content":"Occurs when the shared recognizer reports the level of its audio input.","nodes":[{"content":"Occurs when the shared recognizer reports the level of its audio input.","pos":[0,71]}]}],"pos":[19418,19490],"yaml":true},{"content":"The recognizer raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.  \n  \n To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> property.  \n  \n When you create a delegate for an `AudioLevelUpdated` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,165],"content":"The recognizer raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.","nodes":[{"content":"The recognizer raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.","pos":[0,165],"nodes":[{"content":"The recognizer raises this event multiple times per second.","pos":[0,59]},{"content":"The frequency with which the event is raised depends on the computer on which the application is running.","pos":[60,165]}]}]},{"pos":[172,548],"content":"To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> property.","nodes":[{"content":"To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> property.","pos":[0,376],"nodes":[{"content":"To get the audio level at the time of the event, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ph>.","pos":[0,218],"source":"To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>."},{"content":"To get the current audio level of the input to the recognizer, use the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A&gt;</ph> property.","pos":[219,376],"source":" To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> property."}]}]},{"pos":[555,975],"content":"When you create a delegate for an `AudioLevelUpdated` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for an <ph id=\"ph1\">`AudioLevelUpdated`</ph> event, you identify the method that will handle the event.","pos":[0,112],"source":"When you create a delegate for an `AudioLevelUpdated` event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[113,206]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[207,293]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[294,420],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[19501,20483],"yaml":true,"extradata":"MT"},{"content":"Gets the current location in the audio stream being generated by the device that is providing input to the speech recognizer.","nodes":[{"pos":[0,125],"content":"Gets the current location in the audio stream being generated by the device that is providing input to the speech recognizer.","nodes":[{"content":"Gets the current location in the audio stream being generated by the device that is providing input to the speech recognizer.","pos":[0,125]}]}],"pos":[22601,22727],"yaml":true},{"content":"The shared recognizer receives input while the desktop speech recognition is running.  \n  \n The `AudioPosition` property references the input device's position in its generated audio stream. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property references the recognizer's position in processing audio input. These positions can be different.  For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property.","nodes":[{"pos":[0,85],"content":"The shared recognizer receives input while the desktop speech recognition is running.","nodes":[{"content":"The shared recognizer receives input while the desktop speech recognition is running.","pos":[0,85]}]},{"pos":[92,713],"content":"The `AudioPosition` property references the input device's position in its generated audio stream. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property references the recognizer's position in processing audio input. These positions can be different.  For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property.","nodes":[{"content":"The <ph id=\"ph1\">`AudioPosition`</ph> property references the input device's position in its generated audio stream.","pos":[0,98],"source":"The `AudioPosition` property references the input device's position in its generated audio stream."},{"content":"By contrast, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A&gt;</ph> property references the recognizer's position in processing audio input.","pos":[99,265],"source":" By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property references the recognizer's position in processing audio input."},{"content":"These positions can be different.","pos":[266,299]},{"content":"For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A&gt;</ph> property is less than the value of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A&gt;</ph> property.","pos":[301,621],"source":"  For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property."}]}],"pos":[22738,23456],"yaml":true,"extradata":"MT"},{"content":"The current location in the speech recognizer's audio input stream through which it has received input.","nodes":[{"pos":[0,103],"content":"The current location in the speech recognizer's audio input stream through which it has received input.","nodes":[{"content":"The current location in the speech recognizer's audio input stream through which it has received input.","pos":[0,103]}]}],"pos":[26707,26811],"yaml":true},{"content":"Occurs when the recognizer encounters a problem in the audio signal.","nodes":[{"pos":[0,68],"content":"Occurs when the recognizer encounters a problem in the audio signal.","nodes":[{"content":"Occurs when the recognizer encounters a problem in the audio signal.","pos":[0,68]}]}],"pos":[27935,28004],"yaml":true},{"content":"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  \n  \n When you create a delegate for an `AudioSignalProblemOccurred` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,226],"content":"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.","nodes":[{"content":"To get which problem occurred, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ph>.","pos":[0,226],"source":"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>."}]},{"pos":[233,662],"content":"When you create a delegate for an `AudioSignalProblemOccurred` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for an <ph id=\"ph1\">`AudioSignalProblemOccurred`</ph> event, you identify the method that will handle the event.","pos":[0,121],"source":"When you create a delegate for an `AudioSignalProblemOccurred` event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[122,215]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[216,302]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[303,429],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[28015,28682],"yaml":true,"extradata":"MT"},{"content":"Gets the state of the audio being received by the speech recognizer.","nodes":[{"pos":[0,68],"content":"Gets the state of the audio being received by the speech recognizer.","nodes":[{"content":"Gets the state of the audio being received by the speech recognizer.","pos":[0,68]}]}],"pos":[31226,31295],"yaml":true},{"content":"The state of the audio input to the speech recognizer.","nodes":[{"pos":[0,54],"content":"The state of the audio input to the speech recognizer.","nodes":[{"content":"The state of the audio input to the speech recognizer.","pos":[0,54]}]}],"pos":[31462,31517],"yaml":true},{"content":"Occurs when the state changes in the audio being received by the recognizer.","nodes":[{"pos":[0,76],"content":"Occurs when the state changes in the audio being received by the recognizer.","nodes":[{"content":"Occurs when the state changes in the audio being received by the recognizer.","pos":[0,76]}]}],"pos":[32584,32661],"yaml":true},{"content":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.  \n  \n When you create a delegate for an `AudioStateChanged` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,481],"content":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.","nodes":[{"content":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.","pos":[0,481],"nodes":[{"content":"To get the audio state at the time of the event, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ph>.","pos":[0,218],"source":"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>."},{"content":"To get the current audio state of the input to the recognizer, use the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A&gt;</ph> property.","pos":[219,376],"source":" To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> property."},{"content":"For more information about audio state, see the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.AudioState&gt;</ph> enumeration.","pos":[377,481],"source":" For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration."}]}]},{"pos":[488,908],"content":"When you create a delegate for an `AudioStateChanged` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for an <ph id=\"ph1\">`AudioStateChanged`</ph> event, you identify the method that will handle the event.","pos":[0,112],"source":"When you create a delegate for an `AudioStateChanged` event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[113,206]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[207,293]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[294,420],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[32672,33585],"yaml":true,"extradata":"MT"},{"content":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object and releases resources used during the session.","nodes":[{"pos":[0,131],"content":"Disposes the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer\"&gt;&lt;/xref&gt;</ph> object and releases resources used during the session.","source":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object and releases resources used during the session."}],"pos":[37450,37582],"yaml":true},{"content":"`true` to release both managed and unmanaged resources; `false` to release only unmanaged resources.","nodes":[{"pos":[0,100],"content":"<ph id=\"ph1\">`true`</ph> to release both managed and unmanaged resources; <ph id=\"ph2\">`false`</ph> to release only unmanaged resources.","source":"`true` to release both managed and unmanaged resources; `false` to release only unmanaged resources."}],"pos":[37736,37839],"yaml":true},{"content":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object.","nodes":[{"pos":[0,84],"content":"Disposes the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer\"&gt;&lt;/xref&gt;</ph> object.","source":"Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object."}],"pos":[38850,38935],"yaml":true},{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition.","nodes":[{"pos":[0,123],"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition.","nodes":[{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition.","pos":[0,123]}]}],"pos":[40129,40253],"yaml":true},{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"pos":[0,474],"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[0,474],"nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase.","pos":[0,131]},{"content":"For more information about this type of comparison, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration values <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> and <ph id=\"ph3\">&lt;xref:System.Globalization.CompareOptions.IgnoreWidth&gt;</ph>.","pos":[132,374],"source":" For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>."},{"content":"The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[375,474]}]}]}],"pos":[40264,40739],"yaml":true,"extradata":"MT"},{"content":"The input for the recognition operation.","nodes":[{"pos":[0,40],"content":"The input for the recognition operation.","nodes":[{"content":"The input for the recognition operation.","pos":[0,40]}]}],"pos":[42955,42996],"yaml":true},{"content":"The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state.","nodes":[{"pos":[0,159],"content":"The recognition result for the recognition operation, or <ph id=\"ph1\">`null`</ph>, if the operation is not successful or Windows Speech Recognition is in the <bpt id=\"p1\">**</bpt>Sleeping<ept id=\"p1\">**</ept> state.","source":"The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state."}],"pos":[43083,43243],"yaml":true},{"content":"Emulates input of specific words to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","nodes":[{"pos":[0,251],"content":"Emulates input of specific words to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","nodes":[{"content":"Emulates input of specific words to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","pos":[0,251]}]}],"pos":[44677,44929],"yaml":true},{"content":"This method creates a <xref:System.Speech.Recognition.RecognitionResult> object using the information provided in the `wordUnits` parameter.  \n  \n The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,140],"content":"This method creates a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object using the information provided in the <ph id=\"ph2\">`wordUnits`</ph> parameter.","source":"This method creates a <xref:System.Speech.Recognition.RecognitionResult> object using the information provided in the `wordUnits` parameter."},{"pos":[147,749],"content":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses the <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,91],"source":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[92,297],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizers always ignore the character width and never ignore the Kana type.","pos":[298,379]},{"content":"The recognizers also ignore new lines and extra white space and treats punctuation as literal input.","pos":[380,480]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[481,602],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[44940,45694],"yaml":true,"extradata":"MT"},{"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"pos":[0,77],"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"content":"An array of word units that contains the input for the recognition operation.","pos":[0,77]}]}],"pos":[46011,46089],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[46181,46313],"yaml":true},{"content":"The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state.","nodes":[{"pos":[0,159],"content":"The recognition result for the recognition operation, or <ph id=\"ph1\">`null`</ph>, if the operation is not successful or Windows Speech Recognition is in the <bpt id=\"p1\">**</bpt>Sleeping<ept id=\"p1\">**</ept> state.","source":"The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state."}],"pos":[46400,46560],"yaml":true},{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"pos":[0,246],"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","pos":[0,246]}]}],"pos":[47853,48100],"yaml":true},{"content":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,602],"content":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses the <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,91],"source":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[92,297],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizers always ignore the character width and never ignore the Kana type.","pos":[298,379]},{"content":"The recognizers also ignore new lines and extra white space and treats punctuation as literal input.","pos":[380,480]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[481,602],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[48111,48714],"yaml":true,"extradata":"MT"},{"content":"The input phrase for the recognition operation.","nodes":[{"pos":[0,47],"content":"The input phrase for the recognition operation.","nodes":[{"content":"The input phrase for the recognition operation.","pos":[0,47]}]}],"pos":[48958,49006],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[49098,49230],"yaml":true},{"content":"The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state.","nodes":[{"pos":[0,159],"content":"The recognition result for the recognition operation, or <ph id=\"ph1\">`null`</ph>, if the operation is not successful or Windows Speech Recognition is in the <bpt id=\"p1\">**</bpt>Sleeping<ept id=\"p1\">**</ept> state.","source":"The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state."}],"pos":[49317,49477],"yaml":true},{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition.","nodes":[{"pos":[0,124],"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition.","nodes":[{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition.","pos":[0,124]}]}],"pos":[50644,50769],"yaml":true},{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"pos":[0,474],"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[0,474],"nodes":[{"content":"The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase.","pos":[0,131]},{"content":"For more information about this type of comparison, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration values <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> and <ph id=\"ph3\">&lt;xref:System.Globalization.CompareOptions.IgnoreWidth&gt;</ph>.","pos":[132,374],"source":" For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>."},{"content":"The recognizers also ignore new lines and extra white space and treat punctuation as literal input.","pos":[375,474]}]}]}],"pos":[50780,51255],"yaml":true,"extradata":"MT"},{"content":"The input for the recognition operation.","nodes":[{"pos":[0,40],"content":"The input for the recognition operation.","nodes":[{"content":"The input for the recognition operation.","pos":[0,40]}]}],"pos":[54597,54638],"yaml":true},{"content":"Emulates input of specific words to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","nodes":[{"pos":[0,252],"content":"Emulates input of specific words to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","nodes":[{"content":"Emulates input of specific words to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.","pos":[0,252]}]}],"pos":[56107,56360],"yaml":true},{"content":"This method creates a <xref:System.Speech.Recognition.RecognitionResult> object using the information provided in the `wordUnits` parameter.  \n  \n The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,140],"content":"This method creates a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object using the information provided in the <ph id=\"ph2\">`wordUnits`</ph> parameter.","source":"This method creates a <xref:System.Speech.Recognition.RecognitionResult> object using the information provided in the `wordUnits` parameter."},{"pos":[147,749],"content":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses the <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,91],"source":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[92,297],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizers always ignore the character width and never ignore the Kana type.","pos":[298,379]},{"content":"The recognizers also ignore new lines and extra white space and treats punctuation as literal input.","pos":[380,480]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[481,602],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[56371,57125],"yaml":true,"extradata":"MT"},{"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"pos":[0,77],"content":"An array of word units that contains the input for the recognition operation.","nodes":[{"content":"An array of word units that contains the input for the recognition operation.","pos":[0,77]}]}],"pos":[57408,57486],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[57578,57710],"yaml":true},{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"pos":[0,247],"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","nodes":[{"content":"Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.","pos":[0,247]}]}],"pos":[59038,59286],"yaml":true},{"content":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"pos":[0,602],"content":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.","nodes":[{"content":"The recognizer uses the <ph id=\"ph1\">`compareOptions`</ph> when it applies grammar rules to the input phrase.","pos":[0,91],"source":"The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase."},{"content":"The recognizers that ship with Vista and Windows 7 ignore case if the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions.OrdinalIgnoreCase&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Globalization.CompareOptions.IgnoreCase&gt;</ph> value is present.","pos":[92,297],"source":" The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present."},{"content":"The recognizers always ignore the character width and never ignore the Kana type.","pos":[298,379]},{"content":"The recognizers also ignore new lines and extra white space and treats punctuation as literal input.","pos":[380,480]},{"content":"For more information about character width and Kana type, see the <ph id=\"ph1\">&lt;xref:System.Globalization.CompareOptions&gt;</ph> enumeration.","pos":[481,602],"source":" For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration."}]}],"pos":[59297,59900],"yaml":true,"extradata":"MT"},{"content":"The input phrase for the recognition operation.","nodes":[{"pos":[0,47],"content":"The input phrase for the recognition operation.","nodes":[{"content":"The input phrase for the recognition operation.","pos":[0,47]}]}],"pos":[60110,60158],"yaml":true},{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"pos":[0,131],"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","nodes":[{"content":"A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.","pos":[0,131]}]}],"pos":[60250,60382],"yaml":true},{"content":"Occurs when the shared recognizer finalizes an asynchronous recognition operation for emulated input.","nodes":[{"pos":[0,101],"content":"Occurs when the shared recognizer finalizes an asynchronous recognition operation for emulated input.","nodes":[{"content":"Occurs when the shared recognizer finalizes an asynchronous recognition operation for emulated input.","pos":[0,101]}]}],"pos":[61508,61610],"yaml":true},{"content":"Each <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation. The recognizer raises the `EmulateRecognizeCompleted` event when it finalizes the asynchronous operation.  \n  \n The asynchronous recognition operation can raise the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.  \n  \n When you create a delegate for an `EmulateRecognizeCompleted` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,238],"content":"Each <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation. The recognizer raises the `EmulateRecognizeCompleted` event when it finalizes the asynchronous operation.","nodes":[{"content":"Each <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A&gt;</ph> method begins an asynchronous recognition operation.","pos":[0,132],"source":"Each <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation."},{"content":"The recognizer raises the <ph id=\"ph1\">`EmulateRecognizeCompleted`</ph> event when it finalizes the asynchronous operation.","pos":[133,238],"source":" The recognizer raises the `EmulateRecognizeCompleted` event when it finalizes the asynchronous operation."}]},{"pos":[245,748],"content":"The asynchronous recognition operation can raise the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.","nodes":[{"content":"The asynchronous recognition operation can raise the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.","pos":[0,503],"nodes":[{"content":"The asynchronous recognition operation can raise the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected&gt;</ph>, <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized&gt;</ph>, <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected&gt;</ph>, and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized&gt;</ph> events.","pos":[0,344],"source":"The asynchronous recognition operation can raise the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted&gt;</ph> event is the last such event that the recognizer raises for a given operation.","pos":[345,503],"source":" The <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation."}]}]},{"pos":[755,1183],"content":"When you create a delegate for an `EmulateRecognizeCompleted` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for an <ph id=\"ph1\">`EmulateRecognizeCompleted`</ph> event, you identify the method that will handle the event.","pos":[0,120],"source":"When you create a delegate for an `EmulateRecognizeCompleted` event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[121,214]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[215,301]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[302,428],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[61621,62811],"yaml":true,"extradata":"MT"},{"content":"Gets or sets a value that indicates whether this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object is ready to process speech.","nodes":[{"pos":[0,147],"content":"Gets or sets a value that indicates whether this <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer\"&gt;&lt;/xref&gt;</ph> object is ready to process speech.","source":"Gets or sets a value that indicates whether this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object is ready to process speech."}],"pos":[67347,67495],"yaml":true},{"content":"Changes to this property do not affect other instances of the <xref:System.Speech.Recognition.SpeechRecognizer> class.  \n  \n By default, the value of the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property is `true` for a newly instantiated instance of <xref:System.Speech.Recognition.SpeechRecognizer>. While the recognizer is disabled, none of the recognizer's speech recognition grammars are available for recognition operations. Setting the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property has no effect on the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property.","nodes":[{"pos":[0,118],"content":"Changes to this property do not affect other instances of the <xref:System.Speech.Recognition.SpeechRecognizer> class.","nodes":[{"content":"Changes to this property do not affect other instances of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> class.","pos":[0,118],"source":"Changes to this property do not affect other instances of the <xref:System.Speech.Recognition.SpeechRecognizer> class."}]},{"pos":[125,648],"content":"By default, the value of the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property is `true` for a newly instantiated instance of <xref:System.Speech.Recognition.SpeechRecognizer>. While the recognizer is disabled, none of the recognizer's speech recognition grammars are available for recognition operations. Setting the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property has no effect on the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property.","nodes":[{"content":"By default, the value of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A&gt;</ph> property is <ph id=\"ph2\">`true`</ph> for a newly instantiated instance of <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph>.","pos":[0,196],"source":"By default, the value of the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property is `true` for a newly instantiated instance of <xref:System.Speech.Recognition.SpeechRecognizer>."},{"content":"While the recognizer is disabled, none of the recognizer's speech recognition grammars are available for recognition operations.","pos":[197,325]},{"content":"Setting the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A&gt;</ph> property has no effect on the recognizer's <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.State%2A&gt;</ph> property.","pos":[326,523],"source":" Setting the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property has no effect on the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property."}]}],"pos":[67506,68159],"yaml":true,"extradata":"MT"},{"content":"`true` if this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object is performing speech recognition; otherwise, `false`.","nodes":[{"pos":[0,139],"content":"<ph id=\"ph1\">`true`</ph> if this <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer\"&gt;&lt;/xref&gt;</ph> object is performing speech recognition; otherwise, <ph id=\"ph3\">`false`</ph>.","source":"`true` if this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object is performing speech recognition; otherwise, `false`."}],"pos":[68274,68416],"yaml":true},{"content":"Gets a collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that are loaded in this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> instance.","nodes":[{"pos":[0,185],"content":"Gets a collection of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> objects that are loaded in this <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer\"&gt;&lt;/xref&gt;</ph> instance.","source":"Gets a collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that are loaded in this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> instance."}],"pos":[69429,69615],"yaml":true},{"content":"This property does not return any speech recognition grammars loaded by another application.","nodes":[{"pos":[0,92],"content":"This property does not return any speech recognition grammars loaded by another application.","nodes":[{"content":"This property does not return any speech recognition grammars loaded by another application.","pos":[0,92]}]}],"pos":[69626,69719],"yaml":true,"extradata":"MT"},{"content":"A collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that the application loaded into the current instance of the shared recognizer.","nodes":[{"pos":[0,162],"content":"A collection of the <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.Grammar\"&gt;&lt;/xref&gt;</ph> objects that the application loaded into the current instance of the shared recognizer.","source":"A collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that the application loaded into the current instance of the shared recognizer."}],"pos":[71402,71565],"yaml":true},{"content":"Loads a speech recognition grammar.","nodes":[{"pos":[0,35],"content":"Loads a speech recognition grammar.","nodes":[{"content":"Loads a speech recognition grammar.","pos":[0,35]}]}],"pos":[72727,72763],"yaml":true},{"content":"The shared recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \n  \n To load a speech recognition grammar asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method.","nodes":[{"pos":[0,399],"content":"The shared recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","nodes":[{"content":"The shared recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[0,399],"nodes":[{"content":"The shared recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer.","pos":[0,169]},{"content":"If the recognizer is running, applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[170,399],"source":" If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar."}]}]},{"pos":[406,544],"content":"To load a speech recognition grammar asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method.","nodes":[{"content":"To load a speech recognition grammar asynchronously, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A&gt;</ph> method.","pos":[0,138],"source":"To load a speech recognition grammar asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method."}]}],"pos":[72774,73323],"yaml":true,"extradata":"MT"},{"content":"The speech recognition grammar to load.","nodes":[{"pos":[0,39],"content":"The speech recognition grammar to load.","nodes":[{"content":"The speech recognition grammar to load.","pos":[0,39]}]}],"pos":[76798,76838],"yaml":true},{"content":"Asynchronously loads a speech recognition grammar.","nodes":[{"pos":[0,50],"content":"Asynchronously loads a speech recognition grammar.","nodes":[{"content":"Asynchronously loads a speech recognition grammar.","pos":[0,50]}]}],"pos":[78033,78084],"yaml":true},{"content":"When the recognizer completes this asynchronous operation, it raises a <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> event. The recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \n  \n To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> method.","nodes":[{"pos":[0,541],"content":"When the recognizer completes this asynchronous operation, it raises a <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> event. The recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","nodes":[{"content":"When the recognizer completes this asynchronous operation, it raises a <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> event. The recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[0,541],"nodes":[{"content":"When the recognizer completes this asynchronous operation, it raises a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted&gt;</ph> event.","pos":[0,148],"source":"When the recognizer completes this asynchronous operation, it raises a <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> event."},{"content":"The recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer.","pos":[149,311]},{"content":"If the recognizer is running, applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[312,541],"source":" If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar."}]}]},{"pos":[548,680],"content":"To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> method.","nodes":[{"content":"To load a speech recognition grammar synchronously, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A&gt;</ph> method.","pos":[0,132],"source":"To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> method."}]}],"pos":[78095,78780],"yaml":true,"extradata":"MT"},{"content":"The speech recognition grammar to load.","nodes":[{"pos":[0,39],"content":"The speech recognition grammar to load.","nodes":[{"content":"The speech recognition grammar to load.","pos":[0,39]}]}],"pos":[78976,79016],"yaml":true},{"content":"Occurs when the recognizer finishes the asynchronous loading of a speech recognition grammar.","nodes":[{"pos":[0,93],"content":"Occurs when the recognizer finishes the asynchronous loading of a speech recognition grammar.","nodes":[{"content":"Occurs when the recognizer finishes the asynchronous loading of a speech recognition grammar.","pos":[0,93]}]}],"pos":[80107,80201],"yaml":true},{"content":"The recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method initiates an asynchronous operation. The recognizer raises the `LoadGrammarCompleted` event when it completes the operation. To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> property.  \n  \n When you create a delegate for a `LoadGrammarCompleted` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,666],"content":"The recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method initiates an asynchronous operation. The recognizer raises the `LoadGrammarCompleted` event when it completes the operation. To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> property.","nodes":[{"content":"The recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A&gt;</ph> method initiates an asynchronous operation.","pos":[0,130],"source":"The recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method initiates an asynchronous operation."},{"content":"The recognizer raises the <ph id=\"ph1\">`LoadGrammarCompleted`</ph> event when it completes the operation.","pos":[131,218],"source":" The recognizer raises the `LoadGrammarCompleted` event when it completes the operation."},{"content":"To get the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object that the recognizer loaded, use the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A&gt;</ph> property of the associated <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ph>.","pos":[219,478],"source":" To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>."},{"content":"To get the current <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects the recognizer has loaded, use the recognizer's <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A&gt;</ph> property.","pos":[479,666],"source":" To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> property."}]},{"pos":[673,1095],"content":"When you create a delegate for a `LoadGrammarCompleted` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for a <ph id=\"ph1\">`LoadGrammarCompleted`</ph> event, you identify the method that will handle the event.","pos":[0,114],"source":"When you create a delegate for a `LoadGrammarCompleted` event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[115,208]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[209,295]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[296,422],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[80212,81312],"yaml":true,"extradata":"MT"},{"content":"Gets or sets the maximum number of alternate recognition results that the shared recognizer returns for each recognition operation.","nodes":[{"pos":[0,131],"content":"Gets or sets the maximum number of alternate recognition results that the shared recognizer returns for each recognition operation.","nodes":[{"content":"Gets or sets the maximum number of alternate recognition results that the shared recognizer returns for each recognition operation.","pos":[0,131]}]}],"pos":[86627,86759],"yaml":true},{"content":"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent other candidate interpretations of the input.  \n  \n The default value for <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> is 10.","nodes":[{"pos":[0,287],"content":"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent other candidate interpretations of the input.","nodes":[{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionResult.Alternates%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> class contains the collection of <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizedPhrase&gt;</ph> objects that represent other candidate interpretations of the input.","pos":[0,287],"source":"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent other candidate interpretations of the input."}]},{"pos":[294,389],"content":"The default value for <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> is 10.","nodes":[{"content":"The default value for <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A&gt;</ph> is 10.","pos":[0,95],"source":"The default value for <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> is 10."}]}],"pos":[86770,87164],"yaml":true,"extradata":"MT"},{"content":"The maximum number of alternate results that the speech recognizer returns for each recognition operation.","nodes":[{"pos":[0,106],"content":"The maximum number of alternate results that the speech recognizer returns for each recognition operation.","nodes":[{"content":"The maximum number of alternate results that the speech recognizer returns for each recognition operation.","pos":[0,106]}]}],"pos":[87282,87389],"yaml":true},{"content":"Gets or sets a value that indicates whether the shared recognizer pauses recognition operations while an application is handling a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"></xref> event.","nodes":[{"pos":[0,225],"content":"Gets or sets a value that indicates whether the shared recognizer pauses recognition operations while an application is handling a <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"&gt;&lt;/xref&gt;</ph> event.","source":"Gets or sets a value that indicates whether the shared recognizer pauses recognition operations while an application is handling a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"></xref> event."}],"pos":[88528,88754],"yaml":true},{"content":"Set this property to `true`, if within the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler your application needs to change the state of the speech recognition service or change the loaded or enabled speech recognition grammars before the speech recognition service processes more input.  \n  \n> [!NOTE]\n>  Setting the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> property to `true` causes each <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler in every application to block the Windows speech recognition service.  \n  \n To synchronize the changes to the shared recognizer with your application state, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.  \n  \n When <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> is `true`, during the execution of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> handler the speech recognition service pauses and buffers new audio input as it arrives. Once the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler exits, the speech recognition service resumes recognition and starts processing information from its input buffer.  \n  \n To enable or disable the speech recognition service, use the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property.","nodes":[{"pos":[0,327],"content":"Set this property to <ph id=\"ph1\">`true`</ph>, if within the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event handler your application needs to change the state of the speech recognition service or change the loaded or enabled speech recognition grammars before the speech recognition service processes more input.","source":"Set this property to `true`, if within the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler your application needs to change the state of the speech recognition service or change the loaded or enabled speech recognition grammars before the speech recognition service processes more input."},{"pos":[335,620],"content":"[!NOTE]\n Setting the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> property to `true` causes each <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler in every application to block the Windows speech recognition service.","leadings":["","> "],"nodes":[{"content":"Setting the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> property to <ph id=\"ph2\">`true`</ph> causes each <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event handler in every application to block the Windows speech recognition service.","pos":[9,283],"source":" Setting the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> property to `true` causes each <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler in every application to block the Windows speech recognition service."}]},{"pos":[627,800],"content":"To synchronize the changes to the shared recognizer with your application state, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.","nodes":[{"content":"To synchronize the changes to the shared recognizer with your application state, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[0,173],"source":"To synchronize the changes to the shared recognizer with your application state, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method."}]},{"pos":[807,1307],"content":"When <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> is `true`, during the execution of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> handler the speech recognition service pauses and buffers new audio input as it arrives. Once the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler exits, the speech recognition service resumes recognition and starts processing information from its input buffer.","nodes":[{"content":"When <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A&gt;</ph> is <ph id=\"ph2\">`true`</ph>, during the execution of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> handler the speech recognition service pauses and buffers new audio input as it arrives.","pos":[0,288],"source":"When <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> is `true`, during the execution of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> handler the speech recognition service pauses and buffers new audio input as it arrives."},{"content":"Once the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized&gt;</ph> event handler exits, the speech recognition service resumes recognition and starts processing information from its input buffer.","pos":[289,500],"source":" Once the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler exits, the speech recognition service resumes recognition and starts processing information from its input buffer."}]},{"pos":[1314,1445],"content":"To enable or disable the speech recognition service, use the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property.","nodes":[{"content":"To enable or disable the speech recognition service, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A&gt;</ph> property.","pos":[0,131],"source":"To enable or disable the speech recognition service, use the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property."}]}],"pos":[88765,90222],"yaml":true,"extradata":"MT"},{"content":"`true` if the shared recognizer waits to process input while any application is handling the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"></xref> event; otherwise, `false`.","nodes":[{"pos":[0,207],"content":"<ph id=\"ph1\">`true`</ph> if the shared recognizer waits to process input while any application is handling the <ph id=\"ph2\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"&gt;&lt;/xref&gt;</ph> event; otherwise, <ph id=\"ph3\">`false`</ph>.","source":"`true` if the shared recognizer waits to process input while any application is handling the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"></xref> event; otherwise, `false`."}],"pos":[90358,90568],"yaml":true},{"content":"Gets the current location of the recognizer in the audio input that it is processing.","nodes":[{"pos":[0,85],"content":"Gets the current location of the recognizer in the audio input that it is processing.","nodes":[{"content":"Gets the current location of the recognizer in the audio input that it is processing.","pos":[0,85]}]}],"pos":[91692,91778],"yaml":true},{"content":"The `RecognizerAudioPosition` property references the recognizer's position in processing its audio input. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property references the input device's position in its generated audio stream. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property.","nodes":[{"pos":[0,624],"content":"The `RecognizerAudioPosition` property references the recognizer's position in processing its audio input. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property references the input device's position in its generated audio stream. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property.","nodes":[{"content":"The <ph id=\"ph1\">`RecognizerAudioPosition`</ph> property references the recognizer's position in processing its audio input.","pos":[0,106],"source":"The `RecognizerAudioPosition` property references the recognizer's position in processing its audio input."},{"content":"By contrast, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A&gt;</ph> property references the input device's position in its generated audio stream.","pos":[107,269],"source":" By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property references the input device's position in its generated audio stream."},{"content":"These positions can be different.","pos":[270,303]},{"content":"For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A&gt;</ph> property is less than the value of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A&gt;</ph> property.","pos":[304,624],"source":" For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property."}]}],"pos":[91789,92414],"yaml":true,"extradata":"MT"},{"content":"The position of the recognizer in the audio input that it is processing.","nodes":[{"pos":[0,72],"content":"The position of the recognizer in the audio input that it is processing.","nodes":[{"content":"The position of the recognizer in the audio input that it is processing.","pos":[0,72]}]}],"pos":[92545,92618],"yaml":true},{"content":"Gets information about the shared speech recognizer.","nodes":[{"pos":[0,52],"content":"Gets information about the shared speech recognizer.","nodes":[{"content":"Gets information about the shared speech recognizer.","pos":[0,52]}]}],"pos":[93683,93736],"yaml":true},{"content":"This property returns information about the speech recognizer in use by Windows Speech Recognition.","nodes":[{"pos":[0,99],"content":"This property returns information about the speech recognizer in use by Windows Speech Recognition.","nodes":[{"content":"This property returns information about the speech recognizer in use by Windows Speech Recognition.","pos":[0,99]}]}],"pos":[93747,93847],"yaml":true,"extradata":"MT"},{"content":"Information about the shared speech recognizer.","nodes":[{"pos":[0,47],"content":"Information about the shared speech recognizer.","nodes":[{"content":"Information about the shared speech recognizer.","pos":[0,47]}]}],"pos":[94911,94959],"yaml":true},{"content":"Occurs when the recognizer pauses to synchronize recognition and other operations.","nodes":[{"pos":[0,82],"content":"Occurs when the recognizer pauses to synchronize recognition and other operations.","nodes":[{"content":"Occurs when the recognizer pauses to synchronize recognition and other operations.","pos":[0,82]}]}],"pos":[96066,96149],"yaml":true},{"content":"Applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognizer> before modifying its <xref:System.Speech.Recognition.Grammar> objects. For example, while the <xref:System.Speech.Recognition.SpeechRecognizer> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognizer> raises this event when it is ready to accept modifications.  \n  \n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,540],"content":"Applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognizer> before modifying its <xref:System.Speech.Recognition.Grammar> objects. For example, while the <xref:System.Speech.Recognition.SpeechRecognizer> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognizer> raises this event when it is ready to accept modifications.","nodes":[{"content":"Applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognizer> before modifying its <xref:System.Speech.Recognition.Grammar> objects. For example, while the <xref:System.Speech.Recognition.SpeechRecognizer> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognizer> raises this event when it is ready to accept modifications.","pos":[0,540],"nodes":[{"content":"Applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> to pause a running instance of <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> before modifying its <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects.","pos":[0,250],"source":"Applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognizer> before modifying its <xref:System.Speech.Recognition.Grammar> objects."},{"content":"For example, while the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> is paused, you can load, unload, enable, and disable <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> objects.","pos":[251,426],"source":" For example, while the <xref:System.Speech.Recognition.SpeechRecognizer> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> raises this event when it is ready to accept modifications.","pos":[427,540],"source":" The <xref:System.Speech.Recognition.SpeechRecognizer> raises this event when it is ready to accept modifications."}]}]},{"pos":[547,1020],"content":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached&gt;</ph> event, you identify the method that will handle the event.","pos":[0,165],"source":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[166,259]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[260,346]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[347,473],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[96160,97185],"yaml":true,"extradata":"MT"},{"content":"Requests that the shared recognizer pause and update its state and provides an offset and a user token for the associated event.","nodes":[{"pos":[0,128],"content":"Requests that the shared recognizer pause and update its state and provides an offset and a user token for the associated event.","nodes":[{"content":"Requests that the shared recognizer pause and update its state and provides an offset and a user token for the associated event.","pos":[0,128]}]}],"pos":[102957,103086],"yaml":true},{"content":"The recognizer does not initiate the recognizer update request until the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> equals the current <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> plus the value of the `audioPositionAheadToRaiseUpdate` parameter.  \n  \n When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.","nodes":[{"pos":[0,315],"content":"The recognizer does not initiate the recognizer update request until the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A&gt;</ph> equals the current <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A&gt;</ph> plus the value of the <ph id=\"ph3\">`audioPositionAheadToRaiseUpdate`</ph> parameter.","source":"The recognizer does not initiate the recognizer update request until the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> equals the current <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> plus the value of the `audioPositionAheadToRaiseUpdate` parameter."},{"pos":[322,650],"content":"When the recognizer generates the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached&gt;</ph> event, the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A&gt;</ph> property of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ph> contains the value of the <ph id=\"ph4\">`userToken`</ph> parameter.","source":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter."}],"pos":[103097,103752],"yaml":true,"extradata":"MT"},{"content":"User-defined information that contains information for the operation.","nodes":[{"pos":[0,69],"content":"User-defined information that contains information for the operation.","nodes":[{"content":"User-defined information that contains information for the operation.","pos":[0,69]}]}],"pos":[103954,104024],"yaml":true},{"content":"The offset from the current <xref href=\"System.Speech.Recognition.SpeechRecognizer.AudioPosition\"></xref> to delay the request.","nodes":[{"pos":[0,127],"content":"The offset from the current <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer.AudioPosition\"&gt;&lt;/xref&gt;</ph> to delay the request.","source":"The offset from the current <xref href=\"System.Speech.Recognition.SpeechRecognizer.AudioPosition\"></xref> to delay the request."}],"pos":[104113,104241],"yaml":true},{"content":"Requests that the shared recognizer pause and update its state and provides a user token for the associated event.","nodes":[{"pos":[0,114],"content":"Requests that the shared recognizer pause and update its state and provides a user token for the associated event.","nodes":[{"content":"Requests that the shared recognizer pause and update its state and provides a user token for the associated event.","pos":[0,114]}]}],"pos":[105427,105542],"yaml":true},{"content":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.  \n  \n To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.","nodes":[{"pos":[0,328],"content":"When the recognizer generates the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached&gt;</ph> event, the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A&gt;</ph> property of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ph> contains the value of the <ph id=\"ph4\">`userToken`</ph> parameter.","source":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter."},{"pos":[335,464],"content":"To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.","nodes":[{"content":"To specify an audio position offset, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[0,129],"source":"To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method."}]}],"pos":[105553,106022],"yaml":true,"extradata":"MT"},{"content":"User-defined information that contains information for the operation.","nodes":[{"pos":[0,69],"content":"User-defined information that contains information for the operation.","nodes":[{"content":"User-defined information that contains information for the operation.","pos":[0,69]}]}],"pos":[106182,106252],"yaml":true},{"content":"Requests that the shared recognizer pause and update its state.","nodes":[{"pos":[0,63],"content":"Requests that the shared recognizer pause and update its state.","nodes":[{"content":"Requests that the shared recognizer pause and update its state.","pos":[0,63]}]}],"pos":[107375,107439],"yaml":true},{"content":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> is `null`.  \n  \n To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.","nodes":[{"pos":[0,290],"content":"When the recognizer generates the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached&gt;</ph> event, the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A&gt;</ph> property of the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ph> is <ph id=\"ph4\">`null`</ph>.","source":"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> is `null`."},{"pos":[297,624],"content":"To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.","nodes":[{"content":"To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.","pos":[0,327],"nodes":[{"content":"To provide a user token, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[0,197],"source":"To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method."},{"content":"To specify an audio position offset, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> method.","pos":[198,327],"source":" To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method."}]}]}],"pos":[107450,108079],"yaml":true,"extradata":"MT"},{"content":"Occurs when the recognizer detects input that it can identify as speech.","nodes":[{"pos":[0,72],"content":"Occurs when the recognizer detects input that it can identify as speech.","nodes":[{"content":"Occurs when the recognizer detects input that it can identify as speech.","pos":[0,72]}]}],"pos":[109223,109296],"yaml":true},{"content":"The shared recognizer can raise this event in response to input. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. For more information see the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods.  \n  \n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,664],"content":"The shared recognizer can raise this event in response to input. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. For more information see the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods.","nodes":[{"content":"The shared recognizer can raise this event in response to input. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. For more information see the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods.","pos":[0,664],"nodes":[{"content":"The shared recognizer can raise this event in response to input.","pos":[0,64]},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ph> object indicates location in the input stream where the recognizer detected speech.","pos":[65,310],"source":" The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech."},{"content":"For more information see the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A&gt;</ph> and <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A&gt;</ph> properties and the <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A&gt;</ph> and <ph id=\"ph4\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A&gt;</ph> methods.","pos":[311,664],"source":" For more information see the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods."}]}]},{"pos":[671,1135],"content":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected&gt;</ph> event, you identify the method that will handle the event.","pos":[0,156],"source":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[157,250]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[251,337]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[338,464],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[109307,110447],"yaml":true,"extradata":"MT"},{"content":"Occurs when the recognizer has recognized a word or words that may be a component of multiple complete phrases in a grammar.","nodes":[{"pos":[0,124],"content":"Occurs when the recognizer has recognized a word or words that may be a component of multiple complete phrases in a grammar.","nodes":[{"content":"Occurs when the recognizer has recognized a word or words that may be a component of multiple complete phrases in a grammar.","pos":[0,124]}]}],"pos":[114286,114411],"yaml":true},{"content":"The shared recognizer can raise this event when the input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.  \n  \n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,267],"content":"The shared recognizer can raise this event when the input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","nodes":[{"content":"The shared recognizer can raise this event when the input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","pos":[0,267],"nodes":[{"content":"The shared recognizer can raise this event when the input is ambiguous.","pos":[0,71]},{"content":"For example, for a speech recognition grammar that supports recognition of either \"new game please\" or \"new game\", \"new game please\" is an unambiguous input, and \"new game\" is an ambiguous input.","pos":[72,267]}]}]},{"pos":[274,742],"content":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized&gt;</ph> event, you identify the method that will handle the event.","pos":[0,160],"source":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[161,254]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[255,341]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[342,468],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[114422,115177],"yaml":true,"extradata":"MT"},{"content":"Occurs when the recognizer receives input that does not match any of the speech recognition grammars it has loaded.","nodes":[{"pos":[0,115],"content":"Occurs when the recognizer receives input that does not match any of the speech recognition grammars it has loaded.","nodes":[{"content":"Occurs when the recognizer receives input that does not match any of the speech recognition grammars it has loaded.","pos":[0,115]}]}],"pos":[119302,119418],"yaml":true},{"content":"The shared recognizer raises this event if it determines that input does not match with sufficient confidence any of the loaded speech recognition grammars. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object.  \n  \n Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.  \n  \n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,389],"content":"The shared recognizer raises this event if it determines that input does not match with sufficient confidence any of the loaded speech recognition grammars. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object.","nodes":[{"content":"The shared recognizer raises this event if it determines that input does not match with sufficient confidence any of the loaded speech recognition grammars. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object.","pos":[0,389],"nodes":[{"content":"The shared recognizer raises this event if it determines that input does not match with sufficient confidence any of the loaded speech recognition grammars.","pos":[0,156]},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ph> contains the rejected <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object.","pos":[157,389],"source":" The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object."}]}]},{"pos":[396,676],"content":"Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.","nodes":[{"content":"Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.","pos":[0,280],"nodes":[{"content":"Confidence thresholds for the shared recognizer, managed by <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph>, are associated with a user profile and stored in the Windows registry.","pos":[0,181],"source":"Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry."},{"content":"Applications should not write changes to the registry for the properties of the shared recognizer.","pos":[182,280]}]}]},{"pos":[683,1158],"content":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected&gt;</ph> event, you identify the method that will handle the event.","pos":[0,167],"source":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[168,261]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[262,348]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[349,475],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[119429,120594],"yaml":true,"extradata":"MT"},{"content":"Occurs when the recognizer receives input that matches one of its speech recognition grammars.","nodes":[{"pos":[0,94],"content":"Occurs when the recognizer receives input that matches one of its speech recognition grammars.","nodes":[{"content":"Occurs when the recognizer receives input that matches one of its speech recognition grammars.","pos":[0,94]}]}],"pos":[124922,125017],"yaml":true},{"content":"The recognizer raises the `SpeechRecognized` event if it determines with sufficient confidence that input matches one of the loaded and enabled speech recognition grammars. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object.  \n  \n Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.  \n  \n When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event.  \n  \n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,405],"content":"The recognizer raises the `SpeechRecognized` event if it determines with sufficient confidence that input matches one of the loaded and enabled speech recognition grammars. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object.","nodes":[{"content":"The recognizer raises the <ph id=\"ph1\">`SpeechRecognized`</ph> event if it determines with sufficient confidence that input matches one of the loaded and enabled speech recognition grammars.","pos":[0,172],"source":"The recognizer raises the `SpeechRecognized` event if it determines with sufficient confidence that input matches one of the loaded and enabled speech recognition grammars."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A&gt;</ph> property of the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ph> contains the accepted <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.RecognitionResult&gt;</ph> object.","pos":[173,405],"source":" The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object."}]},{"pos":[412,692],"content":"Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.","nodes":[{"content":"Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.","pos":[0,280],"nodes":[{"content":"Confidence thresholds for the shared recognizer, managed by <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph>, are associated with a user profile and stored in the Windows registry.","pos":[0,181],"source":"Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry."},{"content":"Applications should not write changes to the registry for the properties of the shared recognizer.","pos":[182,280]}]}]},{"pos":[699,1123],"content":"When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event.","nodes":[{"content":"When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event.","pos":[0,424],"nodes":[{"content":"When the recognizer receives input that matches a grammar, the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object can raise the <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> event.","pos":[0,189],"source":"When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event."},{"content":"The <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.Grammar&gt;</ph> object's <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.Grammar.SpeechRecognized&gt;</ph> event is raised prior to the speech recognizer's <ph id=\"ph3\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized&gt;</ph> event.","pos":[190,424],"source":" The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event."}]}]},{"pos":[1130,1596],"content":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized&gt;</ph> event, you identify the method that will handle the event.","pos":[0,158],"source":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[159,252]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[253,339]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[340,466],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[125028,126633],"yaml":true,"extradata":"MT"},{"content":"Gets the state of a <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object.","nodes":[{"pos":[0,91],"content":"Gets the state of a <ph id=\"ph1\">&lt;xref href=\"System.Speech.Recognition.SpeechRecognizer\"&gt;&lt;/xref&gt;</ph> object.","source":"Gets the state of a <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object."}],"pos":[131728,131820],"yaml":true},{"content":"This read-only property indicates whether the shared recognizer resident in Windows is in the `Stopped` or the `Listening` state. For more information, see the <xref:System.Speech.Recognition.RecognizerState> enumeration.","nodes":[{"pos":[0,221],"content":"This read-only property indicates whether the shared recognizer resident in Windows is in the `Stopped` or the `Listening` state. For more information, see the <xref:System.Speech.Recognition.RecognizerState> enumeration.","nodes":[{"content":"This read-only property indicates whether the shared recognizer resident in Windows is in the <ph id=\"ph1\">`Stopped`</ph> or the <ph id=\"ph2\">`Listening`</ph> state.","pos":[0,129],"source":"This read-only property indicates whether the shared recognizer resident in Windows is in the `Stopped` or the `Listening` state."},{"content":"For more information, see the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizerState&gt;</ph> enumeration.","pos":[130,221],"source":" For more information, see the <xref:System.Speech.Recognition.RecognizerState> enumeration."}]}],"pos":[131831,132053],"yaml":true,"extradata":"MT"},{"content":"The state of the `SpeechRecognizer` object.","nodes":[{"pos":[0,43],"content":"The state of the <ph id=\"ph1\">`SpeechRecognizer`</ph> object.","source":"The state of the `SpeechRecognizer` object."}],"pos":[132225,132269],"yaml":true},{"content":"Occurs when the running state of the Windows Desktop Speech Technology recognition engine changes.","nodes":[{"pos":[0,98],"content":"Occurs when the running state of the Windows Desktop Speech Technology recognition engine changes.","nodes":[{"content":"Occurs when the running state of the Windows Desktop Speech Technology recognition engine changes.","pos":[0,98]}]}],"pos":[133301,133400],"yaml":true},{"content":"The shared recognizer raises this event when the state of Windows Speech Recognition changes to the <xref:System.Speech.Recognition.RecognizerState.Listening> or <xref:System.Speech.Recognition.RecognizerState.Stopped> state.  \n  \n To get the state of the shared recognizer at the time of the event, use the <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> property of the associated <xref:System.Speech.Recognition.StateChangedEventArgs>. To get the current state of the shared recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property.  \n  \n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"pos":[0,225],"content":"The shared recognizer raises this event when the state of Windows Speech Recognition changes to the <xref:System.Speech.Recognition.RecognizerState.Listening> or <xref:System.Speech.Recognition.RecognizerState.Stopped> state.","nodes":[{"content":"The shared recognizer raises this event when the state of Windows Speech Recognition changes to the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.RecognizerState.Listening&gt;</ph> or <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.RecognizerState.Stopped&gt;</ph> state.","pos":[0,225],"source":"The shared recognizer raises this event when the state of Windows Speech Recognition changes to the <xref:System.Speech.Recognition.RecognizerState.Listening> or <xref:System.Speech.Recognition.RecognizerState.Stopped> state."}]},{"pos":[232,605],"content":"To get the state of the shared recognizer at the time of the event, use the <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> property of the associated <xref:System.Speech.Recognition.StateChangedEventArgs>. To get the current state of the shared recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property.","nodes":[{"content":"To get the state of the shared recognizer at the time of the event, use the <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> property of the associated <xref:System.Speech.Recognition.StateChangedEventArgs>. To get the current state of the shared recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property.","pos":[0,373],"nodes":[{"content":"To get the state of the shared recognizer at the time of the event, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A&gt;</ph> property of the associated <ph id=\"ph2\">&lt;xref:System.Speech.Recognition.StateChangedEventArgs&gt;</ph>.","pos":[0,232],"source":"To get the state of the shared recognizer at the time of the event, use the <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> property of the associated <xref:System.Speech.Recognition.StateChangedEventArgs>."},{"content":"To get the current state of the shared recognizer, use the recognizer's <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.State%2A&gt;</ph> property.","pos":[233,373],"source":" To get the current state of the shared recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property."}]}]},{"pos":[612,1074],"content":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).","nodes":[{"content":"When you create a delegate for a <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.StateChanged&gt;</ph> event, you identify the method that will handle the event.","pos":[0,154],"source":"When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> event, you identify the method that will handle the event."},{"content":"To associate the event with your event handler, add an instance of the delegate to the event.","pos":[155,248]},{"content":"The event handler is called whenever the event occurs, unless you remove the delegate.","pos":[249,335]},{"content":"For more information about event-handler delegates, see <bpt id=\"p1\">[</bpt>Events and Delegates<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=162418)</ept>.","pos":[336,462],"source":" For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418)."}]}],"pos":[133411,134492],"yaml":true,"extradata":"MT"},{"content":"Unloads all speech recognition grammars from the shared recognizer.","nodes":[{"pos":[0,67],"content":"Unloads all speech recognition grammars from the shared recognizer.","nodes":[{"content":"Unloads all speech recognition grammars from the shared recognizer.","pos":[0,67]}]}],"pos":[139620,139688],"yaml":true},{"content":"If the recognizer is currently loading a grammar asynchronously, this method waits until the grammar is loaded, before it unloads all of the recognizer's grammars.  \n  \n To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> method.","nodes":[{"pos":[0,163],"content":"If the recognizer is currently loading a grammar asynchronously, this method waits until the grammar is loaded, before it unloads all of the recognizer's grammars.","nodes":[{"content":"If the recognizer is currently loading a grammar asynchronously, this method waits until the grammar is loaded, before it unloads all of the recognizer's grammars.","pos":[0,163]}]},{"pos":[170,282],"content":"To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> method.","nodes":[{"content":"To unload a specific grammar, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A&gt;</ph> method.","pos":[0,112],"source":"To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> method."}]}],"pos":[139699,139986],"yaml":true,"extradata":"MT"},{"content":"Unloads a specified speech recognition grammar from the shared recognizer.","nodes":[{"pos":[0,74],"content":"Unloads a specified speech recognition grammar from the shared recognizer.","nodes":[{"content":"Unloads a specified speech recognition grammar from the shared recognizer.","pos":[0,74]}]}],"pos":[141245,141320],"yaml":true},{"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar. To unload all grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> method.","nodes":[{"pos":[0,340],"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar. To unload all grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> method.","nodes":[{"content":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar. To unload all grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> method.","pos":[0,340],"nodes":[{"content":"If the recognizer is running, applications must use <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A&gt;</ph> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.","pos":[0,229],"source":"If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar."},{"content":"To unload all grammars, use the <ph id=\"ph1\">&lt;xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A&gt;</ph> method.","pos":[230,340],"source":" To unload all grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> method."}]}]}],"pos":[141331,141672],"yaml":true,"extradata":"MT"},{"content":"The grammar to unload.","nodes":[{"pos":[0,22],"content":"The grammar to unload.","nodes":[{"content":"The grammar to unload.","pos":[0,22]}]}],"pos":[141865,141888],"yaml":true}],"content":"### YamlMime:ManagedReference\nitems:\n- uid: System.Speech.Recognition.SpeechRecognizer\n  commentId: T:System.Speech.Recognition.SpeechRecognizer\n  id: SpeechRecognizer\n  children:\n  - System.Speech.Recognition.SpeechRecognizer.#ctor\n  - System.Speech.Recognition.SpeechRecognizer.AudioFormat\n  - System.Speech.Recognition.SpeechRecognizer.AudioLevel\n  - System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated\n  - System.Speech.Recognition.SpeechRecognizer.AudioPosition\n  - System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred\n  - System.Speech.Recognition.SpeechRecognizer.AudioState\n  - System.Speech.Recognition.SpeechRecognizer.AudioStateChanged\n  - System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)\n  - System.Speech.Recognition.SpeechRecognizer.Dispose\n  - System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)\n  - System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)\n  - System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  - System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted\n  - System.Speech.Recognition.SpeechRecognizer.Enabled\n  - System.Speech.Recognition.SpeechRecognizer.Grammars\n  - System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)\n  - System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  - System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted\n  - System.Speech.Recognition.SpeechRecognizer.MaxAlternates\n  - System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition\n  - System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition\n  - System.Speech.Recognition.SpeechRecognizer.RecognizerInfo\n  - System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached\n  - System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  - System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)\n  - System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate\n  - System.Speech.Recognition.SpeechRecognizer.SpeechDetected\n  - System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized\n  - System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected\n  - System.Speech.Recognition.SpeechRecognizer.SpeechRecognized\n  - System.Speech.Recognition.SpeechRecognizer.State\n  - System.Speech.Recognition.SpeechRecognizer.StateChanged\n  - System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars\n  - System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)\n  langs:\n  - csharp\n  name: SpeechRecognizer\n  nameWithType: SpeechRecognizer\n  fullName: System.Speech.Recognition.SpeechRecognizer\n  type: Class\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Provides access to the shared speech recognition service available on the Windows desktop.\n  remarks: \"Applications use the shared recognizer to access Windows Speech Recognition. Use the <xref:System.Speech.Recognition.SpeechRecognizer> object to add to the Windows speech user experience.  \\n  \\n This class provides control over various aspects of the speech recognition process:  \\n  \\n-   To manage speech recognition grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>.  \\n  \\n-   To get information about current speech recognition operations, subscribe to the <xref:System.Speech.Recognition.SpeechRecognizer>’s <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events.  \\n  \\n-   To view or modify the number of alternate results the recognizer returns, use the <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> property. The recognizer returns recognition results in a <xref:System.Speech.Recognition.RecognitionResult> object.  \\n  \\n-   To access or monitor the state of the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, and <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged>, and <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> events.  \\n  \\n-   To synchronize changes to the recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. The shared recognizer uses more than one thread to perform tasks.  \\n  \\n-   To emulate input to the shared recognizer, use the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods.  \\n  \\n The configuration of Windows Speech Recognition is managed by the use of the **Speech Properties** dialog in the **Control Panel**. This interface is used to select the default desktop speech recognition engine and language, the audio input device, and the sleep behavior of speech recognition. If the configuration of Windows Speech Recognition is changed while the application is running, (for instance, if speech recognition is disabled or the input language is changed), the change affects all <xref:System.Speech.Recognition.SpeechRecognizer> objects.  \\n  \\n To create an in-process speech recognizer that is independent of Windows Speech Recognition, use the <xref:System.Speech.Recognition.SpeechRecognitionEngine> class.  \\n  \\n> [!NOTE]\\n>  Always call <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> before you release your last reference to the speech recognizer. Otherwise, the resources it is using will not be freed until the garbage collector calls the recognizer object's `Finalize` method.\"\n  example:\n  - \"The following example is part of a console application that loads a speech recognition grammar and demonstrates asynchronous emulated input, the associated recognition results, and the associated events raised by the speech recognizer.  If Windows Speech Recognition is not running, then starting this application will also start Windows Speech Recognition. If Windows Speech Recognition is in the **Sleeping** state, then <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> always returns null.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n  \\n    // Indicate whether the asynchronous emulate recognition  \\n    // operation has completed.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an instance of the shared recognizer.  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n  \\n        // Create and load a sample grammar.  \\n        Grammar testGrammar =  \\n          new Grammar(new GrammarBuilder(\\\"testing testing\\\"));  \\n        testGrammar.Name = \\\"Test Grammar\\\";  \\n        recognizer.LoadGrammar(testGrammar);  \\n  \\n        // Attach event handlers for recognition events.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.EmulateRecognizeCompleted +=  \\n          new EventHandler<EmulateRecognizeCompletedEventArgs>(  \\n            EmulateRecognizeCompletedHandler);  \\n  \\n        completed = false;  \\n  \\n        // Start asynchronous emulated recognition.   \\n        // This matches the grammar and generates a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing testing\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        completed = false;  \\n  \\n        // Start asynchronous emulated recognition.  \\n        // This does not match the grammar or generate a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing one two three\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"Recognition result = {0}\\\",  \\n          e.Result.Text ?? \\\"<no text>\\\");  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No recognition result\\\");  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognizeCompleted event.  \\n    static void EmulateRecognizeCompletedHandler(  \\n      object sender, EmulateRecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Result == null)  \\n      {  \\n        Console.WriteLine(\\\"No result generated.\\\");  \\n      }  \\n  \\n      // Indicate the asynchronous operation is complete.  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: 'public class SpeechRecognizer : IDisposable'\n  inheritance:\n  - System.Object\n  implements:\n  - System.IDisposable\n  inheritedMembers:\n  - System.Object.Equals(System.Object)\n  - System.Object.Equals(System.Object,System.Object)\n  - System.Object.GetHashCode\n  - System.Object.GetType\n  - System.Object.MemberwiseClone\n  - System.Object.ReferenceEquals(System.Object,System.Object)\n  - System.Object.ToString\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.#ctor\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.#ctor\n  id: '#ctor'\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: SpeechRecognizer()\n  nameWithType: SpeechRecognizer.SpeechRecognizer()\n  fullName: SpeechRecognizer.SpeechRecognizer()\n  type: Constructor\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Initializes a new instance of the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> class.\n  remarks: Each <xref:System.Speech.Recognition.SpeechRecognizer> object maintains a separate set of speech recognition grammars.\n  example:\n  - \"The following example is part of a console application that loads a speech recognition grammar and demonstrates asynchronous emulated input, the associated recognition results, and the associated events raised by the speech recognizer. If Windows Speech Recognition is not running, then starting this application will also start Windows Speech Recognition. If Windows Speech Recognition is in the **Sleeping** state, then <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> always returns null.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n  \\n    // Indicate whether the asynchronous emulate recognition  \\n    // operation has completed.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an instance of the shared recognizer.  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n  \\n        // Create and load a sample grammar.  \\n        Grammar testGrammar =  \\n          new Grammar(new GrammarBuilder(\\\"testing testing\\\"));  \\n        testGrammar.Name = \\\"Test Grammar\\\";  \\n        recognizer.LoadGrammar(testGrammar);  \\n  \\n        // Attach event handlers for recognition events.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.EmulateRecognizeCompleted +=  \\n          new EventHandler<EmulateRecognizeCompletedEventArgs>(  \\n            EmulateRecognizeCompletedHandler);  \\n  \\n        completed = false;  \\n  \\n        // Start asynchronous emulated recognition.   \\n        // This matches the grammar and generates a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing testing\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        completed = false;  \\n  \\n        // Start asynchronous emulated recognition.  \\n        // This does not match the grammar or generate a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing one two three\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"Recognition result = {0}\\\",  \\n          e.Result.Text ?? \\\"<no text>\\\");  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No recognition result\\\");  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognizeCompleted event.  \\n    static void EmulateRecognizeCompletedHandler(  \\n      object sender, EmulateRecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Result == null)  \\n      {  \\n        Console.WriteLine(\\\"No result generated.\\\");  \\n      }  \\n  \\n      // Indicate the asynchronous operation is complete.  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public SpeechRecognizer ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognizer.#ctor*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioFormat\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.AudioFormat\n  id: AudioFormat\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: AudioFormat\n  nameWithType: SpeechRecognizer.AudioFormat\n  fullName: SpeechRecognizer.AudioFormat\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the format of the audio being received by the speech recognizer.\n  syntax:\n    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }\n    return:\n      type: System.Speech.AudioFormat.SpeechAudioFormatInfo\n      description: The audio input format for the speech recognizer, or `null` if the input to the recognizer is not configured.\n  overload: System.Speech.Recognition.SpeechRecognizer.AudioFormat*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioLevel\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.AudioLevel\n  id: AudioLevel\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: AudioLevel\n  nameWithType: SpeechRecognizer.AudioLevel\n  fullName: SpeechRecognizer.AudioLevel\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the level of the audio being received by the speech recognizer.\n  syntax:\n    content: public int AudioLevel { get; }\n    return:\n      type: System.Int32\n      description: The audio level of the input to the speech recognizer, from 0 through 100.\n  overload: System.Speech.Recognition.SpeechRecognizer.AudioLevel*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated\n  id: AudioLevelUpdated\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: AudioLevelUpdated\n  nameWithType: SpeechRecognizer.AudioLevelUpdated\n  fullName: SpeechRecognizer.AudioLevelUpdated\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the shared recognizer reports the level of its audio input.\n  remarks: \"The recognizer raises this event multiple times per second. The frequency with which the event is raised depends on the computer on which the application is running.  \\n  \\n To get the audio level at the time of the event, use the <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> property of the associated <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. To get the current audio level of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> property.  \\n  \\n When you create a delegate for an `AudioLevelUpdated` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example adds a handler for the `AudioLevelUpdated` event to a <xref:System.Speech.Recognition.SpeechRecognizer> object. The handler outputs the new audio level to the console.  \\n  \\n```csharp  \\nprivate SpeechRecognizer recognizer;  \\n  \\n// Initialize the SpeechRecognizer object.   \\nprivate void Initialize()  \\n{  \\n  recognizer = new SpeechRecognizer();  \\n  \\n  // Add an event handler for the AudioLevelUpdated event.  \\n  recognizer.AudioLevelUpdated +=   \\n    new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  \\n  \\n  // Add other initialization code here.  \\n  \\n}  \\n  \\n// Write the audio level to the console when the AudioLevelUpdated event is raised.  \\nvoid recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  \\n{  \\n  Console.WriteLine(\\\"The audio level is now: {0}.\\\", e.AudioLevel);  \\n}  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.AudioLevelUpdatedEventArgs> AudioLevelUpdated;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.AudioLevelUpdatedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioPosition\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.AudioPosition\n  id: AudioPosition\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: AudioPosition\n  nameWithType: SpeechRecognizer.AudioPosition\n  fullName: SpeechRecognizer.AudioPosition\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the current location in the audio stream being generated by the device that is providing input to the speech recognizer.\n  remarks: \"The shared recognizer receives input while the desktop speech recognition is running.  \\n  \\n The `AudioPosition` property references the input device's position in its generated audio stream. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property references the recognizer's position in processing audio input. These positions can be different.  For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property.\"\n  example:\n  - \"In the following example, the shared speech recognizer uses a dictation grammar to match speech input. A handler for the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> event writes to the console the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, and  <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> when the speech recognizer detects speech at its input.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognizer recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize a shared speech recognition engine.  \\n      recognizer = new SpeechRecognizer();  \\n  \\n      // Add handlers for events.  \\n      recognizer.LoadGrammarCompleted +=   \\n        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n      recognizer.SpeechRecognized +=   \\n        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n      recognizer.StateChanged +=   \\n        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  \\n      recognizer.SpeechDetected +=   \\n        new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  \\n  \\n      // Create a dictation grammar.  \\n      Grammar dictation = new DictationGrammar();  \\n      dictation.Name = \\\"Dictation\\\";  \\n  \\n      // Load the grammar object to the recognizer.  \\n      recognizer.LoadGrammarAsync(dictation);  \\n  \\n      // Keep the console window open.  \\n      Console.ReadLine();  \\n    }  \\n  \\n    // Gather information about detected speech and write it to the console.  \\n    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Speech detected:\\\");  \\n      Console.WriteLine(\\\"  Audio level: \\\" + recognizer.AudioLevel);  \\n      Console.WriteLine(\\\"  Audio position: \\\" + recognizer.AudioPosition);  \\n      Console.WriteLine(\\\"  Recognizer audio position: \\\" + recognizer.RecognizerAudioPosition);  \\n    }  \\n  \\n    // Write the text of the recognition result to the console.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {   \\n      Console.WriteLine(\\\"Speech recognized: \\\" + e.Result.Text);  \\n  \\n      // Add event handler code here.  \\n    }  \\n  \\n    // Write the name of the loaded grammar to the console.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n    }  \\n  \\n    // Put the shared speech recognizer into \\\"listening\\\" mode.  \\n    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  \\n    {  \\n      if (e.RecognizerState != RecognizerState.Stopped)  \\n      {  \\n        recognizer.EmulateRecognizeAsync(\\\"Start listening\\\");  \\n      }  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public TimeSpan AudioPosition { get; }\n    return:\n      type: System.TimeSpan\n      description: The current location in the speech recognizer's audio input stream through which it has received input.\n  overload: System.Speech.Recognition.SpeechRecognizer.AudioPosition*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred\n  id: AudioSignalProblemOccurred\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: AudioSignalProblemOccurred\n  nameWithType: SpeechRecognizer.AudioSignalProblemOccurred\n  fullName: SpeechRecognizer.AudioSignalProblemOccurred\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the recognizer encounters a problem in the audio signal.\n  remarks: \"To get which problem occurred, use the <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> property of the associated <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  \\n  \\n When you create a delegate for an `AudioSignalProblemOccurred` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example defines an event handler that gathers information about an `AudioSignalProblemOccurred` event.  \\n  \\n```  \\nprivate SpeechRecognizer recognizer;  \\n  \\n// Initialize the speech recognition engine.  \\nprivate void Initialize()  \\n{  \\n  recognizer = new SpeechRecognizer();  \\n  \\n  // Add a handler for the AudioSignalProblemOccurred event.  \\n  recognizer.AudioSignalProblemOccurred +=   \\n    new EventHandler<AudioSignalProblemOccurredEventArgs>(  \\n      recognizer_AudioSignalProblemOccurred);  \\n}  \\n  \\n// Gather information when the AudioSignalProblemOccurred event is raised.  \\nvoid recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  \\n{  \\n  StringBuilder details = new StringBuilder();  \\n  \\n  details.AppendLine(\\\"Audio signal problem information:\\\");  \\n  details.AppendFormat(  \\n    \\\" Audio level:               {0}\\\" + Environment.NewLine +  \\n    \\\" Audio position:            {1}\\\" + Environment.NewLine +  \\n    \\\" Audio signal problem:      {2}\\\" + Environment.NewLine +  \\n    \\\" Recognition engine audio position: {3}\\\" + Environment.NewLine,  \\n    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  \\n    e.recoEngineAudioPosition);  \\n  \\n  // Insert additional event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.AudioSignalProblemOccurredEventArgs> AudioSignalProblemOccurred;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.AudioSignalProblemOccurredEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioState\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.AudioState\n  id: AudioState\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: AudioState\n  nameWithType: SpeechRecognizer.AudioState\n  fullName: SpeechRecognizer.AudioState\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the state of the audio being received by the speech recognizer.\n  syntax:\n    content: public System.Speech.Recognition.AudioState AudioState { get; }\n    return:\n      type: System.Speech.Recognition.AudioState\n      description: The state of the audio input to the speech recognizer.\n  overload: System.Speech.Recognition.SpeechRecognizer.AudioState*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioStateChanged\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged\n  id: AudioStateChanged\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: AudioStateChanged\n  nameWithType: SpeechRecognizer.AudioStateChanged\n  fullName: SpeechRecognizer.AudioStateChanged\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the state changes in the audio being received by the recognizer.\n  remarks: \"To get the audio state at the time of the event, use the <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> property of the associated <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. To get the current audio state of the input to the recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> property. For more information about audio state, see the <xref:System.Speech.Recognition.AudioState> enumeration.  \\n  \\n When you create a delegate for an `AudioStateChanged` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example uses a handler for the `AudioStateChanged` event to write the recognizer's new <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> to the console each time it changes using a member of the <xref:System.Speech.Recognition.AudioState> enumeration.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognizer recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize a shared speech recognition engine.  \\n      recognizer = new SpeechRecognizer();  \\n  \\n        // Create and load a grammar.  \\n        Grammar dictation = new DictationGrammar();  \\n        dictation.Name = \\\"Dictation Grammar\\\";  \\n        recognizer.LoadGrammar(dictation);  \\n  \\n        // Attach event handlers.  \\n        recognizer.AudioStateChanged +=  \\n          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.StateChanged +=  \\n          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n  \\n    // Handle the AudioStateChanged event.  \\n    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"The new audio state is: \\\" + e.AudioState);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null && e.Result.Text != null)  \\n      {  \\n        Console.WriteLine();  \\n        Console.WriteLine(\\\"  Recognized text =  {0}\\\", e.Result.Text);  \\n        Console.WriteLine();  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"  Recognized text not available.\\\");  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Done.\\\");  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Put the recognizer into Listening mode.  \\n    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  \\n    {  \\n      if (e.RecognizerState != RecognizerState.Stopped)  \\n      {  \\n        Console.WriteLine();  \\n        recognizer.EmulateRecognizeAsync(\\\"Start listening\\\");  \\n      }  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.AudioStateChangedEventArgs> AudioStateChanged;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.AudioStateChangedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)\n  id: Dispose(System.Boolean)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: Dispose(Boolean)\n  nameWithType: SpeechRecognizer.Dispose(Boolean)\n  fullName: SpeechRecognizer.Dispose(Boolean)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object and releases resources used during the session.\n  syntax:\n    content: protected virtual void Dispose (bool disposing);\n    parameters:\n    - id: disposing\n      type: System.Boolean\n      description: '`true` to release both managed and unmanaged resources; `false` to release only unmanaged resources.'\n  overload: System.Speech.Recognition.SpeechRecognizer.Dispose*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.Dispose\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.Dispose\n  id: Dispose\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: Dispose()\n  nameWithType: SpeechRecognizer.Dispose()\n  fullName: SpeechRecognizer.Dispose()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Disposes the <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object.\n  syntax:\n    content: public void Dispose ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognizer.Dispose*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)\n  id: EmulateRecognize(System.String)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: EmulateRecognize(String)\n  nameWithType: SpeechRecognizer.EmulateRecognize(String)\n  fullName: SpeechRecognizer.EmulateRecognize(String)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition.\n  remarks: The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.\n  example:\n  - \"The following example loads a sample grammar to the shared recognizer and emulates input to the recognizer. If Windows Speech Recognition is not running, then starting this application will also start Windows Speech Recognition. If Windows Speech Recognition is in the **Sleeping** state, then <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> always returns null.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      // Initialize an instance of the shared recognizer.  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n        // Create and load a sample grammar.  \\n        Grammar testGrammar =  \\n          new Grammar(new GrammarBuilder(\\\"testing testing\\\"));  \\n        testGrammar.Name = \\\"Test Grammar\\\";  \\n  \\n        recognizer.LoadGrammar(testGrammar);  \\n  \\n        RecognitionResult result;  \\n  \\n        // This EmulateRecognize call matches the grammar and returns a  \\n        // recognition result.  \\n        result = recognizer.EmulateRecognize(\\\"testing testing\\\");  \\n        OutputResult(result);  \\n  \\n        // This EmulateRecognize call does not match the grammar and   \\n        // returns null.  \\n        result = recognizer.EmulateRecognize(\\\"testing one two three\\\");  \\n        OutputResult(result);  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Output information about a recognition result to the console.  \\n    private static void OutputResult(RecognitionResult result)  \\n    {  \\n      if (result != null)  \\n      {  \\n        Console.WriteLine(\\\"Recognition result = {0}\\\",  \\n          result.Text ?? \\\"<no text>\\\");  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No recognition result\\\");  \\n      }  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input for the recognition operation.\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state.\n  overload: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  id: EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of specific words to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.\n  remarks: \"This method creates a <xref:System.Speech.Recognition.RecognitionResult> object using the information provided in the `wordUnits` parameter.  \\n  \\n The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\"\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: wordUnits\n      type: System.Speech.Recognition.RecognizedWordUnit[]\n      description: An array of word units that contains the input for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state.\n  overload: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  id: EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: EmulateRecognize(String, CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognize(String, CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognize(String, CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the shared speech recognizer, using text instead of audio for synchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.\n  remarks: The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\n  syntax:\n    content: public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input phrase for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n    return:\n      type: System.Speech.Recognition.RecognitionResult\n      description: The recognition result for the recognition operation, or `null`, if the operation is not successful or Windows Speech Recognition is in the **Sleeping** state.\n  overload: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)\n  id: EmulateRecognizeAsync(System.String)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: EmulateRecognizeAsync(String)\n  nameWithType: SpeechRecognizer.EmulateRecognizeAsync(String)\n  fullName: SpeechRecognizer.EmulateRecognizeAsync(String)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition.\n  remarks: The recognizers that ship with Vista and Windows 7 ignore case and character width when applying grammar rules to the input phrase. For more information about this type of comparison, see the <xref:System.Globalization.CompareOptions> enumeration values <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> and <xref:System.Globalization.CompareOptions.IgnoreWidth>. The recognizers also ignore new lines and extra white space and treat punctuation as literal input.\n  example:\n  - \"The following example is part of a console application that loads a speech recognition grammar and demonstrates asynchronous emulated input, the associated recognition results, and the associated events raised by the speech recognizer. If Windows Speech Recognition is not running, then starting this application will also start Windows Speech Recognition. If Windows Speech Recognition is in the **Sleeping** state, then <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> always returns null.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      // Initialize an instance of the shared recognizer.  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n        // Create and load a sample grammar.  \\n        Grammar testGrammar =  \\n          new Grammar(new GrammarBuilder(\\\"testing testing\\\"));  \\n        testGrammar.Name = \\\"Test Grammar\\\";  \\n  \\n        recognizer.LoadGrammar(testGrammar);  \\n  \\n        // Attach event handlers for recognition events.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.EmulateRecognizeCompleted +=  \\n          new EventHandler<EmulateRecognizeCompletedEventArgs>(  \\n            EmulateRecognizeCompletedHandler);  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing testing\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call does not match the grammar   \\n        // or generate a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing one two three\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"Recognition result = {0}\\\",  \\n          e.Result.Text ?? \\\"<no text>\\\");  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No recognition result\\\");  \\n      }  \\n    }  \\n  \\n    // Handle the EmulateRecognizeCompleted event.   \\n    static void EmulateRecognizeCompletedHandler(  \\n      object sender, EmulateRecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Result == null)  \\n      {  \\n        Console.WriteLine(\\\"No result generated.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void EmulateRecognizeAsync (string inputText);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input for the recognition operation.\n  overload: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  id: EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of specific words to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the words and the loaded speech recognition grammars.\n  remarks: \"This method creates a <xref:System.Speech.Recognition.RecognitionResult> object using the information provided in the `wordUnits` parameter.  \\n  \\n The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\"\n  syntax:\n    content: public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: wordUnits\n      type: System.Speech.Recognition.RecognizedWordUnit[]\n      description: An array of word units that contains the input for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n  overload: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  id: EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: EmulateRecognizeAsync(String, CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognizeAsync(String, CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognizeAsync(String, CompareOptions)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Emulates input of a phrase to the shared speech recognizer, using text instead of audio for asynchronous speech recognition, and specifies how the recognizer handles Unicode comparison between the phrase and the loaded speech recognition grammars.\n  remarks: The recognizer uses the `compareOptions` when it applies grammar rules to the input phrase. The recognizers that ship with Vista and Windows 7 ignore case if the <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> or <xref:System.Globalization.CompareOptions.IgnoreCase> value is present. The recognizers always ignore the character width and never ignore the Kana type. The recognizers also ignore new lines and extra white space and treats punctuation as literal input. For more information about character width and Kana type, see the <xref:System.Globalization.CompareOptions> enumeration.\n  syntax:\n    content: public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);\n    parameters:\n    - id: inputText\n      type: System.String\n      description: The input phrase for the recognition operation.\n    - id: compareOptions\n      type: System.Globalization.CompareOptions\n      description: A bitwise combination of the enumeration values that describe the type of comparison to use for the emulated recognition operation.\n  overload: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted\n  id: EmulateRecognizeCompleted\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: EmulateRecognizeCompleted\n  nameWithType: SpeechRecognizer.EmulateRecognizeCompleted\n  fullName: SpeechRecognizer.EmulateRecognizeCompleted\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the shared recognizer finalizes an asynchronous recognition operation for emulated input.\n  remarks: \"Each <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> method begins an asynchronous recognition operation. The recognizer raises the `EmulateRecognizeCompleted` event when it finalizes the asynchronous operation.  \\n  \\n The asynchronous recognition operation can raise the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events. The <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> event is the last such event that the recognizer raises for a given operation.  \\n  \\n When you create a delegate for an `EmulateRecognizeCompleted` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example is part of a console application that loads a speech recognition grammar and demonstrates asynchronous emulated input, the associated recognition results, and the associated events raised by the speech recognizer. If Windows Speech Recognition is not running, then starting this application will also start Windows Speech Recognition. If Windows Speech Recognition is in the **Sleeping** mode, then <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> always returns null.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether the asynchronous emulate recognition  \\n    // operation has completed.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize an instance of the shared recognizer.  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n        // Create and load a sample grammar.  \\n        Grammar testGrammar =  \\n          new Grammar(new GrammarBuilder(\\\"testing testing\\\"));  \\n        testGrammar.Name = \\\"Test Grammar\\\";  \\n        recognizer.LoadGrammar(testGrammar);  \\n  \\n        // Attach event handlers for recognition events.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  \\n        recognizer.EmulateRecognizeCompleted +=   \\n          new EventHandler<EmulateRecognizeCompletedEventArgs>(  \\n            EmulateRecognizeCompletedHandler);  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing testing\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call does not match the grammar  \\n        // or generate a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing one two three\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"Recognition result = {0}\\\",  \\n          e.Result.Text ?? \\\"<no text>\\\");  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No recognition result\\\");  \\n      }  \\n    }  \\n  \\n    // Handle the EmulateRecognizeCompleted event.  \\n    static void EmulateRecognizeCompletedHandler(  \\n      object sender, EmulateRecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Result == null)  \\n      {  \\n        Console.WriteLine(\\\"No result generated.\\\");  \\n      }  \\n  \\n      // Indicate the asynchronous operation is complete.  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> EmulateRecognizeCompleted;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.EmulateRecognizeCompletedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.Enabled\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.Enabled\n  id: Enabled\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: Enabled\n  nameWithType: SpeechRecognizer.Enabled\n  fullName: SpeechRecognizer.Enabled\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets a value that indicates whether this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object is ready to process speech.\n  remarks: \"Changes to this property do not affect other instances of the <xref:System.Speech.Recognition.SpeechRecognizer> class.  \\n  \\n By default, the value of the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property is `true` for a newly instantiated instance of <xref:System.Speech.Recognition.SpeechRecognizer>. While the recognizer is disabled, none of the recognizer's speech recognition grammars are available for recognition operations. Setting the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property has no effect on the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property.\"\n  syntax:\n    content: public bool Enabled { get; set; }\n    return:\n      type: System.Boolean\n      description: '`true` if this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object is performing speech recognition; otherwise, `false`.'\n  overload: System.Speech.Recognition.SpeechRecognizer.Enabled*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.Grammars\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.Grammars\n  id: Grammars\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: Grammars\n  nameWithType: SpeechRecognizer.Grammars\n  fullName: SpeechRecognizer.Grammars\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets a collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that are loaded in this <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> instance.\n  remarks: This property does not return any speech recognition grammars loaded by another application.\n  example:\n  - \"The following example outputs information to the console for each speech recognition grammar loaded into the shared speech recognizer.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Collections.Generic;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n        Grammar sampleGrammar = new Grammar(new GrammarBuilder(\\\"sample phrase\\\"));  \\n        sampleGrammar.Name = \\\"Sample Grammar\\\";  \\n        recognizer.LoadGrammar(sampleGrammar);  \\n  \\n        OutputGrammarList(recognizer);  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    private static void OutputGrammarList(SpeechRecognizer recognizer)  \\n    {  \\n      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  \\n      if (grammars.Count > 0)  \\n      {  \\n        Console.WriteLine(\\\"Loaded grammars:\\\");  \\n        foreach (Grammar g in grammars)  \\n        {  \\n          Console.WriteLine(\\\"  Grammar: {0}\\\",  \\n            (g.Name != null) ? g.Name : \\\"<no name>\\\");  \\n        }  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No grammars loaded.\\\");  \\n      }  \\n    }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public System.Collections.ObjectModel.ReadOnlyCollection<System.Speech.Recognition.Grammar> Grammars { get; }\n    return:\n      type: System.Collections.ObjectModel.ReadOnlyCollection{System.Speech.Recognition.Grammar}\n      description: A collection of the <xref href=\"System.Speech.Recognition.Grammar\"></xref> objects that the application loaded into the current instance of the shared recognizer.\n  overload: System.Speech.Recognition.SpeechRecognizer.Grammars*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)\n  id: LoadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: LoadGrammar(Grammar)\n  nameWithType: SpeechRecognizer.LoadGrammar(Grammar)\n  fullName: SpeechRecognizer.LoadGrammar(Grammar)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Loads a speech recognition grammar.\n  remarks: \"The shared recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \\n  \\n To load a speech recognition grammar asynchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method.\"\n  example:\n  - \"The following example is part of a console application that loads a speech recognition grammar and demonstrates asynchronous emulated input, the associated recognition results, and the associated events raised by the speech recognizer. If Windows Speech Recognition is not running, then starting this application will also start Windows Speech Recognition. If Windows Speech Recognition is in the **Sleeping** state, then <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> always returns null.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Threading;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n    // Indicate whether the asynchronous emulate recognition  \\n    // operation has completed.  \\n    static bool completed;  \\n  \\n    static void Main(string[] args)  \\n    {  \\n      // Initialize an instance of the shared recognizer.  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n        // Create and load a sample grammar.  \\n        Grammar testGrammar =  \\n          new Grammar(new GrammarBuilder(\\\"testing testing\\\"));  \\n        testGrammar.Name = \\\"Test Grammar\\\";  \\n  \\n        recognizer.LoadGrammar(testGrammar);  \\n  \\n        // Attach event handlers for recognition events.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(  \\n            SpeechRecognizedHandler);  \\n        recognizer.EmulateRecognizeCompleted +=  \\n          new EventHandler<EmulateRecognizeCompletedEventArgs>(  \\n            EmulateRecognizeCompletedHandler);  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing testing\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n  \\n        completed = false;  \\n  \\n        // This EmulateRecognizeAsync call does not match the grammar   \\n        // or generate a SpeechRecognized event.  \\n        recognizer.EmulateRecognizeAsync(\\\"testing one two three\\\");  \\n  \\n        // Wait for the asynchronous operation to complete.  \\n        while (!completed)  \\n        {  \\n          Thread.Sleep(333);  \\n        }  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void SpeechRecognizedHandler(  \\n      object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      if (e.Result != null)  \\n      {  \\n        Console.WriteLine(\\\"Recognition result = {0}\\\",  \\n          e.Result.Text ?? \\\"<no text>\\\");  \\n      }  \\n      else  \\n      {  \\n        Console.WriteLine(\\\"No recognition result\\\");  \\n      }  \\n    }   \\n  \\n    // Handle the EmulateRecognizeCompleted event.   \\n    static void EmulateRecognizeCompletedHandler(  \\n      object sender, EmulateRecognizeCompletedEventArgs e)  \\n    {  \\n      if (e.Result == null)  \\n      {  \\n        Console.WriteLine(\\\"No result generated.\\\");  \\n      }  \\n  \\n      completed = true;  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public void LoadGrammar (System.Speech.Recognition.Grammar grammar);\n    parameters:\n    - id: grammar\n      type: System.Speech.Recognition.Grammar\n      description: The speech recognition grammar to load.\n  overload: System.Speech.Recognition.SpeechRecognizer.LoadGrammar*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  id: LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: LoadGrammarAsync(Grammar)\n  nameWithType: SpeechRecognizer.LoadGrammarAsync(Grammar)\n  fullName: SpeechRecognizer.LoadGrammarAsync(Grammar)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Asynchronously loads a speech recognition grammar.\n  remarks: \"When the recognizer completes this asynchronous operation, it raises a <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> event. The recognizer throws an exception if the speech recognition grammar is already loaded, is being asynchronously loaded, or has failed to load into any recognizer. If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar.  \\n  \\n To load a speech recognition grammar synchronously, use the <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> method.\"\n  syntax:\n    content: public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);\n    parameters:\n    - id: grammar\n      type: System.Speech.Recognition.Grammar\n      description: The speech recognition grammar to load.\n  overload: System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted\n  id: LoadGrammarCompleted\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: LoadGrammarCompleted\n  nameWithType: SpeechRecognizer.LoadGrammarCompleted\n  fullName: SpeechRecognizer.LoadGrammarCompleted\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the recognizer finishes the asynchronous loading of a speech recognition grammar.\n  remarks: \"The recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> method initiates an asynchronous operation. The recognizer raises the `LoadGrammarCompleted` event when it completes the operation. To get the <xref:System.Speech.Recognition.Grammar> object that the recognizer loaded, use the <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> property of the associated <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. To get the current <xref:System.Speech.Recognition.Grammar> objects the recognizer has loaded, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> property.  \\n  \\n When you create a delegate for a `LoadGrammarCompleted` event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example creates a shared speech recognizer, and then creates two types of grammars for recognizing specific words and for accepting free dictation. The example asynchronously loads all the created grammars to the recognizer. Handlers for the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> events write to the console the name of the grammar that was used to perform the recognition and the text of the recognition result, respectively.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognizer recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize a shared speech recognition engine.  \\n      recognizer = new SpeechRecognizer();  \\n  \\n        // Add a handler for the LoadGrammarCompleted event.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n  \\n        // Add a handler for the SpeechRecognized event.  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Add a handler for the StateChanged event.  \\n        recognizer.StateChanged +=  \\n          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  \\n  \\n        // Create \\\"yesno\\\" grammar.  \\n        Choices yesChoices = new Choices(new string[] { \\\"yes\\\", \\\"yup\\\", \\\"yeah}\\\" });  \\n        SemanticResultValue yesValue =  \\n            new SemanticResultValue(yesChoices, (bool)true);  \\n        Choices noChoices = new Choices(new string[] { \\\"no\\\", \\\"nope\\\", \\\"neah\\\" });  \\n        SemanticResultValue noValue =  \\n            new SemanticResultValue(noChoices, (bool)false);  \\n        SemanticResultKey yesNoKey =  \\n            new SemanticResultKey(\\\"yesno\\\", new Choices(new GrammarBuilder[] { yesValue, noValue }));  \\n        Grammar yesnoGrammar = new Grammar(yesNoKey);  \\n        yesnoGrammar.Name = \\\"yesNo\\\";  \\n  \\n        // Create \\\"done\\\" grammar.  \\n        Grammar doneGrammar =  \\n          new Grammar(new Choices(new string[] { \\\"done\\\", \\\"exit\\\", \\\"quit\\\", \\\"stop\\\" }));  \\n        doneGrammar.Name = \\\"Done\\\";  \\n  \\n        // Create dictation grammar.  \\n        Grammar dictation = new DictationGrammar();  \\n        dictation.Name = \\\"Dictation\\\";  \\n  \\n        // Load grammars to the recognizer.  \\n        recognizer.LoadGrammarAsync(yesnoGrammar);  \\n        recognizer.LoadGrammarAsync(doneGrammar);  \\n        recognizer.LoadGrammarAsync(dictation);  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar({0}): {1}\\\", e.Result.Grammar.Name, e.Result.Text);  \\n  \\n      // Add event handler code here.  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.   \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      string grammarName = e.Grammar.Name;  \\n      bool grammarLoaded = e.Grammar.Loaded;  \\n  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\"LoadGrammar for {0} failed with a {1}.\\\",  \\n        grammarName, e.Error.GetType().Name);  \\n  \\n        // Add exception handling code here.  \\n      }  \\n  \\n      Console.WriteLine(\\\"Grammar {0} {1} loaded.\\\",  \\n      grammarName, (grammarLoaded) ? \\\"is\\\" : \\\"is not\\\");  \\n    }  \\n  \\n    // Put the shared speech recognizer into \\\"listening\\\" mode.   \\n    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  \\n    {  \\n      if (e.RecognizerState != RecognizerState.Stopped)  \\n      {  \\n        recognizer.EmulateRecognizeAsync(\\\"Start listening\\\");  \\n      }  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.LoadGrammarCompletedEventArgs> LoadGrammarCompleted;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.LoadGrammarCompletedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.MaxAlternates\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.MaxAlternates\n  id: MaxAlternates\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: MaxAlternates\n  nameWithType: SpeechRecognizer.MaxAlternates\n  fullName: SpeechRecognizer.MaxAlternates\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets the maximum number of alternate recognition results that the shared recognizer returns for each recognition operation.\n  remarks: \"The <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> property of the <xref:System.Speech.Recognition.RecognitionResult> class contains the collection of <xref:System.Speech.Recognition.RecognizedPhrase> objects that represent other candidate interpretations of the input.  \\n  \\n The default value for <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> is 10.\"\n  syntax:\n    content: public int MaxAlternates { get; set; }\n    return:\n      type: System.Int32\n      description: The maximum number of alternate results that the speech recognizer returns for each recognition operation.\n  overload: System.Speech.Recognition.SpeechRecognizer.MaxAlternates*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition\n  id: PauseRecognizerOnRecognition\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: PauseRecognizerOnRecognition\n  nameWithType: SpeechRecognizer.PauseRecognizerOnRecognition\n  fullName: SpeechRecognizer.PauseRecognizerOnRecognition\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets or sets a value that indicates whether the shared recognizer pauses recognition operations while an application is handling a <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"></xref> event.\n  remarks: \"Set this property to `true`, if within the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler your application needs to change the state of the speech recognition service or change the loaded or enabled speech recognition grammars before the speech recognition service processes more input.  \\n  \\n> [!NOTE]\\n>  Setting the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> property to `true` causes each <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler in every application to block the Windows speech recognition service.  \\n  \\n To synchronize the changes to the shared recognizer with your application state, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.  \\n  \\n When <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> is `true`, during the execution of the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> handler the speech recognition service pauses and buffers new audio input as it arrives. Once the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> event handler exits, the speech recognition service resumes recognition and starts processing information from its input buffer.  \\n  \\n To enable or disable the speech recognition service, use the <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> property.\"\n  syntax:\n    content: public bool PauseRecognizerOnRecognition { get; set; }\n    return:\n      type: System.Boolean\n      description: '`true` if the shared recognizer waits to process input while any application is handling the <xref href=\"System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized\"></xref> event; otherwise, `false`.'\n  overload: System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition\n  id: RecognizerAudioPosition\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: RecognizerAudioPosition\n  nameWithType: SpeechRecognizer.RecognizerAudioPosition\n  fullName: SpeechRecognizer.RecognizerAudioPosition\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the current location of the recognizer in the audio input that it is processing.\n  remarks: The `RecognizerAudioPosition` property references the recognizer's position in processing its audio input. By contrast, the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property references the input device's position in its generated audio stream. These positions can be different. For example, if the recognizer has received input for which it has not yet generated a recognition result then the value of the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> property is less than the value of the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> property.\n  syntax:\n    content: public TimeSpan RecognizerAudioPosition { get; }\n    return:\n      type: System.TimeSpan\n      description: The position of the recognizer in the audio input that it is processing.\n  overload: System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerInfo\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.RecognizerInfo\n  id: RecognizerInfo\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: RecognizerInfo\n  nameWithType: SpeechRecognizer.RecognizerInfo\n  fullName: SpeechRecognizer.RecognizerInfo\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets information about the shared speech recognizer.\n  remarks: This property returns information about the speech recognizer in use by Windows Speech Recognition.\n  example:\n  - \"The following example sends information about the shared recognizer to the console.  \\n  \\n```csharp  \\n  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SharedRecognizer  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n    {  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n        Console.WriteLine(\\\"Recognizer information for the shared recognizer:\\\");  \\n        Console.WriteLine(\\\"  Name: {0}\\\", recognizer.RecognizerInfo.Name);  \\n        Console.WriteLine(\\\"  Culture: {0}\\\", recognizer.RecognizerInfo.Culture.ToString());  \\n        Console.WriteLine(\\\"  Description: {0}\\\", recognizer.RecognizerInfo.Description);  \\n      }  \\n  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }\n    return:\n      type: System.Speech.Recognition.RecognizerInfo\n      description: Information about the shared speech recognizer.\n  overload: System.Speech.Recognition.SpeechRecognizer.RecognizerInfo*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached\n  id: RecognizerUpdateReached\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: RecognizerUpdateReached\n  nameWithType: SpeechRecognizer.RecognizerUpdateReached\n  fullName: SpeechRecognizer.RecognizerUpdateReached\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the recognizer pauses to synchronize recognition and other operations.\n  remarks: \"Applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause a running instance of <xref:System.Speech.Recognition.SpeechRecognizer> before modifying its <xref:System.Speech.Recognition.Grammar> objects. For example, while the <xref:System.Speech.Recognition.SpeechRecognizer> is paused, you can load, unload, enable, and disable <xref:System.Speech.Recognition.Grammar> objects. The <xref:System.Speech.Recognition.SpeechRecognizer> raises this event when it is ready to accept modifications.  \\n  \\n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example shows a console application that loads and unloads <xref:System.Speech.Recognition.Grammar> objects. The application uses the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method to request the speech recognition engine to pause so it can receive an update. The application then loads or unloads a <xref:System.Speech.Recognition.Grammar> object.  \\n  \\n At each update, a handler for <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event writes the name and status of the currently loaded <xref:System.Speech.Recognition.Grammar> objects to the console. As grammars are loaded and unloaded, the application first recognizes the names of farm animals, then the names of farm animals and the names of fruits, then only the names of fruits.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\nusing System.Collections.Generic;  \\nusing System.Threading;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognizer recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize a shared speech recognition engine.  \\n      recognizer = new SpeechRecognizer();  \\n  \\n      // Create the first grammar - Farm.  \\n      Choices animals = new Choices(new string[] { \\\"cow\\\", \\\"pig\\\", \\\"goat\\\" });  \\n      GrammarBuilder farm = new GrammarBuilder(animals);  \\n      Grammar farmAnimals = new Grammar(farm);  \\n      farmAnimals.Name = \\\"Farm\\\";  \\n  \\n      // Create the second grammar - Fruit.  \\n      Choices fruit = new Choices(new string[] { \\\"apples\\\", \\\"peaches\\\", \\\"oranges\\\" });  \\n      GrammarBuilder favorite = new GrammarBuilder(fruit);  \\n      Grammar favoriteFruit = new Grammar(favorite);  \\n      favoriteFruit.Name = \\\"Fruit\\\";  \\n  \\n      // Attach event handlers.  \\n      recognizer.SpeechRecognized +=  \\n        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n      recognizer.RecognizerUpdateReached +=  \\n        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  \\n      recognizer.StateChanged +=   \\n        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  \\n  \\n      // Load the Farm grammar.  \\n      recognizer.LoadGrammar(farmAnimals);  \\n      Console.WriteLine(\\\"Grammar Farm is loaded\\\");  \\n  \\n      // Pause to recognize farm animals.  \\n      Thread.Sleep(7000);  \\n      Console.WriteLine();  \\n  \\n      // Request an update and load the Fruit grammar.  \\n      recognizer.RequestRecognizerUpdate();  \\n      recognizer.LoadGrammarAsync(favoriteFruit);  \\n      Thread.Sleep(5000);  \\n  \\n      // Request an update and unload the Farm grammar.  \\n      recognizer.RequestRecognizerUpdate();  \\n      recognizer.UnloadGrammar(farmAnimals);  \\n      Thread.Sleep(5000);  \\n  \\n      // Keep the console window open.  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Press any key to exit...\\\");  \\n      Console.ReadKey();  \\n    }  \\n  \\n    // Put the shared speech recognizer into \\\"listening\\\" mode.  \\n    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  \\n    {  \\n      if (e.RecognizerState != RecognizerState.Stopped)  \\n      {  \\n        recognizer.EmulateRecognizeAsync(\\\"Start listening\\\");  \\n      }  \\n    }  \\n  \\n    // At the update, get the names and enabled status of the currently loaded grammars.  \\n    public static void recognizer_RecognizerUpdateReached(  \\n      object sender, RecognizerUpdateReachedEventArgs e)  \\n    {  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Update reached:\\\");  \\n      Thread.Sleep(1000);  \\n  \\n      string qualifier;  \\n      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  \\n      foreach (Grammar g in grammars)  \\n      {  \\n        qualifier = (g.Enabled) ? \\\"enabled\\\" : \\\"disabled\\\";  \\n        Console.WriteLine(\\\"  Grammar {0} is loaded and is {1}.\\\",  \\n        g.Name, qualifier);  \\n      }  \\n    }  \\n  \\n    // Write the text of the recognized phrase to the console.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"  Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.RecognizerUpdateReachedEventArgs> RecognizerUpdateReached;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.RecognizerUpdateReachedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  id: RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: RequestRecognizerUpdate(Object, TimeSpan)\n  nameWithType: SpeechRecognizer.RequestRecognizerUpdate(Object, TimeSpan)\n  fullName: SpeechRecognizer.RequestRecognizerUpdate(Object, TimeSpan)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Requests that the shared recognizer pause and update its state and provides an offset and a user token for the associated event.\n  remarks: \"The recognizer does not initiate the recognizer update request until the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> equals the current <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> plus the value of the `audioPositionAheadToRaiseUpdate` parameter.  \\n  \\n When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.\"\n  syntax:\n    content: public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);\n    parameters:\n    - id: userToken\n      type: System.Object\n      description: User-defined information that contains information for the operation.\n    - id: audioPositionAheadToRaiseUpdate\n      type: System.TimeSpan\n      description: The offset from the current <xref href=\"System.Speech.Recognition.SpeechRecognizer.AudioPosition\"></xref> to delay the request.\n  overload: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)\n  id: RequestRecognizerUpdate(System.Object)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: RequestRecognizerUpdate(Object)\n  nameWithType: SpeechRecognizer.RequestRecognizerUpdate(Object)\n  fullName: SpeechRecognizer.RequestRecognizerUpdate(Object)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Requests that the shared recognizer pause and update its state and provides a user token for the associated event.\n  remarks: \"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contains the value of the `userToken` parameter.  \\n  \\n To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.\"\n  syntax:\n    content: public void RequestRecognizerUpdate (object userToken);\n    parameters:\n    - id: userToken\n      type: System.Object\n      description: User-defined information that contains information for the operation.\n  overload: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate\n  id: RequestRecognizerUpdate\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: RequestRecognizerUpdate()\n  nameWithType: SpeechRecognizer.RequestRecognizerUpdate()\n  fullName: SpeechRecognizer.RequestRecognizerUpdate()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Requests that the shared recognizer pause and update its state.\n  remarks: \"When the recognizer generates the <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> event, the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> property of the <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> is `null`.  \\n  \\n To provide a user token, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> or <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method. To specify an audio position offset, use the <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> method.\"\n  syntax:\n    content: public void RequestRecognizerUpdate ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechDetected\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected\n  id: SpeechDetected\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: SpeechDetected\n  nameWithType: SpeechRecognizer.SpeechDetected\n  fullName: SpeechRecognizer.SpeechDetected\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the recognizer detects input that it can identify as speech.\n  remarks: \"The shared recognizer can raise this event in response to input. The <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> property of the associated <xref:System.Speech.Recognition.SpeechDetectedEventArgs> object indicates location in the input stream where the recognizer detected speech. For more information see the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> properties and the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> methods.  \\n  \\n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example is part of a console application for choosing origin and destination cities for a flight. The application recognizes phrases such as \\\"I want to fly from Miami to Chicago.\\\"  The example uses the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> event to report the <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> each time speech is detected.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize a shared speech recognition engine.  \\n    {  \\n      using (SpeechRecognizer recognizer =  \\n         new SpeechRecognizer())  \\n      {  \\n  \\n        // Create a grammar.  \\n        Choices cities = new Choices(new string[] {   \\n          \\\"Los Angeles\\\", \\\"New York\\\", \\\"Chicago\\\", \\\"San Francisco\\\", \\\"Miami\\\", \\\"Dallas\\\" });  \\n  \\n        GrammarBuilder gb = new GrammarBuilder();  \\n        gb.Append(\\\"I would like to fly from\\\");  \\n        gb.Append(cities);  \\n        gb.Append(\\\"to\\\");  \\n        gb.Append(cities);  \\n  \\n        // Create a Grammar object and load it to the recognizer.  \\n        Grammar g = new Grammar(gb);  \\n        g.Name = (\\\"City Chooser\\\");  \\n        recognizer.LoadGrammarAsync(g);  \\n  \\n        // Attach event handlers.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n        recognizer.SpeechDetected +=   \\n          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechDetected event.  \\n    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech detected at AudioPosition = {0}\\\", e.AudioPosition);  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechDetectedEventArgs> SpeechDetected;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechDetectedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized\n  id: SpeechHypothesized\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: SpeechHypothesized\n  nameWithType: SpeechRecognizer.SpeechHypothesized\n  fullName: SpeechRecognizer.SpeechHypothesized\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the recognizer has recognized a word or words that may be a component of multiple complete phrases in a grammar.\n  remarks: \"The shared recognizer can raise this event when the input is ambiguous. For example, for a speech recognition grammar that supports recognition of either \\\"new game please\\\" or \\\"new game\\\", \\\"new game please\\\" is an unambiguous input, and \\\"new game\\\" is an ambiguous input.  \\n  \\n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example recognizes phrases such as \\\"Display the list of artists in the jazz category\\\". The example uses the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> event to display incomplete phrase fragments in the console as they are recognized.  \\n  \\n```  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize a shared speech recognition engine.  \\n    {  \\n      using (SpeechRecognizer recognizer =  \\n         new SpeechRecognizer())  \\n      {  \\n  \\n        // Create a grammar.  \\n        //  Create lists of alternative choices.  \\n        Choices listTypes = new Choices(new string[] { \\\"albums\\\", \\\"artists\\\" });  \\n        Choices genres = new Choices(new string[] {   \\n          \\\"blues\\\", \\\"classical\\\", \\\"gospel\\\", \\\"jazz\\\", \\\"rock\\\" });  \\n  \\n        //  Create a GrammarBuilder object and assemble the grammar components.  \\n        GrammarBuilder mediaMenu = new GrammarBuilder(\\\"Display the list of\\\");  \\n        mediaMenu.Append(listTypes);  \\n        mediaMenu.Append(\\\"in the\\\");  \\n        mediaMenu.Append(genres);  \\n        mediaMenu.Append(\\\"category.\\\");  \\n  \\n        //  Build a Grammar object from the GrammarBuilder.  \\n        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  \\n        mediaMenuGrammar.Name = \\\"Media Chooser\\\";  \\n  \\n        // Attach event handlers.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.SpeechHypothesized +=   \\n          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  \\n  \\n        // Load the grammar object to the recognizer.  \\n        recognizer.LoadGrammarAsync(mediaMenuGrammar);  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechHypothesized event.  \\n    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech hypothesized: \\\" + e.Result.Text);  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechHypothesizedEventArgs> SpeechHypothesized;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechHypothesizedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected\n  id: SpeechRecognitionRejected\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: SpeechRecognitionRejected\n  nameWithType: SpeechRecognizer.SpeechRecognitionRejected\n  fullName: SpeechRecognizer.SpeechRecognitionRejected\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the recognizer receives input that does not match any of the speech recognition grammars it has loaded.\n  remarks: \"The shared recognizer raises this event if it determines that input does not match with sufficient confidence any of the loaded speech recognition grammars. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the rejected <xref:System.Speech.Recognition.RecognitionResult> object.  \\n  \\n Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.  \\n  \\n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example recognizes phrases such as \\\"Display the list of artists in the jazz category\\\" or \\\"Display albums gospel\\\". The example uses a handler for the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> event to display a notification in the console when the speech input cannot be matched to the contents of the grammar with sufficient confidence to produce a successful recognition.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize a shared speech recognition engine.  \\n    {  \\n      using (SpeechRecognizer recognizer =  \\n         new SpeechRecognizer())  \\n      {  \\n  \\n        // Create a grammar.  \\n        //  Create lists of alternative choices.  \\n        Choices listTypes = new Choices(new string[] { \\\"albums\\\", \\\"artists\\\" });  \\n        Choices genres = new Choices(new string[] {   \\n          \\\"blues\\\", \\\"classical\\\", \\\"gospel\\\", \\\"jazz\\\", \\\"rock\\\" });  \\n  \\n        //  Create a GrammarBuilder object and assemble the grammar components.  \\n        GrammarBuilder mediaMenu = new GrammarBuilder(\\\"Display\\\");  \\n        mediaMenu.Append(\\\"the list of\\\", 0, 1);  \\n        mediaMenu.Append(listTypes);  \\n        mediaMenu.Append(\\\"in the\\\", 0, 1);  \\n        mediaMenu.Append(genres);  \\n        mediaMenu.Append(\\\"category\\\", 0, 1);  \\n  \\n        //  Build a Grammar object from the GrammarBuilder.  \\n        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  \\n        mediaMenuGrammar.Name = \\\"Media Chooser\\\";  \\n  \\n        // Attach event handlers.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n        recognizer.SpeechRecognized +=  \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n        recognizer.SpeechRecognitionRejected +=   \\n          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  \\n  \\n        // Load the grammar object to the recognizer.  \\n        recognizer.LoadGrammarAsync(mediaMenuGrammar);  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the SpeechRecognitionRejected event.  \\n    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech input was rejected.\\\");  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech recognized: \\\" + e.Result.Text);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> SpeechRecognitionRejected;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechRecognitionRejectedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechRecognized\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized\n  id: SpeechRecognized\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: SpeechRecognized\n  nameWithType: SpeechRecognizer.SpeechRecognized\n  fullName: SpeechRecognizer.SpeechRecognized\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the recognizer receives input that matches one of its speech recognition grammars.\n  remarks: \"The recognizer raises the `SpeechRecognized` event if it determines with sufficient confidence that input matches one of the loaded and enabled speech recognition grammars. The <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> property of the <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contains the accepted <xref:System.Speech.Recognition.RecognitionResult> object.  \\n  \\n Confidence thresholds for the shared recognizer, managed by <xref:System.Speech.Recognition.SpeechRecognizer>, are associated with a user profile and stored in the Windows registry. Applications should not write changes to the registry for the properties of the shared recognizer.  \\n  \\n When the recognizer receives input that matches a grammar, the <xref:System.Speech.Recognition.Grammar> object can raise the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event. The <xref:System.Speech.Recognition.Grammar> object's <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event is raised prior to the speech recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event.  \\n  \\n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example is part of a console application that loads a speech recognition grammar and demonstrates speech input to the shared recognizer, the associated recognition results, and the associated events raised by the speech recognizer. If Windows Speech Recognition is not running, then starting this application will also start Windows Speech Recognition.  \\n  \\n Spoken input such as \\\"I want to fly from Chicago to Miami\\\" will trigger a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event. Speaking the phrase \\\"Fly me from Houston to Chicago \\\" will not trigger a <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event.  \\n  \\n The example uses a handler for the <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> event to display successfully recognized phrases and the semantics they contain in the console.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    static void Main(string[] args)  \\n  \\n    // Initialize a shared speech recognition engine.  \\n    {  \\n      using (SpeechRecognizer recognizer = new SpeechRecognizer())  \\n      {  \\n  \\n        // Create SemanticResultValue objects that contain cities and airport codes.  \\n        SemanticResultValue chicago = new SemanticResultValue(\\\"Chicago\\\", \\\"ORD\\\");  \\n        SemanticResultValue boston = new SemanticResultValue(\\\"Boston\\\", \\\"BOS\\\");  \\n        SemanticResultValue miami = new SemanticResultValue(\\\"Miami\\\", \\\"MIA\\\");  \\n        SemanticResultValue dallas = new SemanticResultValue(\\\"Dallas\\\", \\\"DFW\\\");  \\n  \\n        // Create a Choices object and add the SemanticResultValue objects, using  \\n        // implicit conversion from SemanticResultValue to GrammarBuilder  \\n        Choices cities = new Choices();  \\n        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  \\n  \\n        // Build the phrase and add SemanticResultKeys.  \\n        GrammarBuilder chooseCities = new GrammarBuilder();  \\n        chooseCities.Append(\\\"I want to fly from\\\");  \\n        chooseCities.Append(new SemanticResultKey(\\\"origin\\\", cities));  \\n        chooseCities.Append(\\\"to\\\");  \\n        chooseCities.Append(new SemanticResultKey(\\\"destination\\\", cities));  \\n  \\n        // Build a Grammar object from the GrammarBuilder.  \\n        Grammar bookFlight = new Grammar(chooseCities);  \\n        bookFlight.Name = \\\"Book Flight\\\";  \\n  \\n        // Add a handler for the LoadGrammarCompleted event.  \\n        recognizer.LoadGrammarCompleted +=  \\n          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n  \\n        // Add a handler for the SpeechRecognized event.  \\n        recognizer.SpeechRecognized +=   \\n          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n        // Load the grammar object to the recognizer.  \\n        recognizer.LoadGrammarAsync(bookFlight);  \\n  \\n        // Keep the console window open.  \\n        Console.ReadLine();  \\n      }  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Grammar loaded: \\\" + e.Grammar.Name);  \\n      Console.WriteLine();  \\n    }  \\n  \\n    // Handle the SpeechRecognized event.  \\n    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n      Console.WriteLine(\\\"Speech recognized:  \\\" + e.Result.Text);  \\n      Console.WriteLine();  \\n      Console.WriteLine(\\\"Semantic results:\\\");  \\n      Console.WriteLine(\\\"  The flight origin is \\\" + e.Result.Semantics[\\\"origin\\\"].Value);  \\n      Console.WriteLine(\\\"  The flight destination is \\\" + e.Result.Semantics[\\\"destination\\\"].Value);  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.SpeechRecognizedEventArgs> SpeechRecognized;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.SpeechRecognizedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.State\n  commentId: P:System.Speech.Recognition.SpeechRecognizer.State\n  id: State\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: State\n  nameWithType: SpeechRecognizer.State\n  fullName: SpeechRecognizer.State\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the state of a <xref href=\"System.Speech.Recognition.SpeechRecognizer\"></xref> object.\n  remarks: This read-only property indicates whether the shared recognizer resident in Windows is in the `Stopped` or the `Listening` state. For more information, see the <xref:System.Speech.Recognition.RecognizerState> enumeration.\n  syntax:\n    content: public System.Speech.Recognition.RecognizerState State { get; }\n    return:\n      type: System.Speech.Recognition.RecognizerState\n      description: The state of the `SpeechRecognizer` object.\n  overload: System.Speech.Recognition.SpeechRecognizer.State*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.StateChanged\n  commentId: E:System.Speech.Recognition.SpeechRecognizer.StateChanged\n  id: StateChanged\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: StateChanged\n  nameWithType: SpeechRecognizer.StateChanged\n  fullName: SpeechRecognizer.StateChanged\n  type: Event\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Occurs when the running state of the Windows Desktop Speech Technology recognition engine changes.\n  remarks: \"The shared recognizer raises this event when the state of Windows Speech Recognition changes to the <xref:System.Speech.Recognition.RecognizerState.Listening> or <xref:System.Speech.Recognition.RecognizerState.Stopped> state.  \\n  \\n To get the state of the shared recognizer at the time of the event, use the <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> property of the associated <xref:System.Speech.Recognition.StateChangedEventArgs>. To get the current state of the shared recognizer, use the recognizer's <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> property.  \\n  \\n When you create a delegate for a <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> event, you identify the method that will handle the event. To associate the event with your event handler, add an instance of the delegate to the event. The event handler is called whenever the event occurs, unless you remove the delegate. For more information about event-handler delegates, see [Events and Delegates](http://go.microsoft.com/fwlink/?LinkId=162418).\"\n  example:\n  - \"The following example creates a shared speech recognizer, and then creates two types of grammars for recognizing specific words and for accepting free dictation. The example asynchronously loads all the created grammars to the recognizer.  A handler for the <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> event uses the <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> method to put Windows Recognition in \\\"listening\\\" mode.  \\n  \\n```csharp  \\nusing System;  \\nusing System.Speech.Recognition;  \\n  \\nnamespace SampleRecognition  \\n{  \\n  class Program  \\n  {  \\n    private static SpeechRecognizer recognizer;  \\n    public static void Main(string[] args)  \\n    {  \\n  \\n      // Initialize a shared speech recognition engine.  \\n      recognizer = new SpeechRecognizer();  \\n  \\n      // Add a handler for the LoadGrammarCompleted event.  \\n      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  \\n  \\n      // Add a handler for the SpeechRecognized event.  \\n      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  \\n  \\n      // Add a handler for the StateChanged event.  \\n      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  \\n  \\n      // Create \\\"yesno\\\" grammar.  \\n      Choices yesChoices = new Choices(new string[] { \\\"yes\\\", \\\"yup\\\", \\\"yah}\\\" });  \\n      SemanticResultValue yesValue =  \\n          new SemanticResultValue(yesChoices, (bool)true);  \\n      Choices noChoices = new Choices(new string[] { \\\"no\\\", \\\"nope\\\", \\\"nah\\\" });  \\n      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  \\n      SemanticResultKey yesNoKey =  \\n          new SemanticResultKey(\\\"yesno\\\", new Choices(new GrammarBuilder[] { yesValue, noValue }));  \\n      Grammar yesnoGrammar = new Grammar(yesNoKey);  \\n      yesnoGrammar.Name = \\\"yesNo\\\";  \\n  \\n      // Create \\\"done\\\" grammar.  \\n      Grammar doneGrammar =  \\n        new Grammar(new Choices(new string[] { \\\"done\\\", \\\"exit\\\", \\\"quit\\\", \\\"stop\\\" }));  \\n      doneGrammar.Name = \\\"Done\\\";  \\n  \\n      // Create dictation grammar.  \\n      Grammar dictation = new DictationGrammar();  \\n      dictation.Name = \\\"Dictation\\\";  \\n  \\n      // Load grammars to the recognizer.  \\n      recognizer.LoadGrammarAsync(yesnoGrammar);  \\n      recognizer.LoadGrammarAsync(doneGrammar);  \\n      recognizer.LoadGrammarAsync(dictation);  \\n  \\n      // Keep the console window open.  \\n      Console.ReadLine();  \\n    }  \\n  \\n    // Put the shared speech recognizer into \\\"listening\\\" mode.  \\n    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  \\n    {  \\n     if (e.RecognizerState != RecognizerState.Stopped)  \\n      {  \\n        recognizer.EmulateRecognizeAsync(\\\"Start listening\\\");  \\n      }  \\n    }  \\n  \\n    // Write the text of the recognized phrase to the console.  \\n    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  \\n    {  \\n     Console.WriteLine(\\\"Grammar({0}): {1}\\\", e.Result.Grammar.Name, e.Result.Text);  \\n  \\n      // Add event handler code here.  \\n    }  \\n  \\n    // Handle the LoadGrammarCompleted event.  \\n    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  \\n    {  \\n     string grammarName = e.Grammar.Name;  \\n      bool grammarLoaded = e.Grammar.Loaded;  \\n      if (e.Error != null)  \\n      {  \\n        Console.WriteLine(\\\"LoadGrammar for {0} failed with a {1}.\\\",  \\n        grammarName, e.Error.GetType().Name);  \\n      }  \\n  \\n      // Add exception handling code here.  \\n      Console.WriteLine(\\\"Grammar {0} {1} loaded.\\\",  \\n      grammarName, (grammarLoaded) ? \\\"is\\\" : \\\"is not\\\");  \\n    }  \\n  }  \\n}  \\n  \\n```\"\n  syntax:\n    content: public event EventHandler<System.Speech.Recognition.StateChangedEventArgs> StateChanged;\n    return:\n      type: System.EventHandler{System.Speech.Recognition.StateChangedEventArgs}\n      description: ''\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars\n  id: UnloadAllGrammars\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: UnloadAllGrammars()\n  nameWithType: SpeechRecognizer.UnloadAllGrammars()\n  fullName: SpeechRecognizer.UnloadAllGrammars()\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Unloads all speech recognition grammars from the shared recognizer.\n  remarks: \"If the recognizer is currently loading a grammar asynchronously, this method waits until the grammar is loaded, before it unloads all of the recognizer's grammars.  \\n  \\n To unload a specific grammar, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> method.\"\n  syntax:\n    content: public void UnloadAllGrammars ();\n    parameters: []\n  overload: System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\n- uid: System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)\n  commentId: M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)\n  id: UnloadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  langs:\n  - csharp\n  name: UnloadGrammar(Grammar)\n  nameWithType: SpeechRecognizer.UnloadGrammar(Grammar)\n  fullName: SpeechRecognizer.UnloadGrammar(Grammar)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Unloads a specified speech recognition grammar from the shared recognizer.\n  remarks: If the recognizer is running, applications must use <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> to pause the speech recognition engine before loading, unloading,  enabling, or disabling a grammar. To unload all grammars, use the <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> method.\n  syntax:\n    content: public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);\n    parameters:\n    - id: grammar\n      type: System.Speech.Recognition.Grammar\n      description: The grammar to unload.\n  overload: System.Speech.Recognition.SpeechRecognizer.UnloadGrammar*\n  exceptions: []\n  version:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n  ms.technology:\n  - dotnet-standard\n  ms.author: kbridge\n  manager: ghogen\nreferences:\n- uid: System.Object\n  parent: System\n  isExternal: false\n  name: Object\n  nameWithType: Object\n  fullName: System.Object\n- uid: System.Speech.Recognition.SpeechRecognizer.#ctor\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: SpeechRecognizer()\n  nameWithType: SpeechRecognizer.SpeechRecognizer()\n  fullName: SpeechRecognizer.SpeechRecognizer()\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioFormat\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioFormat\n  nameWithType: SpeechRecognizer.AudioFormat\n  fullName: SpeechRecognizer.AudioFormat\n- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo\n  parent: System.Speech.AudioFormat\n  isExternal: false\n  name: SpeechAudioFormatInfo\n  nameWithType: SpeechAudioFormatInfo\n  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioLevel\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioLevel\n  nameWithType: SpeechRecognizer.AudioLevel\n  fullName: SpeechRecognizer.AudioLevel\n- uid: System.Int32\n  parent: System\n  isExternal: false\n  name: Int32\n  nameWithType: Int32\n  fullName: System.Int32\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioLevelUpdated\n  nameWithType: SpeechRecognizer.AudioLevelUpdated\n  fullName: SpeechRecognizer.AudioLevelUpdated\n- uid: System.EventHandler`1\n  name: EventHandler<TEventArgs>\n  nameWithType: EventHandler<TEventArgs>\n  fullName: System.EventHandler<TEventArgs>\n- uid: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n  name: AudioLevelUpdatedEventArgs\n  nameWithType: AudioLevelUpdatedEventArgs\n  fullName: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.AudioLevelUpdatedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<AudioLevelUpdatedEventArgs>\n  nameWithType: EventHandler<AudioLevelUpdatedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.AudioLevelUpdatedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n    name: AudioLevelUpdatedEventArgs\n    nameWithType: AudioLevelUpdatedEventArgs\n    fullName: System.Speech.Recognition.AudioLevelUpdatedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioPosition\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioPosition\n  nameWithType: SpeechRecognizer.AudioPosition\n  fullName: SpeechRecognizer.AudioPosition\n- uid: System.TimeSpan\n  parent: System\n  isExternal: false\n  name: TimeSpan\n  nameWithType: TimeSpan\n  fullName: System.TimeSpan\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioSignalProblemOccurred\n  nameWithType: SpeechRecognizer.AudioSignalProblemOccurred\n  fullName: SpeechRecognizer.AudioSignalProblemOccurred\n- uid: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n  name: AudioSignalProblemOccurredEventArgs\n  nameWithType: AudioSignalProblemOccurredEventArgs\n  fullName: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.AudioSignalProblemOccurredEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<AudioSignalProblemOccurredEventArgs>\n  nameWithType: EventHandler<AudioSignalProblemOccurredEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n    name: AudioSignalProblemOccurredEventArgs\n    nameWithType: AudioSignalProblemOccurredEventArgs\n    fullName: System.Speech.Recognition.AudioSignalProblemOccurredEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioState\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioState\n  nameWithType: SpeechRecognizer.AudioState\n  fullName: SpeechRecognizer.AudioState\n- uid: System.Speech.Recognition.AudioState\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: AudioState\n  nameWithType: AudioState\n  fullName: System.Speech.Recognition.AudioState\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioStateChanged\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioStateChanged\n  nameWithType: SpeechRecognizer.AudioStateChanged\n  fullName: SpeechRecognizer.AudioStateChanged\n- uid: System.Speech.Recognition.AudioStateChangedEventArgs\n  name: AudioStateChangedEventArgs\n  nameWithType: AudioStateChangedEventArgs\n  fullName: System.Speech.Recognition.AudioStateChangedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.AudioStateChangedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<AudioStateChangedEventArgs>\n  nameWithType: EventHandler<AudioStateChangedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.AudioStateChangedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.AudioStateChangedEventArgs\n    name: AudioStateChangedEventArgs\n    nameWithType: AudioStateChangedEventArgs\n    fullName: System.Speech.Recognition.AudioStateChangedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: Dispose(Boolean)\n  nameWithType: SpeechRecognizer.Dispose(Boolean)\n  fullName: SpeechRecognizer.Dispose(Boolean)\n- uid: System.Boolean\n  parent: System\n  isExternal: false\n  name: Boolean\n  nameWithType: Boolean\n  fullName: System.Boolean\n- uid: System.Speech.Recognition.SpeechRecognizer.Dispose\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: Dispose()\n  nameWithType: SpeechRecognizer.Dispose()\n  fullName: SpeechRecognizer.Dispose()\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognize(String)\n  nameWithType: SpeechRecognizer.EmulateRecognize(String)\n  fullName: SpeechRecognizer.EmulateRecognize(String)\n- uid: System.Speech.Recognition.RecognitionResult\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognitionResult\n  nameWithType: RecognitionResult\n  fullName: System.Speech.Recognition.RecognitionResult\n- uid: System.String\n  parent: System\n  isExternal: false\n  name: String\n  nameWithType: String\n  fullName: System.String\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognize(RecognizedWordUnit[], CompareOptions)\n- uid: System.Speech.Recognition.RecognizedWordUnit\n  name: RecognizedWordUnit\n  nameWithType: RecognizedWordUnit\n  fullName: System.Speech.Recognition.RecognizedWordUnit\n- uid: System.Speech.Recognition.RecognizedWordUnit[]\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizedWordUnit[]\n  nameWithType: RecognizedWordUnit[]\n  fullName: System.Speech.Recognition.RecognizedWordUnit[]\n  spec.csharp:\n  - uid: System.Speech.Recognition.RecognizedWordUnit\n    name: RecognizedWordUnit\n    nameWithType: RecognizedWordUnit\n    fullName: System.Speech.Recognition.RecognizedWordUnit\n  - name: '[]'\n    nameWithType: '[]'\n    fullName: '[]'\n- uid: System.Globalization.CompareOptions\n  parent: System.Globalization\n  isExternal: false\n  name: CompareOptions\n  nameWithType: CompareOptions\n  fullName: System.Globalization.CompareOptions\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognize(String, CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognize(String, CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognize(String, CompareOptions)\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognizeAsync(String)\n  nameWithType: SpeechRecognizer.EmulateRecognizeAsync(String)\n  fullName: SpeechRecognizer.EmulateRecognizeAsync(String)\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognizeAsync(RecognizedWordUnit[], CompareOptions)\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognizeAsync(String, CompareOptions)\n  nameWithType: SpeechRecognizer.EmulateRecognizeAsync(String, CompareOptions)\n  fullName: SpeechRecognizer.EmulateRecognizeAsync(String, CompareOptions)\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognizeCompleted\n  nameWithType: SpeechRecognizer.EmulateRecognizeCompleted\n  fullName: SpeechRecognizer.EmulateRecognizeCompleted\n- uid: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n  name: EmulateRecognizeCompletedEventArgs\n  nameWithType: EmulateRecognizeCompletedEventArgs\n  fullName: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.EmulateRecognizeCompletedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<EmulateRecognizeCompletedEventArgs>\n  nameWithType: EventHandler<EmulateRecognizeCompletedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n    name: EmulateRecognizeCompletedEventArgs\n    nameWithType: EmulateRecognizeCompletedEventArgs\n    fullName: System.Speech.Recognition.EmulateRecognizeCompletedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.Enabled\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: Enabled\n  nameWithType: SpeechRecognizer.Enabled\n  fullName: SpeechRecognizer.Enabled\n- uid: System.Speech.Recognition.SpeechRecognizer.Grammars\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: Grammars\n  nameWithType: SpeechRecognizer.Grammars\n  fullName: SpeechRecognizer.Grammars\n- uid: System.Collections.ObjectModel.ReadOnlyCollection`1\n  name: ReadOnlyCollection<T>\n  nameWithType: ReadOnlyCollection<T>\n  fullName: System.Collections.ObjectModel.ReadOnlyCollection<T>\n- uid: System.Speech.Recognition.Grammar\n  name: Grammar\n  nameWithType: Grammar\n  fullName: System.Speech.Recognition.Grammar\n- uid: System.Collections.ObjectModel.ReadOnlyCollection{System.Speech.Recognition.Grammar}\n  parent: System.Collections.ObjectModel\n  isExternal: false\n  name: ReadOnlyCollection<Grammar>\n  nameWithType: ReadOnlyCollection<Grammar>\n  fullName: System.Collections.ObjectModel.ReadOnlyCollection<System.Speech.Recognition.Grammar>\n  spec.csharp:\n  - uid: System.Collections.ObjectModel.ReadOnlyCollection`1\n    name: ReadOnlyCollection\n    nameWithType: ReadOnlyCollection\n    fullName: System.Collections.ObjectModel.ReadOnlyCollection\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.Grammar\n    name: Grammar\n    nameWithType: Grammar\n    fullName: System.Speech.Recognition.Grammar\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: LoadGrammar(Grammar)\n  nameWithType: SpeechRecognizer.LoadGrammar(Grammar)\n  fullName: SpeechRecognizer.LoadGrammar(Grammar)\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: LoadGrammarAsync(Grammar)\n  nameWithType: SpeechRecognizer.LoadGrammarAsync(Grammar)\n  fullName: SpeechRecognizer.LoadGrammarAsync(Grammar)\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: LoadGrammarCompleted\n  nameWithType: SpeechRecognizer.LoadGrammarCompleted\n  fullName: SpeechRecognizer.LoadGrammarCompleted\n- uid: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n  name: LoadGrammarCompletedEventArgs\n  nameWithType: LoadGrammarCompletedEventArgs\n  fullName: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.LoadGrammarCompletedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<LoadGrammarCompletedEventArgs>\n  nameWithType: EventHandler<LoadGrammarCompletedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.LoadGrammarCompletedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n    name: LoadGrammarCompletedEventArgs\n    nameWithType: LoadGrammarCompletedEventArgs\n    fullName: System.Speech.Recognition.LoadGrammarCompletedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.MaxAlternates\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: MaxAlternates\n  nameWithType: SpeechRecognizer.MaxAlternates\n  fullName: SpeechRecognizer.MaxAlternates\n- uid: System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: PauseRecognizerOnRecognition\n  nameWithType: SpeechRecognizer.PauseRecognizerOnRecognition\n  fullName: SpeechRecognizer.PauseRecognizerOnRecognition\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RecognizerAudioPosition\n  nameWithType: SpeechRecognizer.RecognizerAudioPosition\n  fullName: SpeechRecognizer.RecognizerAudioPosition\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerInfo\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RecognizerInfo\n  nameWithType: SpeechRecognizer.RecognizerInfo\n  fullName: SpeechRecognizer.RecognizerInfo\n- uid: System.Speech.Recognition.RecognizerInfo\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizerInfo\n  nameWithType: RecognizerInfo\n  fullName: System.Speech.Recognition.RecognizerInfo\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RecognizerUpdateReached\n  nameWithType: SpeechRecognizer.RecognizerUpdateReached\n  fullName: SpeechRecognizer.RecognizerUpdateReached\n- uid: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n  name: RecognizerUpdateReachedEventArgs\n  nameWithType: RecognizerUpdateReachedEventArgs\n  fullName: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.RecognizerUpdateReachedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<RecognizerUpdateReachedEventArgs>\n  nameWithType: EventHandler<RecognizerUpdateReachedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.RecognizerUpdateReachedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n    name: RecognizerUpdateReachedEventArgs\n    nameWithType: RecognizerUpdateReachedEventArgs\n    fullName: System.Speech.Recognition.RecognizerUpdateReachedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RequestRecognizerUpdate(Object, TimeSpan)\n  nameWithType: SpeechRecognizer.RequestRecognizerUpdate(Object, TimeSpan)\n  fullName: SpeechRecognizer.RequestRecognizerUpdate(Object, TimeSpan)\n- uid: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RequestRecognizerUpdate(Object)\n  nameWithType: SpeechRecognizer.RequestRecognizerUpdate(Object)\n  fullName: SpeechRecognizer.RequestRecognizerUpdate(Object)\n- uid: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RequestRecognizerUpdate()\n  nameWithType: SpeechRecognizer.RequestRecognizerUpdate()\n  fullName: SpeechRecognizer.RequestRecognizerUpdate()\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechDetected\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: SpeechDetected\n  nameWithType: SpeechRecognizer.SpeechDetected\n  fullName: SpeechRecognizer.SpeechDetected\n- uid: System.Speech.Recognition.SpeechDetectedEventArgs\n  name: SpeechDetectedEventArgs\n  nameWithType: SpeechDetectedEventArgs\n  fullName: System.Speech.Recognition.SpeechDetectedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechDetectedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechDetectedEventArgs>\n  nameWithType: EventHandler<SpeechDetectedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechDetectedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechDetectedEventArgs\n    name: SpeechDetectedEventArgs\n    nameWithType: SpeechDetectedEventArgs\n    fullName: System.Speech.Recognition.SpeechDetectedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: SpeechHypothesized\n  nameWithType: SpeechRecognizer.SpeechHypothesized\n  fullName: SpeechRecognizer.SpeechHypothesized\n- uid: System.Speech.Recognition.SpeechHypothesizedEventArgs\n  name: SpeechHypothesizedEventArgs\n  nameWithType: SpeechHypothesizedEventArgs\n  fullName: System.Speech.Recognition.SpeechHypothesizedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechHypothesizedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechHypothesizedEventArgs>\n  nameWithType: EventHandler<SpeechHypothesizedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechHypothesizedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechHypothesizedEventArgs\n    name: SpeechHypothesizedEventArgs\n    nameWithType: SpeechHypothesizedEventArgs\n    fullName: System.Speech.Recognition.SpeechHypothesizedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: SpeechRecognitionRejected\n  nameWithType: SpeechRecognizer.SpeechRecognitionRejected\n  fullName: SpeechRecognizer.SpeechRecognitionRejected\n- uid: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n  name: SpeechRecognitionRejectedEventArgs\n  nameWithType: SpeechRecognitionRejectedEventArgs\n  fullName: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechRecognitionRejectedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechRecognitionRejectedEventArgs>\n  nameWithType: EventHandler<SpeechRecognitionRejectedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechRecognitionRejectedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n    name: SpeechRecognitionRejectedEventArgs\n    nameWithType: SpeechRecognitionRejectedEventArgs\n    fullName: System.Speech.Recognition.SpeechRecognitionRejectedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.SpeechRecognized\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: SpeechRecognized\n  nameWithType: SpeechRecognizer.SpeechRecognized\n  fullName: SpeechRecognizer.SpeechRecognized\n- uid: System.Speech.Recognition.SpeechRecognizedEventArgs\n  name: SpeechRecognizedEventArgs\n  nameWithType: SpeechRecognizedEventArgs\n  fullName: System.Speech.Recognition.SpeechRecognizedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.SpeechRecognizedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<SpeechRecognizedEventArgs>\n  nameWithType: EventHandler<SpeechRecognizedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.SpeechRecognizedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.SpeechRecognizedEventArgs\n    name: SpeechRecognizedEventArgs\n    nameWithType: SpeechRecognizedEventArgs\n    fullName: System.Speech.Recognition.SpeechRecognizedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.State\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: State\n  nameWithType: SpeechRecognizer.State\n  fullName: SpeechRecognizer.State\n- uid: System.Speech.Recognition.RecognizerState\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizerState\n  nameWithType: RecognizerState\n  fullName: System.Speech.Recognition.RecognizerState\n- uid: System.Speech.Recognition.SpeechRecognizer.StateChanged\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: StateChanged\n  nameWithType: SpeechRecognizer.StateChanged\n  fullName: SpeechRecognizer.StateChanged\n- uid: System.Speech.Recognition.StateChangedEventArgs\n  name: StateChangedEventArgs\n  nameWithType: StateChangedEventArgs\n  fullName: System.Speech.Recognition.StateChangedEventArgs\n- uid: System.EventHandler{System.Speech.Recognition.StateChangedEventArgs}\n  parent: System\n  isExternal: false\n  name: EventHandler<StateChangedEventArgs>\n  nameWithType: EventHandler<StateChangedEventArgs>\n  fullName: System.EventHandler<System.Speech.Recognition.StateChangedEventArgs>\n  spec.csharp:\n  - uid: System.EventHandler`1\n    name: EventHandler\n    nameWithType: EventHandler\n    fullName: System.EventHandler\n  - name: <\n    nameWithType: <\n    fullName: <\n  - uid: System.Speech.Recognition.StateChangedEventArgs\n    name: StateChangedEventArgs\n    nameWithType: StateChangedEventArgs\n    fullName: System.Speech.Recognition.StateChangedEventArgs\n  - name: '>'\n    nameWithType: '>'\n    fullName: '>'\n- uid: System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: UnloadAllGrammars()\n  nameWithType: SpeechRecognizer.UnloadAllGrammars()\n  fullName: SpeechRecognizer.UnloadAllGrammars()\n- uid: System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: UnloadGrammar(Grammar)\n  nameWithType: SpeechRecognizer.UnloadGrammar(Grammar)\n  fullName: SpeechRecognizer.UnloadGrammar(Grammar)\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognize*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognize\n  nameWithType: SpeechRecognizer.EmulateRecognize\n  fullName: SpeechRecognizer.EmulateRecognize\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: EmulateRecognizeAsync\n  nameWithType: SpeechRecognizer.EmulateRecognizeAsync\n  fullName: SpeechRecognizer.EmulateRecognizeAsync\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RequestRecognizerUpdate\n  nameWithType: SpeechRecognizer.RequestRecognizerUpdate\n  fullName: SpeechRecognizer.RequestRecognizerUpdate\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.#ctor*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: SpeechRecognizer\n  nameWithType: SpeechRecognizer.SpeechRecognizer\n  fullName: SpeechRecognizer.SpeechRecognizer\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioFormat*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioFormat\n  nameWithType: SpeechRecognizer.AudioFormat\n  fullName: SpeechRecognizer.AudioFormat\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioLevel*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioLevel\n  nameWithType: SpeechRecognizer.AudioLevel\n  fullName: SpeechRecognizer.AudioLevel\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioPosition*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioPosition\n  nameWithType: SpeechRecognizer.AudioPosition\n  fullName: SpeechRecognizer.AudioPosition\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.AudioState*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: AudioState\n  nameWithType: SpeechRecognizer.AudioState\n  fullName: SpeechRecognizer.AudioState\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.Dispose*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: Dispose\n  nameWithType: SpeechRecognizer.Dispose\n  fullName: SpeechRecognizer.Dispose\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.Enabled*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: Enabled\n  nameWithType: SpeechRecognizer.Enabled\n  fullName: SpeechRecognizer.Enabled\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.Grammars*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: Grammars\n  nameWithType: SpeechRecognizer.Grammars\n  fullName: SpeechRecognizer.Grammars\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammar*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: LoadGrammar\n  nameWithType: SpeechRecognizer.LoadGrammar\n  fullName: SpeechRecognizer.LoadGrammar\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: LoadGrammarAsync\n  nameWithType: SpeechRecognizer.LoadGrammarAsync\n  fullName: SpeechRecognizer.LoadGrammarAsync\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.MaxAlternates*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: MaxAlternates\n  nameWithType: SpeechRecognizer.MaxAlternates\n  fullName: SpeechRecognizer.MaxAlternates\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: PauseRecognizerOnRecognition\n  nameWithType: SpeechRecognizer.PauseRecognizerOnRecognition\n  fullName: SpeechRecognizer.PauseRecognizerOnRecognition\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RecognizerAudioPosition\n  nameWithType: SpeechRecognizer.RecognizerAudioPosition\n  fullName: SpeechRecognizer.RecognizerAudioPosition\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.RecognizerInfo*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: RecognizerInfo\n  nameWithType: SpeechRecognizer.RecognizerInfo\n  fullName: SpeechRecognizer.RecognizerInfo\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.State*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: State\n  nameWithType: SpeechRecognizer.State\n  fullName: SpeechRecognizer.State\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: UnloadAllGrammars\n  nameWithType: SpeechRecognizer.UnloadAllGrammars\n  fullName: SpeechRecognizer.UnloadAllGrammars\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Speech.Recognition.SpeechRecognizer.UnloadGrammar*\n  parent: System.Speech.Recognition.SpeechRecognizer\n  isExternal: false\n  name: UnloadGrammar\n  nameWithType: SpeechRecognizer.UnloadGrammar\n  fullName: SpeechRecognizer.UnloadGrammar\n  monikers:\n  - netframework-4.5.1\n  - netframework-4.5.2\n  - netframework-4.5\n  - netframework-4.6.1\n  - netframework-4.6.2\n  - netframework-4.6\n  - netframework-4.7\n  content_git_url: https://github.com/dotnet/docs/blob/master/xml/System.Speech.Recognition/SpeechRecognizer.xml\n- uid: System.Object.Equals(System.Object)\n  parent: System.Object\n  isExternal: false\n  name: Equals(Object)\n  nameWithType: Object.Equals(Object)\n  fullName: Object.Equals(Object)\n- uid: System.Object.Equals(System.Object,System.Object)\n  parent: System.Object\n  isExternal: false\n  name: Equals(Object, Object)\n  nameWithType: Object.Equals(Object, Object)\n  fullName: Object.Equals(Object, Object)\n- uid: System.Object.GetHashCode\n  parent: System.Object\n  isExternal: false\n  name: GetHashCode()\n  nameWithType: Object.GetHashCode()\n  fullName: Object.GetHashCode()\n- uid: System.Object.GetType\n  parent: System.Object\n  isExternal: false\n  name: GetType()\n  nameWithType: Object.GetType()\n  fullName: Object.GetType()\n- uid: System.Object.MemberwiseClone\n  parent: System.Object\n  isExternal: false\n  name: MemberwiseClone()\n  nameWithType: Object.MemberwiseClone()\n  fullName: Object.MemberwiseClone()\n- uid: System.Object.ReferenceEquals(System.Object,System.Object)\n  parent: System.Object\n  isExternal: false\n  name: ReferenceEquals(Object, Object)\n  nameWithType: Object.ReferenceEquals(Object, Object)\n  fullName: Object.ReferenceEquals(Object, Object)\n- uid: System.Object.ToString\n  parent: System.Object\n  isExternal: false\n  name: ToString()\n  nameWithType: Object.ToString()\n  fullName: Object.ToString()\n- uid: System.IDisposable\n  parent: System\n  isExternal: false\n  name: IDisposable\n  nameWithType: IDisposable\n  fullName: System.IDisposable\n"}