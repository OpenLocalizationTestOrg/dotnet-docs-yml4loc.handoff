<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="markdown" source-language="en-US" target-language="ja-jp">
    <header>
      <tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-fdd610b" tool-company="Microsoft" />
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">290334934c483b9e8802ebea1e4de5de31a5ad12</xliffext:olfilehash>
      <xliffext:olfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">System.Speech.Recognition.DictationGrammar.yml</xliffext:olfilepath>
      <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">net47</xliffext:oltranslationpriority>
      <xliffext:oltranslationtype xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">Human Translation</xliffext:oltranslationtype>
      <xliffext:olskeletonhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">e11631db2ffb7a03ea14830d27c3e99509e99ea0</xliffext:olskeletonhash>
      <xliffext:olxliffhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">93a9401862542b23e200b1b6b298d013141355a1</xliffext:olxliffhash>
    </header>
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Represents a speech recognition grammar used for free text dictation.</source>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve" extradata="MT">
          <source>This class provides applications with a predefined language model that can process spoken user input into text.</source>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve" extradata="MT">
          <source>This class supports both default and custom <ph id="ph1">&lt;xref:System.Speech.Recognition.DictationGrammar&gt;</ph> objects.</source>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve" extradata="MT">
          <source>For information about selecting a dictation grammar, see the <ph id="ph1">&lt;xref:System.Speech.Recognition.DictationGrammar.%23ctor%28System.String%29&gt;</ph> constructor.</source>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve" extradata="MT">
          <source>By default, the <ph id="ph1">&lt;xref:System.Speech.Recognition.DictationGrammar&gt;</ph> language model is context free.</source>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve" extradata="MT">
          <source>It does not make use of specific words or word order to identify and interpret audio input.</source>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve" extradata="MT">
          <source>To add context to the dictation grammar, use the <ph id="ph1">&lt;xref:System.Speech.Recognition.DictationGrammar.SetDictationContext%2A&gt;</ph> method.</source>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve" extradata="MT">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.DictationGrammar&gt;</ph> objects do not support the <ph id="ph2">&lt;xref:System.Speech.Recognition.Grammar.Priority%2A&gt;</ph> property.</source>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve" extradata="MT">
          <source><ph id="ph1">&lt;xref:System.Speech.Recognition.DictationGrammar&gt;</ph> throws a <ph id="ph2">&lt;xref:System.NotSupportedException&gt;</ph> if <ph id="ph3">&lt;xref:System.Speech.Recognition.Grammar.Priority%2A&gt;</ph> is set.</source>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Initializes a new instance of the <ph id="ph1">&lt;xref href="System.Speech.Recognition.DictationGrammar"&gt;&lt;/xref&gt;</ph> class for the default dictation grammar provided by Windows Desktop Speech Technology.</source>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve" extradata="MT">
          <source>The default dictation grammar emulates standard dictation practices, including punctuation.</source>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve" extradata="MT">
          <source>It does not support the spelling of a word.</source>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Initializes a new instance of the <ph id="ph1">&lt;xref href="System.Speech.Recognition.DictationGrammar"&gt;&lt;/xref&gt;</ph> class with a specific dictation grammar.</source>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve" extradata="MT">
          <source>The Speech platform uses a specialized URI syntax to define the custom dictation grammar.</source>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve" extradata="MT">
          <source>The value <ph id="ph1">`grammar:dictation`</ph> indicates the default dictation grammar.</source>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve" extradata="MT">
          <source>The value <ph id="ph1">`grammar:dictation#spelling`</ph> indicates the spelling dictation grammar.</source>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>An XML-compliant Universal Resource Identifier (URI) that specifies the dictation grammar, either <bpt id="p1">&lt;code&gt;</bpt><ph id="ph1">grammar:dictation</ph><ept id="p1">&lt;/code&gt;</ept> or <bpt id="p2">&lt;code&gt;</bpt><ph id="ph2">grammar:dictation#spelling</ph><ept id="p2">&lt;/code&gt;</ept>.</source>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Adds a context to a dictation grammar that has been loaded by a <ph id="ph1">&lt;xref href="System.Speech.Recognition.SpeechRecognizer"&gt;&lt;/xref&gt;</ph> or a <ph id="ph2">&lt;xref href="System.Speech.Recognition.SpeechRecognitionEngine"&gt;&lt;/xref&gt;</ph> object.</source>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve" extradata="MT">
          <source>By default, the dictation grammar does not make use of specific words or word order to identify and interpret audio input.</source>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve" extradata="MT">
          <source>When a context is added to a dictation grammar, the recognition engine uses the <ph id="ph1">`precedingText`</ph> and <ph id="ph2">`subsequentText`</ph> to identify when to interpret speech as dictation.</source>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve" extradata="MT">
          <source>A dictation grammar must be loaded by a <ph id="ph1">&lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;</ph> or <ph id="ph2">&lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</ph> object before you can use <ph id="ph3">&lt;xref:System.Speech.Recognition.DictationGrammar.SetDictationContext%2A&gt;</ph> to add a context.</source>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve" extradata="MT">
          <source>The following table describes how the recognition engine uses the two parameters to determine when to use the dictation grammar.</source>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve" extradata="MT">
          <source>Description</source>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve" extradata="MT">
          <source>not <ph id="ph1">`null`</ph></source>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve" extradata="MT">
          <source>not <ph id="ph1">`null`</ph></source>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve" extradata="MT">
          <source>The recognition engine uses the terms to bracket possible candidate phrases.</source>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve" extradata="MT">
          <source>not <ph id="ph1">`null`</ph></source>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve" extradata="MT">
          <source>The recognition engine uses the <ph id="ph1">`subsequentText`</ph> to finish dictation.</source>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve" extradata="MT">
          <source>not <ph id="ph1">`null`</ph></source>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve" extradata="MT">
          <source>The recognition engine uses the <ph id="ph1">`precedingText`</ph> to start dictation.</source>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve" extradata="MT">
          <source>The recognition engine does not use a context when using the dictation grammar.</source>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Text that indicates the start of a dictation context.</source>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Text that indicates the end of a dictation context.</source>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>